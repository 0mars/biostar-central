<?xml version="1.0" encoding="iso-8859-1"?>

<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Daily Life in an Ivory Basement</title>
<subtitle type="html"><![CDATA[
This Space Intentionally Left Empty
]]></subtitle>
<id>http://ivory.idyll.org/blog/tags/bioinformatics</id>
<link rel="alternate" type="text/html" href="http://ivory.idyll.org/blog" />
<link rel="self" type="text/xml" href="http://ivory.idyll.org/blog/tags/bioinformatics" />

<author>
<name>Titus Brown</name>
<uri>http://ivory.idyll.org/blog/tags/bioinformatics</uri>
<email>titus+blog1@idyll.org</email>
</author>
<rights>Copyright 2004-2009, C. Titus Brown</rights>
<generator uri="http://pyblosxom.sourceforge.net/" version="1.3.2 2/13/2006">
PyBlosxom http://pyblosxom.sourceforge.net/ 1.3.2 2/13/2006
</generator>

<updated>2012-01-05T05:57:17Z</updated>
<!-- icon?  logo?  -->

<entry>
<title type="html">What I&apos;m REALLY thinking about when I use your bioinformatics software</title>
<category term="/u/t/blog/entries/jan-12" />
<id>http://ivory.idyll.org/blog/2012/01/04/top-ten-things-i-hate-about-bioinfo-software</id>
<updated>2012-01-05T05:57:17Z</updated>
<published>2012-01-05T05:57:17Z</published>
<link rel="alternate" type="text/html" href="http://ivory.idyll.org/blog/jan-12/top-ten-things-i-hate-about-bioinfo-software" />
<content type="html">&lt;div class=&quot;document&quot;&gt;
&lt;p&gt;If you&apos;re like me, we pretend to care about the science in
bioinformatics software.  But what we really do is try to find reasons
not to outright loathe the software -- because, lud knows, there are
usually plenty of reasons to hate it.&lt;/p&gt;
&lt;p&gt;In no particular order, here are the top 10 things I hate about your
bioinformatics software.  You know who I&apos;m talking about.&lt;/p&gt;
&lt;ol class=&quot;arabic simple&quot;&gt;
&lt;li&gt;You posted it on SourceForge (and so I can&apos;t download the damn
thing using a simple URL).&lt;/li&gt;
&lt;li&gt;You&apos;re not using version control (and hence are not a scientist).&lt;/li&gt;
&lt;li&gt;You put _ in the damned file name unnecessarily (that requires a shift key on my keyboard).&lt;/li&gt;
&lt;li&gt;fizbam-0.9.3.tar.gz either untars into the current directory, OR a directory named &apos;fizbam&apos;.  Alternatively, you named it fizbam-0.9.3-2011010101010101010101010101020.tar.gz and it untars into THAT monster of a name (and my ls goes off the screen).&lt;/li&gt;
&lt;li&gt;You have no README, or, if you do, it&apos;s a one-liner that refers to a URL (didn&apos;t I already download your damned software?)  OR 5a. Your README file is in HTML (less is better than lynx, dontcha know?)&lt;/li&gt;
&lt;li&gt;Running &apos;make&apos; rebuilds everything from scratch every time you run it (seriously?)&lt;/li&gt;
&lt;li&gt;There are neither tests nor examples; or, if there are, I can&apos;t run &apos;em, and even if they do run, I have no idea if the results are correct.&lt;/li&gt;
&lt;li&gt;The output is in some weird format and/or location (wait, I have to do a find to find the last file written, and then guess as to its format?)&lt;/li&gt;
&lt;li&gt;The command line options are poorly labeled and described, use random abbreviations, and/or are sensitive to order (unlike every good command-line parsing library written).&lt;/li&gt;
&lt;li&gt;You CaMEl-CaSED your software name so that not even tab completion can figure it out (program names should be all lower-case, as Darwin intended).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;p&gt;Post your top ten and send me the URLs... :)&lt;/p&gt;
&lt;p&gt;--titus&lt;/p&gt;
&lt;/div&gt;
</content>
</entry>

<entry>
<title type="html">Paper draft: Scaling metagenome sequence assembly with probabilistic de Bruijn graphs</title>
<category term="/u/t/blog/entries/dec-11" />
<id>http://ivory.idyll.org/blog/2011/12/20/kmer-percolation-posted</id>
<updated>2011-12-20T17:10:20Z</updated>
<published>2011-12-20T17:10:20Z</published>
<link rel="alternate" type="text/html" href="http://ivory.idyll.org/blog/dec-11/kmer-percolation-posted" />
<content type="html">&lt;div class=&quot;document&quot;&gt;
&lt;p&gt;(updated to point to &lt;a class=&quot;reference&quot; href=&quot;http://arxiv.org/&quot;&gt;http://arxiv.org/&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Authors: Jason Pell, Arend Hintze, Rosangela Canino-Koning, Adina Howe, James M. Tiedje, C. Titus Brown&lt;/p&gt;
&lt;p&gt;Abstract:&lt;/p&gt;
&lt;blockquote&gt;
The memory requirements for de novo assembly of short-read shotgun
sequencing data from complex microbial populations are an
increasingly large practical barrier to environmental
studies. Here we introduce a memory-efficient graph representation
with which we can analyze the k-mer connectivity of metagenomic
samples, allowing us to reduce the size of the de novo assembly
process for metagenomes with a &amp;quot;divide and conquer&amp;quot;
algorithm. This graph representation is based on a probabilistic
data structure, a Bloom filter, that allows us to store assembly
graphs in as little as 4 bits per k-mer. We use this approach to
achieve a 20-fold decrease in memory for the assembly of a soil
metagenome sample.&lt;/blockquote&gt;
&lt;p&gt;The paper is available on arXiv here: &lt;a class=&quot;reference&quot; href=&quot;http://arxiv.org/abs/1112.4193&quot;&gt;http://arxiv.org/abs/1112.4193&lt;/a&gt;.
Comments are welcome!  We&apos;re planning to submit it to PNAS later this
week.&lt;/p&gt;
&lt;p&gt;I&apos;ll write a longer blog post about it soon.&lt;/p&gt;
&lt;p&gt;--titus&lt;/p&gt;
&lt;/div&gt;
</content>
</entry>

<entry>
<title type="html">Data Intensive Science, and Workflows</title>
<category term="/u/t/blog/entries/dec-11" />
<id>http://ivory.idyll.org/blog/2011/12/11/data-intensive-science-and-workflows</id>
<updated>2011-12-11T15:00:57Z</updated>
<published>2011-12-11T15:00:57Z</published>
<link rel="alternate" type="text/html" href="http://ivory.idyll.org/blog/dec-11/data-intensive-science-and-workflows" />
<content type="html">&lt;div class=&quot;document&quot;&gt;
&lt;p&gt;I&apos;m writing this on my way back from Stockholm, where I attended a
workshop on the &lt;a class=&quot;reference&quot; href=&quot;http://research.microsoft.com/en-us/collaboration/fourthparadigm/&quot;&gt;4th Paradigm&lt;/a&gt;.  This is the idea (so named by Jim
Gray, I gather?) that data-intensive science is a distinct paradigm
from the first three paradigms of scientific investigation -- theory,
experiment, and simulation.  I was invited to attend as a token
biologist -- someone in biology who works with large scale data, and
thinks about large scale data, but isn&apos;t necessarily &lt;em&gt;only&lt;/em&gt; devoted to
dealing with large scale data.&lt;/p&gt;
&lt;p&gt;The workshop was pretty interesting.  It was run by Microsoft, who
invited me &amp;amp; paid my way out there.  The idea was to identify areas of
opportunity in which Microsoft could make investments to help out
scientists and develop new perspectives on the future of eScience.  To
do that, we played a game that I&apos;ll call the &amp;quot;anchor game&amp;quot;, where we
divvied up into groups to discuss the blocks to our work that stemmed
from algorithms and tools, data, process and workflows,
social/organizational aspects. In each group we put together sticky
notes with our &amp;quot;complaints&amp;quot; and then ranked them by how big of an
anchor they were on us -- &amp;quot;deep&amp;quot; sticky notes held us back more than
shallow sticky notes.  We then reorganized by discipline, and put
together an end-to-end workflow that we hoped in 5 years would be
possible, and then finally we looked for short- and medium-term
projects that would help get us there.&lt;/p&gt;
&lt;p&gt;The big surprise for me in all of this was that it turns out I&apos;m most
interested in workflows and process!  All of my sticky notes had the
same theme: it wasn&apos;t tools, or data management, or social aspects
that were causing me problems, but rather the development of
appropriate workflows for scientific investigation.  Very weird, and
not what I would have predicted from the outset.&lt;/p&gt;
&lt;p&gt;Two questions came up for me during the workshop:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why don&apos;t people use workflows in bioinformatics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first question that comes to mind is, why doesn&apos;t anyone I know
use a formal workflow engine in bioinformatics?  I know they exist
(Taverna, for one, has a bunch of bioinformatics workflows); I&apos;m
reasonably sure they would be useful; but there seems to be some
block against using them!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What would the ideal workflow situation be?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;During the anchor game, our group (which consisted of two biologists,
myself and Hugh Shanahan; a physicist, Geoffrey Fox; and a few
computer scientists, including Alex Wade) came up with an idea for a
specific tool.  The tool would be a bridge between Datascope for
biologists and a workflow engine.  The essential idea is to combine
data exploration with audit trail recording, which could then be
hardened into a workflow template and re-used.&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;p&gt;Thinking about the process I usually use when working on a new
problem, it tends to consist of all these activites mixed together:&lt;/p&gt;
&lt;ol class=&quot;arabic simple&quot; start=&quot;0&quot;&gt;
&lt;li&gt;evaluation of data quality, and application of &amp;quot;computational&amp;quot; controls&lt;/li&gt;
&lt;li&gt;exploration of various data manipulation steps, looking for statistical signal&lt;/li&gt;
&lt;li&gt;solidifying of various subcomponents of the data manipulation steps into scripted actions&lt;/li&gt;
&lt;li&gt;deployment of the entire thing on multiple data sets&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, I&apos;m never quite done -- new data types and questions always come
up, and there are always components to tweak.  But portions of the
workflow become pretty solid by the time I&apos;m halfway done with the
initial project, and the evaluation of data quality accretes more
steps but rarely loses one.  So it could be quite useful to be able to
take a step back and say, &amp;quot;yeah, these steps?  wrap &apos;em up and put &apos;em
into a workflow, I&apos;m done.&amp;quot;  Except that I also want to be able to
edit and change them in the future.  And I&apos;d like to be able to post
the results along with the precise steps taken to generate them.  And
(as long as I&apos;m greedy) I would like to work at the command line,
while I know that others would like to be able to work in a notebook
or graphical format.  And I&apos;d like to be able to &amp;quot;scale out&amp;quot; the
computation by bringing the compute to the data.&lt;/p&gt;
&lt;p&gt;For all of this I need three things: I need workflow &lt;em&gt;agility&lt;/em&gt;, I need
workflow &lt;em&gt;versioning&lt;/em&gt;, and I need workflow &lt;em&gt;tracking&lt;/em&gt;.  And this all
needs to sit on top of a workflow component model that lets me run the
components of the workflow wherever the &lt;em&gt;data&lt;/em&gt; is.&lt;/p&gt;
&lt;p&gt;I&apos;m guessing no tool out there does this, although I know other people
are thinking this way, so maybe I&apos;m wrong.  The Microsoft folk didn&apos;t
know of any, though, and they seemed pretty well informed in this area
:).&lt;/p&gt;
&lt;p&gt;The devil&apos;s choice I personally make in all of this is to go for
workflow agility, and ignore versioning and tracking and the component
model, by scripting the hell out of things.  But this is getting old,
and as I get older and have to teach my wicked ways to grad students
and postdocs, the lack of versioning and tracking and easy scaling out
gets more and more obvious.  And now that I&apos;m trying to actually teach
computational science to biologists, it&apos;s simply appallingly difficult
to convey this stuff in a sensible way.  So I&apos;m looking for something
better.&lt;/p&gt;
&lt;p&gt;One particularly intriguing type of software I&apos;ve been looking at
recently is the &amp;quot;interactive Web notebook&amp;quot; --
&lt;a class=&quot;reference&quot; href=&quot;http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html&quot;&gt;ipython notebook&lt;/a&gt; and
&lt;a class=&quot;reference&quot; href=&quot;http://sagenb.org/&quot;&gt;sagenb&lt;/a&gt;. These are essentially Mathematica or matlab-style notebook
packages that work with IPython or Sage, scientific computing and
mathematical computing systems in Python.  They let you run Python
code interactively, and colocate it with its output (including
graphics); the notebooks can then be saved and loaded and re-run.  I&apos;m
thinking about working one or the other into my class, since it would
let me move away from command-line dependence a bit.  (Command lines
are great, but throwing them at biologists, along with Python programming,
data analysis, and program execution all together, seems a bit cruel.
And not that productive.)&lt;/p&gt;
&lt;p&gt;It would also be great to have cloud-enabled workflow components.  As
I embark on more and more sequence analysis, there are only about a
dozen &amp;quot;things&amp;quot; we actually do, but mixed and matched.  These things
could be hardened, parameterized into components, and placed behind an
RPC interface that would give us a standard way to execute them.
Combined with a suitable data abstraction layer, I could run the
components from location A on data in location B in a semi-transparent
way, and also record and track their use in a variety of ways.  Given
a suitably rich set of components and use cases, I could imagine that
these components and their interactions could be executed from
multiple workflow engines, and with usefully interesting GUIs.  I know
Domain Specific Languages are already passe, but a DSL might be a good
way to find the right division between subcomponents.&lt;/p&gt;
&lt;p&gt;I&apos;d be interested in hearing about such things that may already exist.
I&apos;m aware of Galaxy, but I think the components in Galaxy are not
quite written at the right level of abstraction for me; Galaxy is also
more focused on the GUI than I want.  I don&apos;t know anything about
Taverna, so I&apos;m going to look into that a bit more.  And, inevitably,
we&apos;ll be writing some of our own software in this area, too.&lt;/p&gt;
&lt;p&gt;Overall, I&apos;m really interested in workflow approaches that let me transition
seemlessly between &lt;a class=&quot;reference&quot; href=&quot;http://ivory.idyll.org/blog/dec-11/is-discovery-science-really-bogus.html&quot;&gt;discovery science&lt;/a&gt; and &amp;quot;firing for effect&amp;quot; for hypothesis-driven science.&lt;/p&gt;
&lt;p&gt;A few more specific thoughts:&lt;/p&gt;
&lt;p&gt;In the area of metagenomics (one of my research focuses at the
moment), it would be great to see &lt;a class=&quot;reference&quot; href=&quot;http://img.jgi.doe.gov/cgi-bin/m/main.cgi&quot;&gt;img/m&lt;/a&gt;, &lt;a class=&quot;reference&quot; href=&quot;http://camera.calit2.net/&quot;&gt;camera&lt;/a&gt;, and &lt;a class=&quot;reference&quot; href=&quot;http://metagenomics.anl.gov/&quot;&gt;MG-RAST&lt;/a&gt; move towards a &amp;quot;broken out&amp;quot; workflow
that lets semi-sophisticated computational users (hi mom!) run their
stuff on the Amazon Cloud and on private HPCs or clouds.  While I
appreciate hosted services, there are many drawbacks to them, and I&apos;d love
to get my hands on the guts of those services.  (I&apos;m
sure the MG-RAST folk would like me to note that they are moving
towards making their pipeline more usable outside of Argonne:
&lt;a class=&quot;reference&quot; href=&quot;https://github.com/MG-RAST/MG-RAST-pipeline&quot;&gt;so noted&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;In the comments to my post on &lt;a class=&quot;reference&quot; href=&quot;http://ivory.idyll.org/blog/dec-11/four-reasons-i-wont-use-your-data-analysis-pipeline.html&quot;&gt;Four reasons I won&apos;t use your data
analysis pipeline&lt;/a&gt;,
Andrew Davison reminds me of VisTrails, which some people at the MS
workshop recommended.&lt;/p&gt;
&lt;p&gt;I met David De Roure from &lt;a class=&quot;reference&quot; href=&quot;http://www.myexperiment.org/&quot;&gt;myExperiment&lt;/a&gt; at the MS workshop.  To quote,
&amp;quot;myExperiment makes it easy to find, use, and share scientific
workflows and other Research Objects, and to build communities.&amp;quot;&lt;/p&gt;
&lt;p&gt;David put me in touch with Carole Goble who is involved with
&lt;a class=&quot;reference&quot; href=&quot;http://www.taverna.org.uk/&quot;&gt;Taverna&lt;/a&gt;.  Something to look at.&lt;/p&gt;
&lt;p&gt;In the &lt;a class=&quot;reference&quot; href=&quot;http://pag.confex.com/pag/xx/webprogrampreliminary/Session1139.html&quot;&gt;cloud computing workshop&lt;/a&gt;
I organized at the Planet and Animal Genome conference this January, I
will get a chance to buttonhole one of the Galaxy Cloud developers.  I
hope to make the most of this opportunity ;).&lt;/p&gt;
&lt;p&gt;It&apos;d be interesting to do some social science research on what
difficulties users encounter when they attempt to use workflow
engines.  A lot of money goes into developing them, apparently, but at
least in bioinformatics they are not widely used.  Why?  This is sort
of in line with Greg Wilson&apos;s Software Carpentry and the wonderfully
named blog &lt;a class=&quot;reference&quot; href=&quot;http://www.neverworkintheory.org/&quot;&gt;It will never work in theory&lt;/a&gt;: rather than guessing randomly
at what technical directions need to be pursued, why not study it
empirically?  It is increasingly obvious to me that improving
computational science productivity has more to do with lowering
learning barriers and changing other societal or cultural issues than
with a simple lack of technology, and figuring out how (and if)
appropriate technology could be integrated with the right incentives
and teaching strategy is pretty important.&lt;/p&gt;
&lt;p&gt;--titus&lt;/p&gt;
&lt;p&gt;p.s. Special thanks to Kenji Takeda and Tony Hey for inviting me to the
workshop, and especially for paying my way.  &apos;twas really interesting!&lt;/p&gt;
&lt;/div&gt;
</content>
</entry>

<entry>
<title type="html">Assembling genomes with modern sequencing</title>
<category term="/u/t/blog/entries/aug-11" />
<id>http://ivory.idyll.org/blog/2011/08/09/assemblathon-and-assembly</id>
<updated>2011-08-10T00:10:25Z</updated>
<published>2011-08-10T00:10:25Z</published>
<link rel="alternate" type="text/html" href="http://ivory.idyll.org/blog/aug-11/assemblathon-and-assembly" />
<content type="html">&lt;div class=&quot;document&quot;&gt;
&lt;p&gt;As sequencing gets cheaper and cheaper, one would expect the answer
for how to best sequence (and assemble!) any given genome would
change.  Most biologists assume something along these lines: everyone else
has achieved some standard coverage (say 10x, or 100x) for their genome,
so all we need to do is multiply that number times the size of my genome
of interest, and then multiply that by the cost/bp, and voila! I will
be able to have my very own genome sequence!&lt;/p&gt;
&lt;p&gt;Naturally it&apos;s a bit more complicated than that, for a couple of
reasons.  First, the length of the reads matters quite a bit.  If
you&apos;re reading off a 1 GB eukaryotic genome in chunks of 100 bases,
you&apos;re going to have trouble assembling the darn thing.  First, you
have to worry about complex repeats, which (in the context of
assembly) are just plain evil, because they create connectivity
structures that simply can&apos;t be resolved without additional
information.  Second, you need to think about sequencing bias, such as
GC and AT rich regions -- most sequencers don&apos;t do that well on
GC-rich regions, which are plentiful in big eukaryotic genomes.  And
third, normal sampling variation in shotgun coverage will screw you,
on top of all of this, if you don&apos;t think about it.&lt;/p&gt;
&lt;p&gt;So, what &lt;em&gt;is&lt;/em&gt; the optimal sequencing strategy, then?&lt;/p&gt;
&lt;p&gt;There&apos;s been some interesting discussion on the &lt;a class=&quot;reference&quot; href=&quot;http://assemblathon.org/&quot;&gt;assemblathon&lt;/a&gt; &lt;a class=&quot;reference&quot; href=&quot;http://assemblathon.org/pages/mailing-list&quot;&gt;mailing list&lt;/a&gt; about all of this,
which, for the most part, I&apos;ll be paraphrasing and interpreting: the
list archives are closed and the list policy about citing people is
that I need to ask them for individual permission, and that&apos;s too much
work :).  If you&apos;re interested in the source messages, I recommend
subscribing yourself and looking through the archives for messages
from June 2011; if they open up the archives, I&apos;ll link directly to
some of the more interesting messages.&lt;/p&gt;
&lt;p&gt;A key component of any sequencing strategy discussion nowadays is that
sequencing has become &lt;em&gt;very&lt;/em&gt; commercial.  While this drives down costs
(pretty dramatically!), you also can&apos;t trust a damn thing that
sequencing companies say, because the market is very competitive and
there&apos;s very little percentage in straight-up honesty, much less full
disclosure.  (Paranoid much?  Yeah, buy me beer sometime.)  Moreover,
there are several competing sequencing centers -- primarily the Broad
Institute and the Beijing Genome Institute, as well as the Joint
Genome Institute, Sanger, and St. Louis, and probably another five that
I&apos;m missing -- that all appear to have
adopted different policies with respect to sequencing genomes.  I
don&apos;t really know what they are in detail, but (for example) Broad has
a stereotyped sequencing strategy for which it has written its own
software suite (see &lt;a class=&quot;reference&quot; href=&quot;http://www.broadinstitute.org/software/allpaths-lg/blog/&quot;&gt;ALLPATHS-LG&lt;/a&gt;), and
you can read the details in &lt;a class=&quot;reference&quot; href=&quot;http://www.pnas.org/content/early/2010/12/20/1017351108.full.pdf+html&quot;&gt;the PNAS paper&lt;/a&gt;.
The bottom line is you need to talk to people who have experience with
actual sequence, and not be overly trusting of either sequencing centers
or company reps.&lt;/p&gt;
&lt;p&gt;Another key component of any sequencing strategy discussion is the
software being used to assemble.  Some centers have their own
assemblers (BGI has SOAPdenovo, Broad has ALLPATHS-LG), but there are
literally dozens of assemblers out there.  The assemblers can broadly
be broken down into about four different types:
overlap-layout-consensus, de Bruijn graph, greedy local, and &amp;quot;other&amp;quot;.
I&apos;m most familiar with de Bruijn graph assemblers, since that&apos;s what
I&apos;m working with here at MSU, but there are advantages and
disadvantages to the various kinds.  Maybe more on that later.
But the bottom line here is that there are many brilliant, passionate,
opinionated people who have written their own assembler, and will
swear by all that is holy that it is the best one.  How do you choose?&lt;/p&gt;
&lt;p&gt;A third key component of any sequencing strategy discussion is the
genome itself.  Mihai Pop&apos;s group just published a veddy interesting
article on prokaryotic assembly (see &lt;a class=&quot;reference&quot; href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3103447/&quot;&gt;Wetzel et al., 2011&lt;/a&gt;) in which
they argue that the optimal sequencing strategy needs to be
dynamically adjusted to the repeat structure of the genome: that is,
you need to do a first sequencing run; analyze it for repeat
structures; and then plan out your next rounds of sequencing based on
that information.  While I am always suspicious of plans that require
intelligent thought (slow! expen$ive!) to be inserted into sequencing
pipelines (fast! high throughput!), I think they make a pretty good
argument -- and that&apos;s just for prokaryotic genomes, which are simple
compared to eukaryotic genomes... for eukaryotic genomes, you also
have to worry about heterozygosity (how much internal variation there
is between the two haploid genomes you&apos;re sequencing).  So how can
you strategize to deal with your genome?&lt;/p&gt;
&lt;p&gt;But let&apos;s back up.  What are we doing, again?&lt;/p&gt;
&lt;p&gt;Sequencing genomes is like this:&lt;/p&gt;
&lt;p&gt;Long, not-terribly-random strings of (physical) DNA, O(10^7-10^10) in length.&lt;/p&gt;
&lt;p&gt;Goal: determine full sequence and connectivity of strings of DNA.&lt;/p&gt;
&lt;p&gt;Process: fragment into lots of bits, sequence in from both ends of each
bit.  Use overlaps, size of bits (&amp;quot;insert size&amp;quot;), to computationally
reassemble.&lt;/p&gt;
&lt;p&gt;(You can read an earlier blog post about why this is a hard problem
&lt;a class=&quot;reference&quot; href=&quot;http://ivory.idyll.org/blog/aug-10/assembly-part-i.html&quot;&gt;here&lt;/a&gt;, or
go read the UMD CBCB assembly primer &lt;a class=&quot;reference&quot; href=&quot;http://www.cbcb.umd.edu/research/assembly_primer.shtml&quot;&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;The challenge, succinctly put, is this: in the face of uneven coverage
and repetitive subsequences, devise the optimal coverage and range of
insert sizes so that you can (a) sample most of the genome
sufficiently and (b) resolve most repetitive regions by looking at
pairs of ends.  Do so (c) as cheaply as possible.&lt;/p&gt;
&lt;p&gt;OK, so what are the parameters you can twiddle?&lt;/p&gt;
&lt;p&gt;It really boils down to these choices:&lt;/p&gt;
&lt;p&gt;Sequencing technology: 454 or Illumina are the main production
machines these days, although I hear things about PacBio, Ion Torrent,
and ABI SOLiD.  454 is much more expensive per base, but gives longer
reads (500bp +); Illumina is (much) cheaper per base, but the reads
are annoyingly short (100-150 bp).  With Illumina you can get ~600 bp
inserts easily, larger inserts (3kb, 5kb, 10kb) with more difficulty.
Not sure about 454.&lt;/p&gt;
&lt;p&gt;Coverage: how much money do you want to spend, on what sequencing
technology?&lt;/p&gt;
&lt;p&gt;Insert sizes: larger inserts are really useful for bridging repeats, but
also much more expensive.&lt;/p&gt;
&lt;p&gt;And... I think that&apos;s about it.  Or is it?&lt;/p&gt;
&lt;p&gt;Well, you need to ask two more questions: can your assembler of choice
take advantage of mixed read lengths, with mixed error models from
different technologies, and/or various insert sizes?  And can your
sequencing center actually make all the different technologies work
reliably?&lt;/p&gt;
&lt;p&gt;(As I keep telling my students, if it were easy they wouldn&apos;t need brilliant
people like us to work on it, now would they?)&lt;/p&gt;
&lt;hr class=&quot;docutils&quot; /&gt;
&lt;p&gt;When I get swamped with these kinds of questions, I usually try to
retreat back into my reductionist hidey hole to clear my head.  So
let&apos;s back up again.  What are the fundamental issues?&lt;/p&gt;
&lt;p&gt;We can&apos;t do much about sequencing bias or heterozygosity, except to
say that more coverage is generally going to make both biases and
internal sequence variation stand out more reliably from random error.
If we actually want to assemble our genome, we also can&apos;t do much
about improving current assemblers, and it&apos;s unclear how to evaluate
assemblers anyway, and most of them don&apos;t appear to do a great job on
very heterogenous sequence types (i.e. from multiple types of
sequencers) - anyway, these are the questions the assemblathon is
asking, and they&apos;re doing a good job; just read the paper when it
comes out.  And we don&apos;t have much control over whether or not our
sequencing center screws up.&lt;/p&gt;
&lt;p&gt;So we&apos;re left with trying to decide on how much 454, how much
Illumina, and what insert sizes.  (Can you hear the shrieks of pain
from sequencing and assembly aficionados as I ruthlessly strip all
of the subtleties from the argument? Fun!)&lt;/p&gt;
&lt;p&gt;For insert size, I like to point people to these two references:&lt;/p&gt;
&lt;p&gt;Whiteford et al., Nuc. Acid Res, 2005
&lt;a class=&quot;reference&quot; href=&quot;http://nar.oxfordjournals.org/content/33/19/e171.full&quot;&gt;http://nar.oxfordjournals.org/content/33/19/e171.full&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Butler et al., Genome Res, 2008
&lt;a class=&quot;reference&quot; href=&quot;http://genome.cshlp.org/content/18/5/810.full&quot;&gt;http://genome.cshlp.org/content/18/5/810.full&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;which make the nice point that there are many repeat structures that
you simply cannot resolve with single-ended reads -- you &lt;em&gt;need&lt;/em&gt;
paired-end reads to do a good job of assembly.  These two papers have
recently been joined by a third, the Wetzel et al. paper above, which
suggests that there are particular (and surprisingly frequent) repeat
structures that cannot be resolved except by a very specific insert
size.  But barring advance knowledge of repeat structure, I would
argue that a nice range of inserts, from 3k to 5k to 10k, should give
you decent results.  We have that for a parasitic nematode project
in which I&apos;m involved, and it&apos;s given us decent scaffold sizes.&lt;/p&gt;
&lt;p&gt;With 454 vs Illumina, I am skeptical that 454 is a good expenditure of
money at this point.  The number of bases is so astonishingly low
compared to what Illumina is outputting (~1m vs ~1bn for the same
amount of money, I think?  At any rate, at least 100x) that you really
need to justify any 454 expenditure.  That having been said, I may be
so used to working with crappy genome assemblies (buy me beer,
hear me weep) that I&apos;m ignoring how much &lt;em&gt;better&lt;/em&gt; they would be with
~10x 454 coverage.  Certainly Greg Dick&apos;s group at U of M has shown me
pretty good evidence that 454 sequences things that Illumina won&apos;t
touch, in metagenomic data.  So I can&apos;t give you much more than my
experience that Illumina will get you ~80% of the way to a decent genome
assembly -- which is something many people would love to have.&lt;/p&gt;
&lt;p&gt;Is there an elephant in the room, and, if so, what is it?  Well, this
touches heavily on our lab&apos;s research, but I think that sequencing
biases are screwing up the assembly game far more than people think.
Right now assemblers have a bunch of poorly understood heuristics that
address sequencer-specific bias, and our experience with these in
metagenomic sequencing suggests that these artifacts and heuristics
are a significant source of misassembly.  More on that ... later.&lt;/p&gt;
&lt;p&gt;I&apos;m really at a loss about how to conclude any discussion of
sequencing strategy.  It&apos;s ridiculously complicated, comes down to a
lot of guessing about what problems you&apos;re likely to run into, and
involves an extremely rapidly changing technology suite.  Getting
a comprehensive answer out of &lt;em&gt;anyone&lt;/em&gt; is hard... and won&apos;t get any
easier for a while.&lt;/p&gt;
&lt;p&gt;That having been said, I&apos;d appreciate pointers to blog posts and open
discussions of these issues on mailing lists.  Having (tried to) teach
some biologists in this area recently, as part of my NGS course, I
think actually providing these discussions could be incredibly
valuable and could raise the level of discourse a fair bit.&lt;/p&gt;
&lt;p&gt;--titus&lt;/p&gt;
&lt;/div&gt;
</content>
</entry>

<entry>
<title type="html">Why the Cloud does not solve the computational scaling problem in biology</title>
<category term="/u/t/blog/entries/aug-11" />
<id>http://ivory.idyll.org/blog/2011/08/07/cloud-not-the-solution</id>
<updated>2011-08-07T19:47:51Z</updated>
<published>2011-08-07T19:47:51Z</published>
<link rel="alternate" type="text/html" href="http://ivory.idyll.org/blog/aug-11/cloud-not-the-solution" />
<content type="html">&lt;div class=&quot;document&quot;&gt;
&lt;p&gt;There&apos;s been a lot of hooplah in the last year or so about the fact
that our ability to generate sequence has scaled faster than &lt;a class=&quot;reference&quot; href=&quot;http://en.wikipedia.org/wiki/Moore&apos;s_law&quot;&gt;Moore&apos;s
Law&lt;/a&gt; over the last few
years, and the attendant &lt;a class=&quot;reference&quot; href=&quot;http://www.sciencemag.org/content/331/6018/666.full&quot;&gt;challenges of scaling analysis capacity&lt;/a&gt;; see Figure
1a and 1b, &lt;a class=&quot;reference&quot; href=&quot;http://www.reddit.com/r/todayilearned/comments/ik7pz/til_full_dna_sequencing_cost_fell_from_100000000/&quot;&gt;this reddit discussion&lt;/a&gt;,
and also my &lt;a class=&quot;reference&quot; href=&quot;http://ivory.idyll.org/blog/oct-10/sky-is-falling&quot;&gt;the sky is falling! blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There&apos;s also been
been some backlash -- it&apos;s gotten to the point where showing any of
the various graphs is greeted with derision, at least judging by the
talk-associated Twitter feed.&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img alt=&quot;http://www.genome.gov/images/content/cost_per_megabase.jpg&quot; src=&quot;http://www.genome.gov/images/content/cost_per_megabase.jpg&quot; style=&quot;height: 200px;&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;Figure 1a.  DNA sequencing costs, from
&lt;a class=&quot;reference&quot; href=&quot;http://www.genome.gov/sequencingcosts/&quot;&gt;http://www.genome.gov/sequencingcosts/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img alt=&quot;http://ivory.idyll.org/permanent/lstein-ngs-capacity.png&quot; src=&quot;http://ivory.idyll.org/permanent/lstein-ngs-capacity.png&quot; style=&quot;width: 300px;&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;Figure 1b.  Sequencing costs vs hard disk costs.  Slide courtesy of
Lincoln Stein.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From by the discussions I&apos;ve seen, I think people still don&apos;t get
-- or at least don&apos;t talk about -- the implications of this scaling
behavior.  In particular, I am surprised to hear &lt;a class=&quot;reference&quot; href=&quot;http://genomebiology.com/2010/11/5/207&quot;&gt;the cloud (the
cloud! the cloud!) touted as The Solution&lt;/a&gt;, since it&apos;s clearly not a
solution to the actual scaling problem.  (Also see: &lt;a class=&quot;reference&quot; href=&quot;http://en.wikipedia.org/wiki/Cloud_computing&quot;&gt;Wikipedia on
cloud computing&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;To see why, consider the following model.  Take two log-linear
plots, one for cost per unit of compute power (CPU cycles, disk space,
RAM, what have you), and one for cost per unit of sequence ($$ per bp
of DNA).  Now suppose that sequence cost is decreasing faster than
compute cost, so you have two nice, diverging linear lines when
you plot these trends over time on a log-linear plot (see Figure 2).&lt;/p&gt;
&lt;!-- http://ivory.idyll.org/permanent/ --&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img alt=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig2.png&quot; src=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig2.png&quot; style=&quot;height: 200px;&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;Figure 2.  A simple model of the (exponential) decrease in compute
costs vs (exponential) decrease sequencing data costs, against
time.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Suppose we&apos;re interested in how much money to allocate to sequencing,
vs how much money to allocate to compute -- the heart of the
problem.  How do these trends behave?  One way to examine them is to
look at the ratio of the data points.&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img alt=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig3.png&quot; src=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig3.png&quot; style=&quot;height: 200px;&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;Figure 3.  The ratio of compute power to data over time, under the
model in the previous figure.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As you&apos;d expect, the ratio of compute power to data is also log-linear
(Figure 3) -- it&apos;s just the difference between the two lines in
Figure 2.  Straight lines on log-linear plots, however, are in reality
&lt;em&gt;exponential&lt;/em&gt; -- see Figure 4!  This is a linear-scale plot of
compute costs relative to data costs -- and as you can see, compute
costs end up dominating.&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img alt=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig4.png&quot; src=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig4.png&quot; style=&quot;height: 200px;&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;Figure 4.  Ratio of compute power to data, over time, on a linear
plot.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;With this model, then, for the same dollar value of data, your
relative compute costs will increase by a factor of 1000 over 10
years.  This is true whether or not you&apos;re using the cloud!  While
your absolute costs may go up or down depending on infrastructure
investments, Amazon&apos;s pricepoint, etc., the fundamental scaling
behavior doesn&apos;t change.  It doesn&apos;t much matter if Amazon is 2x
cheaper than your HPC -- check out Figures 5a, b, and c if you need
graphical confirmation of the math.&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img alt=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig5a.png&quot; src=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig5a.png&quot; style=&quot;height: 200px;&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img alt=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig5b.png&quot; src=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig5b.png&quot; style=&quot;height: 200px;&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img alt=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig5c.png&quot; src=&quot;http://ivory.idyll.org/permanent/cloud-not-the-solution-fig5c.png&quot; style=&quot;height: 200px;&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;Figure 5a,b,c: Scaling behavior isn&apos;t affected by linearly lower costs.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The bottom line is this: &lt;strong&gt;when your data cost is decreasing faster
than your hardware cost, the long-term solution cannot
be to buy, rent, borrow, beg, or steal more hardware.&lt;/strong&gt; The solution
&lt;strong&gt;must&lt;/strong&gt; lie in software and algorithms.&lt;/p&gt;
&lt;p&gt;People who claim that cloud computing is going to provide an answer to
the scaling issue with sequence, then, &lt;em&gt;must&lt;/em&gt; be operating with some
additional assumptions.  Maybe they think the curves are shifted
relative to one another, so that even 1000x costs are not a big deal -
although figure 1 sort of argues against that.  Like me, maybe they&apos;ve
heard that hard disks are about to start scaling way, way better -- if
so, awesome!  That might change the curves for data storage, if not
analysis.  Perhaps their research depends on using only a bounded
amount of sequence -- e.g. single-genome sequencing, for which you can
stop generating data at a certain point.  Or perhaps they&apos;re proposing
to use algorithms that scale sub-linearly with the amount of data
they&apos;re applied to (although I don&apos;t know of any).  Or perhaps they&apos;re
planning for the shift in Moore&apos;s Law behavior that will come when
that Amazon and other cloud computing providers build self-replicating
compute clusters on the moon (hello, exo-atmospheric computing!)
Whatever the plan, it would be interesting to hear their assumptions
explained.&lt;/p&gt;
&lt;p&gt;I think one likely answer to the Big Data conundrum in biology is that
we&apos;ll come up with cleverer and cleverer approaches for quickly
throwing away data that is unlikely to be of any use.  Assuming these
algorithms are linear in their application to data, but have smaller
constants in front of their big-O, this will at least help stem the
tide.  (It will also, unfortunately, generate more and nastier biases
in the results...) But I don&apos;t have any answers for what will happen
in the medium term if sequencing continues to scale as it does.&lt;/p&gt;
&lt;p&gt;It&apos;s also worth noting that de novo assembly (my current focus...)
remains one of the biggest challenges.  It requires gobs of the most
expensive computational resource (RAM, which is not scaling as fast as
disk and CPU), and there are no good solutions on the horizon for
making it scale faster.  Neither mRNAseq nor metagenomics are
well-bounded problems (you always want more sequence!), and assembly
will remain a critical approach for many people for many years.
Moreover, cloud assembly approaches like Contrail are (sooner or
later) doomed by the same logic as above.  But it&apos;s a problem we need
to solve!  As I said at PyCon, &lt;a class=&quot;reference&quot; href=&quot;http://www.flickr.com/photos/hmason/5520881780/&quot;&gt;&amp;quot;Life&apos;s too short to tackle the easy
problems -- come to academia!&amp;quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;--titus&lt;/p&gt;
&lt;p&gt;p.s. If you want to play with the curves yourself,
&lt;a class=&quot;reference&quot; href=&quot;https://spreadsheets.google.com/spreadsheet/ccc?key=0ArcOEBWnXSBidHFVcFpDTnBPZlJIeHFRa3RQNFNaN2c&amp;amp;hl=en_US#gid=0&quot;&gt;here&apos;s a Google Spreadsheet&lt;/a&gt;,
and you can grab a straight CSV file &lt;a class=&quot;reference&quot; href=&quot;https://spreadsheets.google.com/spreadsheet/pub?hl=en_US&amp;amp;hl=en_US&amp;amp;key=0ArcOEBWnXSBidHFVcFpDTnBPZlJIeHFRa3RQNFNaN2c&amp;amp;single=true&amp;amp;gid=0&amp;amp;output=csv&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content>
</entry>
</feed>
