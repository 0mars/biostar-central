[{"pk": 1, "model": "auth.user", "fields": {"username": "community", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2012-03-28 08:29:09", "groups": [], "user_permissions": [], "password": "", "email": "", "date_joined": "2012-03-28 08:29:09"}}, {"pk": 2, "model": "auth.user", "fields": {"username": "u2", "first_name": "Istvan Albert", "last_name": "", "is_active": true, "is_superuser": true, "is_staff": true, "last_login": "2011-11-08 14:35:57", "groups": [], "user_permissions": [], "password": "", "email": "istvan.albert@gmail.com", "date_joined": "2009-09-30 13:30:39"}}, {"pk": 3, "model": "auth.user", "fields": {"username": "u3", "first_name": "Fabio", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2009-10-05 15:43:12", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-09-30 16:09:06"}}, {"pk": 4, "model": "auth.user", "fields": {"username": "u4", "first_name": "Jason", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-11-04 14:45:04", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-09-30 17:10:54"}}, {"pk": 5, "model": "auth.user", "fields": {"username": "u5", "first_name": "Zhenhai Zhang", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-07-06 17:44:55", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-09-30 18:48:08"}}, {"pk": 6, "model": "auth.user", "fields": {"username": "u6", "first_name": "Tom Koerber", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2009-09-30 19:32:29", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-09-30 19:32:29"}}, {"pk": 7, "model": "auth.user", "fields": {"username": "u7", "first_name": "suk211", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-07-07 03:32:47", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-09-30 19:35:27"}}, {"pk": 8, "model": "auth.user", "fields": {"username": "u8", "first_name": "lemon", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2010-10-13 23:59:00", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-10-03 03:02:07"}}, {"pk": 9, "model": "auth.user", "fields": {"username": "u9", "first_name": "Wubin Qu", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-11-04 00:28:53", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-10-03 10:15:52"}}, {"pk": 10, "model": "auth.user", "fields": {"username": "u10", "first_name": "Question Bot", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2010-03-31 03:06:27", "groups": [], "user_permissions": [], "password": "", "email": "istvan.albert@yahoo.com", "date_joined": "2009-10-05 15:44:30"}}, {"pk": 11, "model": "auth.user", "fields": {"username": "u11", "first_name": "Reka Albert", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2009-10-05 16:47:25", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-10-05 16:47:25"}}, {"pk": 12, "model": "auth.user", "fields": {"username": "u12", "first_name": "Yang Yang", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2009-10-07 01:45:15", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-10-06 18:58:10"}}, {"pk": 13, "model": "auth.user", "fields": {"username": "u13", "first_name": "Gue Su Chang", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2009-10-07 13:04:30", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-10-06 22:04:41"}}, {"pk": 14, "model": "auth.user", "fields": {"username": "u14", "first_name": "Zhaorong", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-10-28 22:18:27", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-10-09 02:54:13"}}, {"pk": 15, "model": "auth.user", "fields": {"username": "u15", "first_name": "nickey", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2009-10-18 03:22:53", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-10-18 03:22:53"}}, {"pk": 16, "model": "auth.user", "fields": {"username": "u16", "first_name": "Renee", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2010-02-20 21:13:56", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-10-23 17:40:27"}}, {"pk": 17, "model": "auth.user", "fields": {"username": "u17", "first_name": "Yu", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-10-24 21:24:37", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-11-04 18:39:12"}}, {"pk": 18, "model": "auth.user", "fields": {"username": "u18", "first_name": "Gue Su", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2009-12-01 16:51:54", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-12-01 14:57:35"}}, {"pk": 19, "model": "auth.user", "fields": {"username": "u19", "first_name": "Mohammed Islaih", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2009-12-08 21:45:54", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-12-08 21:45:54"}}, {"pk": 20, "model": "auth.user", "fields": {"username": "u20", "first_name": "Alex Reynolds", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-11-08 12:23:58", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2009-12-23 01:10:22"}}, {"pk": 21, "model": "auth.user", "fields": {"username": "u21", "first_name": "User 21", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2009-12-28 13:56:46", "groups": [], "user_permissions": [], "password": "", "email": "u21", "date_joined": "2009-12-28 13:56:46"}}, {"pk": 22, "model": "auth.user", "fields": {"username": "u22", "first_name": "User 22", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2010-04-10 02:15:19", "groups": [], "user_permissions": [], "password": "", "email": "u22", "date_joined": "2009-12-31 13:26:29"}}, {"pk": 23, "model": "auth.user", "fields": {"username": "u23", "first_name": "Giovanni M Dall'Olio", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-11-08 13:37:21", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2010-01-18 15:43:55"}}, {"pk": 24, "model": "auth.user", "fields": {"username": "u24", "first_name": "etal", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-11-07 23:26:49", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2010-01-23 17:01:47"}}, {"pk": 25, "model": "auth.user", "fields": {"username": "u25", "first_name": "Fabio", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2010-01-27 00:38:35", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2010-01-26 23:42:13"}}, {"pk": 26, "model": "auth.user", "fields": {"username": "u26", "first_name": "Nicojo", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-10-28 06:58:08", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2010-02-12 21:29:24"}}, {"pk": 27, "model": "auth.user", "fields": {"username": "u27", "first_name": "Allen Yu", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2010-03-11 02:42:46", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2010-02-19 07:25:13"}}, {"pk": 28, "model": "auth.user", "fields": {"username": "u28", "first_name": "Curious George", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2010-02-19 19:35:30", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2010-02-19 13:33:45"}}, {"pk": 29, "model": "auth.user", "fields": {"username": "u29", "first_name": "User 29", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2011-03-17 05:18:47", "groups": [], "user_permissions": [], "password": "", "email": "u29", "date_joined": "2010-02-22 03:24:18"}}, {"pk": 30, "model": "auth.user", "fields": {"username": "u30", "first_name": "Pierre Lindenbaum", "last_name": "", "is_active": true, "is_superuser": true, "is_staff": true, "last_login": "2011-11-08 14:35:24", "groups": [], "user_permissions": [], "password": "", "email": "no.me@nowhere.com", "date_joined": "2010-02-25 17:27:15"}}, {"pk": 31, "model": "auth.user", "fields": {"username": "a0", "first_name": "Default Admin", "last_name": "", "is_active": true, "is_superuser": true, "is_staff": true, "last_login": "2012-03-28 08:29:13", "groups": [], "user_permissions": [], "password": "sha1$4c0f1$c7bf5891040cfa33700b7f76481f08687f444ba1", "email": "your-mail-here@your-server-here.com", "date_joined": "2012-03-28 08:29:13"}}, {"pk": 32, "model": "auth.user", "fields": {"username": "fe0e89a1bb", "first_name": "MSU course 2011", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2012-03-28 08:29:13", "groups": [], "user_permissions": [], "password": "", "email": "ngs-course-announce@lists.idyll.org", "date_joined": "2012-03-28 08:29:13"}}, {"pk": 1, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<pre><code>I am not really a user.\n\nI am a background process that keep things tidy.\n</code></pre>", "display_name": "Community", "uuid": "19e3e567b62d8220fb775cd914452da4", "bronze_badges": 0, "last_visited": "2000-01-01 00:00:00", "location": "server farm", "silver_badges": 0, "website": "localhost:8080", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "\n    I am not really a user.\n    \n    I am a background process that keep things tidy.\n    ", "user": 1, "type": 1, "new_messages": 1}}, {"pk": 2, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<p>Current position: Research Associate Professor of Bioinformatics at Penn State</p>", "display_name": "Istvan Albert", "uuid": "3fcb6c64156c558a7fd846326fb49c8e", "bronze_badges": 10, "last_visited": "2011-11-08 14:35:57", "location": "University Park", "silver_badges": 1, "website": "http://www.personal.psu.edu/iua1/", "my_tags": "", "gold_badges": 0, "score": 8, "about_me": "Current position: Research Associate Professor of Bioinformatics at Penn State", "user": 2, "type": 4, "new_messages": 1}}, {"pk": 3, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Fabio", "uuid": "3f7cce36e1f812d76464ecdd12b1f1d7", "bronze_badges": 1, "last_visited": "2009-10-05 15:43:12", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 2, "about_me": "", "user": 3, "type": 2, "new_messages": 2}}, {"pk": 4, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<p>Grad student at PSU</p>", "display_name": "Jason", "uuid": "80c46b2accfc5fca7fce0f0f219dfdce", "bronze_badges": 4, "last_visited": "2011-11-04 14:45:04", "location": null, "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "Grad student at PSU", "user": 4, "type": 2, "new_messages": 1}}, {"pk": 5, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Zhenhai Zhang", "uuid": "2f5f915aa5c708f270e4db95cb539480", "bronze_badges": 2, "last_visited": "2011-07-06 17:44:55", "location": "502 Wartik Lab, Penn State Univ", "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 4, "about_me": null, "user": 5, "type": 2, "new_messages": 2}}, {"pk": 6, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Tom Koerber", "uuid": "b9c7c50e786c5e833a340a66c8d77ae7", "bronze_badges": 1, "last_visited": "2009-09-30 19:32:29", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 3, "about_me": "", "user": 6, "type": 2, "new_messages": 1}}, {"pk": 7, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<p>interested systems biology,protein folding ,genomics</p>", "display_name": "Suk211", "uuid": "f9e06441992a438fa5e4f7005e58cdb0", "bronze_badges": 5, "last_visited": "2011-07-07 03:32:47", "location": "state college", "silver_badges": 0, "website": "http://twitter.com/suk211", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "interested systems biology,protein folding ,genomics", "user": 7, "type": 2, "new_messages": 1}}, {"pk": 8, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Lemon", "uuid": "01988b425a92594c0a25c4a22e172164", "bronze_badges": 0, "last_visited": "2010-10-13 23:59:00", "location": "China,WH", "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 0, "about_me": null, "user": 8, "type": 2, "new_messages": 1}}, {"pk": 9, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<p>Bioinformatics researcher, author of MFEprimer, MPprimer and CelRNAi.</p>", "display_name": "Wubin Qu", "uuid": "d2422a16d4abc0d4ff1273cdf7211745", "bronze_badges": 0, "last_visited": "2011-11-04 00:28:53", "location": "China", "silver_badges": 0, "website": "http://quwubin.sinaapp.com/", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "Bioinformatics researcher, author of MFEprimer, MPprimer and CelRNAi.", "user": 9, "type": 2, "new_messages": 1}}, {"pk": 10, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Question Bot", "uuid": "e90cc0d1c22d3469155c0ccabaae1c8e", "bronze_badges": 6, "last_visited": "2010-03-31 03:06:27", "location": null, "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 3, "about_me": null, "user": 10, "type": 2, "new_messages": 1}}, {"pk": 11, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Reka Albert", "uuid": "7c3725b69de39c251554a1a93059e8e0", "bronze_badges": 0, "last_visited": "2009-10-05 16:47:25", "location": "University Park", "silver_badges": 0, "website": "http://www.phys.psu.edu/~ralbert/", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": null, "user": 11, "type": 2, "new_messages": 1}}, {"pk": 12, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Yang Yang", "uuid": "baea617d06de3b21cd51dde519ebaaae", "bronze_badges": 1, "last_visited": "2009-10-07 01:45:15", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 2, "about_me": "", "user": 12, "type": 2, "new_messages": 1}}, {"pk": 13, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Gue Su Chang", "uuid": "5922d854eae5fe505642a2dbd55deb52", "bronze_badges": 1, "last_visited": "2009-10-07 13:04:30", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 1, "about_me": "", "user": 13, "type": 2, "new_messages": 1}}, {"pk": 14, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<p>A grad student in bioinformatics studying plant small RNAs.</p>", "display_name": "Zhaorong", "uuid": "63ce6767c166458b38abb57207e3c230", "bronze_badges": 4, "last_visited": "2011-10-28 22:18:27", "location": "State College, PA", "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 2, "about_me": "A grad student in bioinformatics studying plant small RNAs.", "user": 14, "type": 2, "new_messages": 1}}, {"pk": 15, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Nickey", "uuid": "e78f9596a6550894cfc16fd57cc0472f", "bronze_badges": 0, "last_visited": "2009-10-18 03:22:53", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "", "user": 15, "type": 2, "new_messages": 3}}, {"pk": 16, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Renee", "uuid": "01e5a0790fdecead1ee97e69e4a44a35", "bronze_badges": 1, "last_visited": "2010-02-20 21:13:56", "location": null, "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 2, "about_me": null, "user": 16, "type": 2, "new_messages": 2}}, {"pk": 17, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Yu", "uuid": "f8cb117b04363973b7621e02d8a3e0de", "bronze_badges": 0, "last_visited": "2011-10-24 21:24:37", "location": null, "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 0, "about_me": null, "user": 17, "type": 2, "new_messages": 1}}, {"pk": 18, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Gue Su", "uuid": "7fbc38d2a269591b0e831a19991b7441", "bronze_badges": 1, "last_visited": "2009-12-01 16:51:54", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 2, "about_me": "", "user": 18, "type": 2, "new_messages": 1}}, {"pk": 19, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Mohammed Islaih", "uuid": "6f52bc5e437d6c142746e9142b1469df", "bronze_badges": 1, "last_visited": "2009-12-08 21:45:54", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 2, "about_me": "", "user": 19, "type": 2, "new_messages": 1}}, {"pk": 20, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Alex Reynolds", "uuid": "ad2f51a69becd7a471f17c750a5efcc5", "bronze_badges": 3, "last_visited": "2011-11-08 12:23:58", "location": null, "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 2, "about_me": null, "user": 20, "type": 2, "new_messages": 1}}, {"pk": 21, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Fuscie Theanne", "uuid": "d8d390c237832f6c0b159f63c7e22e2a", "bronze_badges": 0, "last_visited": "2009-12-28 13:56:46", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "", "user": 21, "type": 2, "new_messages": 1}}, {"pk": 22, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Perly Casoniarys", "uuid": "57d98ca7c4594dc1b3a90c38133d9147", "bronze_badges": 0, "last_visited": "2010-04-10 02:15:19", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "", "user": 22, "type": 2, "new_messages": 1}}, {"pk": 23, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<p>PhD student at the Pompeu Fabra University in Barcelona. Interested in bioinformatics, R, python programming and agile techniques.</p>", "display_name": "Giovanni M Dall'Olio", "uuid": "0cf6834d4e5ec229241ab119fe510160", "bronze_badges": 11, "last_visited": "2011-11-08 13:37:21", "location": "Barcelona, Spain", "silver_badges": 1, "website": "http://bioinfoblog.it", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "PhD student at the Pompeu Fabra University in Barcelona. Interested in bioinformatics, R, python programming and agile techniques.", "user": 23, "type": 3, "new_messages": 1}}, {"pk": 24, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Etal", "uuid": "55ae05c7a166ed7e65f3563ba4c60a6f", "bronze_badges": 2, "last_visited": "2011-11-07 23:26:49", "location": "Athens, GA", "silver_badges": 0, "website": "http://etalog.blogspot.com/", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": null, "user": 24, "type": 2, "new_messages": 1}}, {"pk": 25, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Fabio", "uuid": "adde66d624c304cb85c12585ada470ee", "bronze_badges": 1, "last_visited": "2010-01-27 00:38:35", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "", "user": 25, "type": 2, "new_messages": 1}}, {"pk": 26, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Nicojo", "uuid": "87072e426b805acc0d30d8745e642933", "bronze_badges": 5, "last_visited": "2011-10-28 06:58:08", "location": null, "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 0, "about_me": null, "user": 26, "type": 2, "new_messages": 1}}, {"pk": 27, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Allen Yu", "uuid": "f346cce1f81c4537468343dd586796dd", "bronze_badges": 2, "last_visited": "2010-03-11 02:42:46", "location": null, "silver_badges": 0, "website": null, "my_tags": "", "gold_badges": 0, "score": 0, "about_me": null, "user": 27, "type": 2, "new_messages": 1}}, {"pk": 28, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Curious George", "uuid": "840a42c729a938201b21acaa73dcb451", "bronze_badges": 1, "last_visited": "2010-02-19 19:35:30", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "", "user": 28, "type": 2, "new_messages": 1}}, {"pk": 29, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "", "display_name": "Seva Ebony", "uuid": "a4ef3f20763f0f907b5ed05e26eb2380", "bronze_badges": 0, "last_visited": "2011-03-17 05:18:47", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "", "user": 29, "type": 2, "new_messages": 1}}, {"pk": 30, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<p>Virology, Bioinformatics , Genetics</p>\n<p><a href=\"http://twitter.com/yokofakun\">http://twitter.com/yokofakun</a></p>\n<p>Neil Saunders is an impostor because he doesn't exist.</p>\n<p>Please, don't vote for him :-)</p>", "display_name": "Pierre Lindenbaum", "uuid": "21385792b00f7978801b02a3ef2e6e18", "bronze_badges": 9, "last_visited": "2011-11-08 14:35:24", "location": "France", "silver_badges": 1, "website": "http://plindenbaum.blogspot.com", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "Virology, Bioinformatics , Genetics\n\nhttp://twitter.com/yokofakun\n\nNeil Saunders is an impostor because he doesn't exist.\n\nPlease, don't vote for him :-)\n", "user": 30, "type": 4, "new_messages": 1}}, {"pk": 31, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<p>about me</p>", "display_name": "Default Admin", "uuid": "f2d9ce2c496e2804a25e36447e2c2d12", "bronze_badges": 0, "last_visited": "2000-01-01 00:00:00", "location": "", "silver_badges": 0, "website": "", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "about me", "user": 31, "type": 4, "new_messages": 1}}, {"pk": 32, "model": "server.userprofile", "fields": {"status": 1, "about_me_html": "<p>ANGUS is a site built around the `2010 course on <a href=\"http://bioinformatics.msu.edu/ngs-summer-course-2010\" title=\"Angus Website\">Analyzing Next-Generation Sequencing Data</a></p>", "display_name": "MSU course 2011", "uuid": "72f3699a5daa70a026d972c7125d3d5b", "bronze_badges": 0, "last_visited": "2000-01-01 00:00:00", "location": "", "silver_badges": 0, "website": "http://ged.msu.edu/angus/tutorials-2011/", "my_tags": "", "gold_badges": 0, "score": 0, "about_me": "ANGUS is a site built around the `2010 course on [Analyzing Next-Generation Sequencing Data](http://bioinformatics.msu.edu/ngs-summer-course-2010 \"Angus Website\")", "user": 32, "type": 1, "new_messages": 1}}, {"pk": 1, "model": "server.tag", "fields": {"count": 2, "name": "guidelines"}}, {"pk": 2, "model": "server.tag", "fields": {"count": 1, "name": "bed"}}, {"pk": 3, "model": "server.tag", "fields": {"count": 1, "name": "gff"}}, {"pk": 4, "model": "server.tag", "fields": {"count": 1, "name": "galaxy"}}, {"pk": 5, "model": "server.tag", "fields": {"count": 1, "name": "yeast"}}, {"pk": 6, "model": "server.tag", "fields": {"count": 2, "name": "motif"}}, {"pk": 7, "model": "server.tag", "fields": {"count": 1, "name": "microarray"}}, {"pk": 8, "model": "server.tag", "fields": {"count": 1, "name": "clustering"}}, {"pk": 9, "model": "server.tag", "fields": {"count": 1, "name": "test"}}, {"pk": 10, "model": "server.tag", "fields": {"count": 1, "name": "nucleotides"}}, {"pk": 11, "model": "server.tag", "fields": {"count": 1, "name": "solid"}}, {"pk": 12, "model": "server.tag", "fields": {"count": 1, "name": "deep"}}, {"pk": 13, "model": "server.tag", "fields": {"count": 1, "name": "sequence"}}, {"pk": 14, "model": "server.tag", "fields": {"count": 2, "name": "boy"}}, {"pk": 15, "model": "server.tag", "fields": {"count": 2, "name": "george"}}, {"pk": 16, "model": "server.tag", "fields": {"count": 1, "name": "geneid"}}, {"pk": 17, "model": "server.tag", "fields": {"count": 1, "name": "accession"}}, {"pk": 18, "model": "server.tag", "fields": {"count": 1, "name": "mapping"}}, {"pk": 19, "model": "server.tag", "fields": {"count": 1, "name": "conversion"}}, {"pk": 20, "model": "server.tag", "fields": {"count": 1, "name": "shrimp"}}, {"pk": 21, "model": "server.tag", "fields": {"count": 1, "name": "sequencing"}}, {"pk": 22, "model": "server.tag", "fields": {"count": 1, "name": "short"}}, {"pk": 23, "model": "server.tag", "fields": {"count": 1, "name": "aligner"}}, {"pk": 24, "model": "server.tag", "fields": {"count": 1, "name": "meme"}}, {"pk": 25, "model": "server.tag", "fields": {"count": 1, "name": "sge"}}, {"pk": 26, "model": "server.tag", "fields": {"count": 1, "name": "compilation"}}, {"pk": 27, "model": "server.tag", "fields": {"count": 1, "name": "forum"}}, {"pk": 28, "model": "server.tag", "fields": {"count": 1, "name": "blast"}}, {"pk": 29, "model": "server.tag", "fields": {"count": 2, "name": "tutorial"}}, {"pk": 30, "model": "server.tag", "fields": {"count": 2, "name": "msu-ngs-2011"}}, {"pk": 31, "model": "server.tag", "fields": {"count": 1, "name": "bowtie"}}, {"pk": 32, "model": "server.tag", "fields": {"count": 1, "name": "bwa"}}, {"pk": 1, "model": "server.post", "fields": {"rank": 1255084100.0, "creation_date": "2009-09-30 14:12:07", "tag_val": "guidelines", "full_score": 4, "title": "Site use guidelines", "content": "Here are a few guidelines:\n\n 1. The site's goal is to answer bioinformatics and systems biology related questions\n 2. Answer questions to gain *reputation*. \n 3. Don't forget to vote for answers that you like! Registered users may vote on answers.\n 4. If you are the one asking the original question you may also select the best answer\n 5. Subscribe to the RSS feeds for all questions or a single question to keep up to date with the developments", "comment_count": 0, "score": 2, "type": 1, "status": 100, "revision_count": 0, "parent": 1, "views": 70, "answer_count": 2, "accepted": true, "slug": "site-use-guidelines", "lastedit_date": "2010-02-26 14:10:59", "url": "", "author": 2, "html": "<p>Here are a few guidelines:</p>\n<ol>\n<li>The site's goal is to answer bioinformatics and systems biology related questions</li>\n<li>Answer questions to gain <em>reputation</em>. </li>\n<li>Don't forget to vote for answers that you like! Registered users may vote on answers.</li>\n<li>If you are the one asking the original question you may also select the best answer</li>\n<li>Subscribe to the RSS feeds for all questions or a single question to keep up to date with the developments</li>\n</ol>", "tag_set": [1], "lastedit_user": 2, "root": 1}}, {"pk": 2, "model": "server.post", "fields": {"rank": 1263570494.0, "creation_date": "2009-09-30 14:55:00", "tag_val": "bed gff galaxy", "full_score": 2, "title": "How do I convert from BED format to GFF format?", "content": "I have a file in GFF format and I need to convert it to BED format. What do I do?", "comment_count": 1, "score": 0, "type": 1, "status": 100, "revision_count": 0, "parent": 2, "views": 187, "answer_count": 2, "accepted": false, "slug": "how-do-i-convert-from-bed-format-to-gff-format", "lastedit_date": "2009-09-30 15:09:43", "url": "", "author": 2, "html": "<p>I have a file in GFF format and I need to convert it to BED format. What do I do?</p>", "tag_set": [2, 3, 4], "lastedit_user": 2, "root": 2}}, {"pk": 3, "model": "server.post", "fields": {"rank": 1254344178.0, "creation_date": "2009-09-30 14:56:18", "tag_val": "", "full_score": 0, "title": "A: How do I convert from BED format to GFF format?", "content": "Both formats are tab delimited text files used to represent DNA features in genomes. The order of columns between the two are different, there are also columns that correspond to attributes missing from one or the other format. Nonetheless **the most important** difference between the two is the coordinate systems that they assume. \n\nThe [BED](http://genome.ucsc.edu/FAQ/FAQformat#format1) format developed at `UCSC` uses a zero based indexing and an open end interval whereas the [GFF](http://www.sanger.ac.uk/Software/formats/GFF/GFF_Spec.shtml) format developed at `Sanger` assumes a 1 based coordinate system that includes both start and end coordinates. Therefore\n\nThe `[0,100]` interval in `BED` format corresponds to `[1,100]` in `GFF` format and both are `100` base long. That the first element in BED format will be have the index of `0` where the last `100th` element will have the index of `99`! Whereas in `GFF` the first element will have the index of `1` and the last element will have the index of `100`.\n\nTo convert between the two you may use [Galaxy](http://main.g2.bx.psu.edu/) and select the section called `Select Formats` that will list various transformation options.\n", "comment_count": 0, "score": 1, "type": 2, "status": 100, "revision_count": 0, "parent": 2, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-how-do-i-convert-from-bed-format-to-gff-format", "lastedit_date": "2009-09-30 19:46:25", "url": "", "author": 2, "html": "<p>Both formats are tab delimited text files used to represent DNA features in genomes. The order of columns between the two are different, there are also columns that correspond to attributes missing from one or the other format. Nonetheless <strong>the most important</strong> difference between the two is the coordinate systems that they assume. </p>\n<p>The <a href=\"http://genome.ucsc.edu/FAQ/FAQformat#format1\">BED</a> format developed at <code>UCSC</code> uses a zero based indexing and an open end interval whereas the <a href=\"http://www.sanger.ac.uk/Software/formats/GFF/GFF_Spec.shtml\">GFF</a> format developed at <code>Sanger</code> assumes a 1 based coordinate system that includes both start and end coordinates. Therefore</p>\n<p>The <code>[0,100]</code> interval in <code>BED</code> format corresponds to <code>[1,100]</code> in <code>GFF</code> format and both are <code>100</code> base long. That the first element in BED format will be have the index of <code>0</code> where the last <code>100th</code> element will have the index of <code>99</code>! Whereas in <code>GFF</code> the first element will have the index of <code>1</code> and the last element will have the index of <code>100</code>.</p>\n<p>To convert between the two you may use <a href=\"http://main.g2.bx.psu.edu/\">Galaxy</a> and select the section called <code>Select Formats</code> that will list various transformation options.</p>", "tag_set": [], "lastedit_user": 2, "root": 2}}, {"pk": 4, "model": "server.post", "fields": {"rank": 1254371549.0, "creation_date": "2009-09-30 16:09:06", "tag_val": "yeast motif", "full_score": 8, "title": "Finding common motifs in sequences", "content": "I have a few hundred yeast sequences (20-80bp long) and I want to find common motifs (conserved bases at certain indices) in them. I am using a Mac", "comment_count": 0, "score": 2, "type": 1, "status": 100, "revision_count": 0, "parent": 4, "views": 150, "answer_count": 3, "accepted": false, "slug": "finding-common-motifs-in-sequences", "lastedit_date": "2009-09-30 16:09:06", "url": "", "author": 3, "html": "<p>I have a few hundred yeast sequences (20-80bp long) and I want to find common motifs (conserved bases at certain indices) in them. I am using a Mac</p>", "tag_set": [5, 6], "lastedit_user": 3, "root": 4}}, {"pk": 5, "model": "server.post", "fields": {"rank": 1254784197.0, "creation_date": "2009-09-30 16:44:22", "tag_val": "microarray clustering", "full_score": 2, "title": "Recommend easy to use microarray clustering software", "content": "Feel free to post your favorite clustering tool.", "comment_count": 0, "score": 1, "type": 1, "status": 100, "revision_count": 0, "parent": 5, "views": 154, "answer_count": 2, "accepted": false, "slug": "recommend-easy-to-use-microarray-clustering-software", "lastedit_date": "2009-10-05 16:10:10", "url": "", "author": 2, "html": "<p>Feel free to post your favorite clustering tool.</p>", "tag_set": [7, 8], "lastedit_user": 2, "root": 5}}, {"pk": 6, "model": "server.post", "fields": {"rank": 1254354579.0, "creation_date": "2009-09-30 18:49:39", "tag_val": "test", "full_score": 0, "title": "test by zhenhai", "content": "Hi, I just created my user id a few minutes ago. \n\nPost this question to see how it works.", "comment_count": 0, "score": 0, "type": 1, "status": 300, "revision_count": 0, "parent": 6, "views": 2, "answer_count": 0, "accepted": false, "slug": "test-by-zhenhai", "lastedit_date": "2009-09-30 18:57:55", "url": "", "author": 5, "html": "<p>Hi, I just created my user id a few minutes ago. </p>\n<p>Post this question to see how it works.</p>", "tag_set": [9], "lastedit_user": 2, "root": 6}}, {"pk": 7, "model": "server.post", "fields": {"rank": 1254365702.0, "creation_date": "2009-09-30 18:55:02", "tag_val": "", "full_score": 0, "title": "A: Finding common motifs in sequences", "content": "try this out?\n\nhttp://fraenkel.mit.edu/webmotifs/form.html", "comment_count": 0, "score": 3, "type": 2, "status": 100, "revision_count": 0, "parent": 4, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-finding-common-motifs-in-sequences", "lastedit_date": "2009-09-30 18:55:02", "url": "", "author": 5, "html": "<p>try this out?</p>\n<p><a href=\"http://fraenkel.mit.edu/webmotifs/form.html\">http://fraenkel.mit.edu/webmotifs/form.html</a></p>", "tag_set": [], "lastedit_user": 5, "root": 4}}, {"pk": 8, "model": "server.post", "fields": {"rank": 1254367949.0, "creation_date": "2009-09-30 19:32:29", "tag_val": "", "full_score": 0, "title": "A: Finding common motifs in sequences", "content": "You can also use MEME:  http://meme.sdsc.edu/.", "comment_count": 0, "score": 3, "type": 2, "status": 100, "revision_count": 0, "parent": 4, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-finding-common-motifs-in-sequences", "lastedit_date": "2009-09-30 19:32:29", "url": "", "author": 6, "html": "<p>You can also use MEME:  <a href=\"http://meme.sdsc.edu/.\">http://meme.sdsc.edu/.</a></p>", "tag_set": [], "lastedit_user": 6, "root": 4}}, {"pk": 9, "model": "server.post", "fields": {"rank": 1254357328.0, "creation_date": "2009-09-30 19:35:28", "tag_val": "", "full_score": 0, "title": "A: Finding common motifs in sequences", "content": "<pre>\nACGGGCCCGACGATGCGTCGTA\n\nACGTACGTCGAACCGTCGTCGT\n\nACGTGCGTCGAAACGTCAGTCG\n\nACGGGTTCGATCGTCGTCGTCG\n</pre>\n may be in Python I will break down the first sequence of required motif length into a sliding window and will search for those list of motifs in the rest of sequences using regular expression in python using `re.search()` method.", "comment_count": 1, "score": 0, "type": 2, "status": 100, "revision_count": 0, "parent": 4, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-finding-common-motifs-in-sequences", "lastedit_date": "2009-10-01 01:09:51", "url": "", "author": 7, "html": "<p>[?]</p>", "tag_set": [], "lastedit_user": 2, "root": 4}}, {"pk": 10, "model": "server.post", "fields": {"rank": 1254790981.0, "creation_date": "2009-10-05 15:51:37", "tag_val": "nucleotides", "full_score": 4, "title": "How to generate multi-nucleotide occupancy counts for each coordinate of my reads?", "content": "I need to generate nucleotide occupancy counts for each position of a given sequence then summed over each of the input sequences. An example desired output (for di-nucleotide AT):\n\n![dinucleotide occupancy][1]\n\n\n  [1]: http://github.com/ialbert/biostar-codesample/raw/master/python/images/dinuc.png", "comment_count": 0, "score": 2, "type": 1, "status": 100, "revision_count": 0, "parent": 10, "views": 18, "answer_count": 1, "accepted": true, "slug": "how-to-generate-multi-nucleotide-occupancy-counts-for-each-coordinate-of-my-reads", "lastedit_date": "2009-10-13 14:53:15", "url": "", "author": 10, "html": "<p>I need to generate nucleotide occupancy counts for each position of a given sequence then summed over each of the input sequences. An example desired output (for di-nucleotide AT):</p>\n<p><img alt=\"dinucleotide occupancy\" src=\"http://github.com/ialbert/biostar-codesample/raw/master/python/images/dinuc.png\" /></p>", "tag_set": [10], "lastedit_user": 2, "root": 10}}, {"pk": 11, "model": "server.post", "fields": {"rank": 1254783781.0, "creation_date": "2009-10-05 16:03:01", "tag_val": "", "full_score": 0, "title": "A: How to generate multi-nucleotide occupancy counts for each coordinate of my reads?", "content": "The code snippet below will populate the ``store`` dictionary keyed by the nucleotide patterns and values as lists that contain the occupancy for each index. (Updated answer now includes arbitrary lenght nucleotide counts)::\n\n\n\n    from itertools import count\n\n    def pattern_update(sequence, width=2, store={}):\n        \"\"\"\n        Accumulates nucleotide patterns of a certain width with \n        position counts at each index.\n        \"\"\"\n       \n        # open intervals need a padding at end for proper slicing\n        size  = len(sequence) + 1\n\n        def zeroes():\n            \"Generates an empty array that holds the positions\"\n            return [ 0 ] * (size - width)\n        \n        # these are the end indices\n        ends = range(width, size)\n\n        for lo, hi in zip(count(), ends):\n            # upon encoutering a missing key initialize \n            # that value for that key to the return value of the empty() function\n            key = sequence[lo:hi]\n            store.setdefault(key, zeroes())[lo] += 1\n\n        return store\n\n\n\nThe code at [multipatt.py][1] demonstrates its use in a full program. Set the ``size`` to the maximal possible sequence size. A typical use case::\n\n\n    store = {}\n    seq1 = 'ATGCT'\n    pattern_update(seq1, width=2, store=store)    \n\n    seq2 = 'ATCGC'\n    pattern_update(seq2, width=2, store=store)    \n\n    print store\n\nwill print::\n\n\n    {'CG': [0, 0, 1, 0], 'GC': [0, 0, 1, 1], 'AT': [2, 0, 0, 0], \n    'TG': [0, 1, 0, 0], 'TC': [0, 1, 0, 0], 'CT': [0, 0, 0, 1]}\n\n\n[1]: http://github.com/ialbert/biostar-codesample/blob/master/python/multipatt.py\n    ", "comment_count": 0, "score": 2, "type": 2, "status": 100, "revision_count": 0, "parent": 10, "views": 0, "answer_count": 0, "accepted": true, "slug": "a-how-to-generate-multi-nucleotide-occupancy-counts-for-each-coordinate-of-my-reads", "lastedit_date": "2009-10-13 15:00:00", "url": "", "author": 2, "html": "<p>The code snippet below will populate the <code>store</code> dictionary keyed by the nucleotide patterns and values as lists that contain the occupancy for each index. (Updated answer now includes arbitrary lenght nucleotide counts)::</p>\n<pre><code>from itertools import count\n\ndef pattern_update(sequence, width=2, store={}):\n    \"\"\"\n    Accumulates nucleotide patterns of a certain width with \n    position counts at each index.\n    \"\"\"\n\n    # open intervals need a padding at end for proper slicing\n    size  = len(sequence) + 1\n\n    def zeroes():\n        \"Generates an empty array that holds the positions\"\n        return [ 0 ] * (size - width)\n\n    # these are the end indices\n    ends = range(width, size)\n\n    for lo, hi in zip(count(), ends):\n        # upon encoutering a missing key initialize \n        # that value for that key to the return value of the empty() function\n        key = sequence[lo:hi]\n        store.setdefault(key, zeroes())[lo] += 1\n\n    return store\n</code></pre>\n<p>The code at <a href=\"http://github.com/ialbert/biostar-codesample/blob/master/python/multipatt.py\">multipatt.py</a> demonstrates its use in a full program. Set the <code>size</code> to the maximal possible sequence size. A typical use case::</p>\n<pre><code>store = {}\nseq1 = 'ATGCT'\npattern_update(seq1, width=2, store=store)\n\nseq2 = 'ATCGC'\npattern_update(seq2, width=2, store=store)\n\nprint store\n</code></pre>\n<p>will print::</p>\n<pre><code>{'CG': [0, 0, 1, 0], 'GC': [0, 0, 1, 1], 'AT': [2, 0, 0, 0], \n'TG': [0, 1, 0, 0], 'TC': [0, 1, 0, 0], 'CT': [0, 0, 0, 1]}\n</code></pre>", "tag_set": [], "lastedit_user": 2, "root": 10}}, {"pk": 12, "model": "server.post", "fields": {"rank": 1254780597.0, "creation_date": "2009-10-05 16:09:57", "tag_val": "", "full_score": 0, "title": "A: Recommend easy to use microarray clustering software", "content": "One of my favorites is the [MEV](http://www.tm4.org/mev.html) micro-array data analysis tool.\nIt is simple to use and it has a very large number of features. \n\nWorks well for any type of data. You can also load into it data from a file that is in a simple text format:\n\n<pre>\nGENE1, value1, value2\nGENE2, value1, value2\n</pre>\n\nFeel free to post your favorite clustering tool.", "comment_count": 0, "score": 1, "type": 2, "status": 100, "revision_count": 0, "parent": 5, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-recommend-easy-to-use-microarray-clustering-software", "lastedit_date": "2009-10-05 16:09:57", "url": "", "author": 2, "html": "<p>One of my favorites is the <a href=\"http://www.tm4.org/mev.html\">MEV</a> micro-array data analysis tool.\nIt is simple to use and it has a very large number of features. </p>\n<p>Works well for any type of data. You can also load into it data from a file that is in a simple text format:</p>\n<p>[?]</p>\n<p>Feel free to post your favorite clustering tool.</p>", "tag_set": [], "lastedit_user": 2, "root": 5}}, {"pk": 13, "model": "server.post", "fields": {"rank": 1254888281.0, "creation_date": "2009-10-06 18:58:10", "tag_val": "solid deep sequence", "full_score": 4, "title": "CHIP DNA deep sequence", "content": "Hi, everyone,\nI am posting this question for my friend.\nHe is analyzing his CHIP DNA solid deep sequence data, and find out that near 80% reads can not be mapped to the human genome. We are wondering if this high percentage unmapped reads is normal in CHIP DNA deep sequence or there may be something wrong with his result.", "comment_count": 0, "score": 2, "type": 1, "status": 100, "revision_count": 0, "parent": 13, "views": 72, "answer_count": 3, "accepted": false, "slug": "chip-dna-deep-sequence", "lastedit_date": "2009-10-06 18:58:10", "url": "", "author": 12, "html": "<p>Hi, everyone,\nI am posting this question for my friend.\nHe is analyzing his CHIP DNA solid deep sequence data, and find out that near 80% reads can not be mapped to the human genome. We are wondering if this high percentage unmapped reads is normal in CHIP DNA deep sequence or there may be something wrong with his result.</p>", "tag_set": [11, 12, 13], "lastedit_user": 12, "root": 13}}, {"pk": 14, "model": "server.post", "fields": {"rank": 1254877832.0, "creation_date": "2009-10-06 20:10:32", "tag_val": "", "full_score": 0, "title": "A: CHIP DNA deep sequence", "content": "I recall that our first samples that we ran on the Solid sequencer have had bad performance. Not quite an 80% loss but around 40%-60% reads were unmappable (yeast). Some other lab members will hopefully chime in with more details. ", "comment_count": 0, "score": 0, "type": 2, "status": 100, "revision_count": 0, "parent": 13, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-chip-dna-deep-sequence", "lastedit_date": "2009-10-06 20:10:32", "url": "", "author": 2, "html": "<p>I recall that our first samples that we ran on the Solid sequencer have had bad performance. Not quite an 80% loss but around 40%-60% reads were unmappable (yeast). Some other lab members will hopefully chime in with more details.</p>", "tag_set": [], "lastedit_user": 2, "root": 13}}, {"pk": 15, "model": "server.post", "fields": {"rank": 1254883284.0, "creation_date": "2009-10-06 20:41:24", "tag_val": "", "full_score": 0, "title": "A: CHIP DNA deep sequence", "content": "Hi there,\n\nWe have done numbers of SOLiD sequencing run on yeast samples. Normally there are only 30-40 percent of total tags can be uniquely mapped back to yeast genome. \n\nWhat I would recommend is do it on solexa. You get much higher quality tags.\n\ncheers,", "comment_count": 0, "score": 1, "type": 2, "status": 100, "revision_count": 0, "parent": 13, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-chip-dna-deep-sequence", "lastedit_date": "2009-10-06 20:41:24", "url": "", "author": 5, "html": "<p>Hi there,</p>\n<p>We have done numbers of SOLiD sequencing run on yeast samples. Normally there are only 30-40 percent of total tags can be uniquely mapped back to yeast genome. </p>\n<p>What I would recommend is do it on solexa. You get much higher quality tags.</p>\n<p>cheers,</p>", "tag_set": [], "lastedit_user": 5, "root": 13}}, {"pk": 16, "model": "server.post", "fields": {"rank": 1254888281.0, "creation_date": "2009-10-06 22:04:41", "tag_val": "", "full_score": 0, "title": "A: CHIP DNA deep sequence", "content": "Your 20% mapping yield looks like low for normal ChIP experiment, even for human. Several factors can reduce this mapping yield. I am wondering which kind of ChIP was used in your case. That is, which kind of proteins was ChIPed?", "comment_count": 0, "score": 1, "type": 2, "status": 100, "revision_count": 0, "parent": 13, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-chip-dna-deep-sequence", "lastedit_date": "2009-10-06 22:04:41", "url": "", "author": 13, "html": "<p>Your 20% mapping yield looks like low for normal ChIP experiment, even for human. Several factors can reduce this mapping yield. I am wondering which kind of ChIP was used in your case. That is, which kind of proteins was ChIPed?</p>", "tag_set": [], "lastedit_user": 13, "root": 13}}, {"pk": 17, "model": "server.post", "fields": {"rank": 1254955304.0, "creation_date": "2009-10-07 16:41:44", "tag_val": "", "full_score": 0, "title": "A: Site use guidelines", "content": "If you are shy about asking the question on your own behalf submit it to to the **Question Bot** and it will be posted anonymously. Send email to the `Question Bot` link at the bottom.", "comment_count": 0, "score": 1, "type": 2, "status": 100, "revision_count": 0, "parent": 1, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-site-use-guidelines", "lastedit_date": "2009-10-07 17:16:18", "url": "", "author": 10, "html": "<p>If you are shy about asking the question on your own behalf submit it to to the <strong>Question Bot</strong> and it will be posted anonymously. Send email to the <code>Question Bot</code> link at the bottom.</p>", "tag_set": [], "lastedit_user": 2, "root": 1}}, {"pk": 18, "model": "server.post", "fields": {"rank": 1255080500.0, "creation_date": "2009-10-09 03:28:20", "tag_val": "", "full_score": 0, "title": "A: Site use guidelines", "content": "Hi,\n\nI don't think a new user can vote on a question or an answer.\nThe site says I need 15 reputation...", "comment_count": 1, "score": 1, "type": 2, "status": 100, "revision_count": 0, "parent": 1, "views": 0, "answer_count": 0, "accepted": true, "slug": "a-site-use-guidelines", "lastedit_date": "2009-10-09 03:28:20", "url": "", "author": 14, "html": "<p>Hi,</p>\n<p>I don't think a new user can vote on a question or an answer.\nThe site says I need 15 reputation...</p>", "tag_set": [], "lastedit_user": 14, "root": 1}}, {"pk": 19, "model": "server.post", "fields": {"rank": 1255713938.0, "creation_date": "2009-10-16 12:25:38", "tag_val": "", "full_score": 0, "title": "A: Recommend easy to use microarray clustering software", "content": "I would recommend a combination of cluster and treeview.\n\npretty powerful!", "comment_count": 0, "score": 0, "type": 2, "status": 100, "revision_count": 0, "parent": 5, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-recommend-easy-to-use-microarray-clustering-software", "lastedit_date": "2009-10-16 12:25:38", "url": "", "author": 5, "html": "<p>I would recommend a combination of cluster and treeview.</p>\n<p>pretty powerful!</p>", "tag_set": [], "lastedit_user": 5, "root": 5}}, {"pk": 20, "model": "server.post", "fields": {"rank": 1255854173.0, "creation_date": "2009-10-18 03:22:53", "tag_val": "boy george", "full_score": 0, "title": "do you have to be a guy to dress up as boy george", "content": "any ideas im a girl", "comment_count": 0, "score": 0, "type": 1, "status": 300, "revision_count": 0, "parent": 20, "views": 0, "answer_count": 0, "accepted": false, "slug": "do-you-have-to-be-a-guy-to-dress-up-as-boy-george", "lastedit_date": "2009-10-18 16:23:04", "url": "", "author": 15, "html": "<p>any ideas im a girl</p>", "tag_set": [14, 15], "lastedit_user": 2, "root": 20}}, {"pk": 21, "model": "server.post", "fields": {"rank": 1255854214.0, "creation_date": "2009-10-18 03:23:34", "tag_val": "boy george", "full_score": 0, "title": "do you have to be a guy to dress up as boy george", "content": "any ideas im a girl", "comment_count": 0, "score": 0, "type": 1, "status": 300, "revision_count": 0, "parent": 21, "views": 0, "answer_count": 0, "accepted": false, "slug": "do-you-have-to-be-a-guy-to-dress-up-as-boy-george", "lastedit_date": "2009-10-18 16:25:02", "url": "", "author": 15, "html": "<p>any ideas im a girl</p>", "tag_set": [14, 15], "lastedit_user": 2, "root": 21}}, {"pk": 22, "model": "server.post", "fields": {"rank": 1260341154.0, "creation_date": "2009-10-23 17:42:24", "tag_val": "geneid accession mapping conversion", "full_score": 4, "title": "Gene ID conversion tool", "content": "Hey,\n\nI was using DAVID (http://david.abcc.ncifcrf.gov/conversion.jsp) to do the gene ID conversion, e.g.conversion between Agilent ID, Genebank accession id and Entrez gene ID, but I found the DAVID database is not updated. Does anyone know a better updataed conversion tool to do this job? Thanks! ", "comment_count": 0, "score": 2, "type": 1, "status": 100, "revision_count": 0, "parent": 22, "views": 493, "answer_count": 2, "accepted": false, "slug": "gene-id-conversion-tool", "lastedit_date": "2009-10-23 17:42:24", "url": "", "author": 16, "html": "<p>Hey,</p>\n<p>I was using DAVID (<a href=\"http://david.abcc.ncifcrf.gov/conversion.jsp\">http://david.abcc.ncifcrf.gov/conversion.jsp</a>) to do the gene ID conversion, e.g.conversion between Agilent ID, Genebank accession id and Entrez gene ID, but I found the DAVID database is not updated. Does anyone know a better updataed conversion tool to do this job? Thanks!</p>", "tag_set": [16, 17, 18, 19], "lastedit_user": 16, "root": 22}}, {"pk": 23, "model": "server.post", "fields": {"rank": 1256345205.0, "creation_date": "2009-10-23 19:46:45", "tag_val": "", "full_score": 0, "title": "A: Gene ID conversion tool", "content": "I don't know of a direct solution myself, but this is a topic that may be of interest for the biological data analysis class that I am teaching. \n\nIf you specify the organism/genomic builds that you are interested in we may be able to generate a full translation list as an in class example or a homework. I was planning on covering an `Affymetrix ID` to `Genebank example` anyhow.\n", "comment_count": 2, "score": 0, "type": 2, "status": 100, "revision_count": 0, "parent": 22, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-gene-id-conversion-tool", "lastedit_date": "2009-10-23 19:46:45", "url": "", "author": 2, "html": "<p>I don't know of a direct solution myself, but this is a topic that may be of interest for the biological data analysis class that I am teaching. </p>\n<p>If you specify the organism/genomic builds that you are interested in we may be able to generate a full translation list as an in class example or a homework. I was planning on covering an <code>Affymetrix ID</code> to <code>Genebank example</code> anyhow.</p>", "tag_set": [], "lastedit_user": 2, "root": 22}}, {"pk": 24, "model": "server.post", "fields": {"rank": 1259715653.0, "creation_date": "2009-12-01 07:13:53", "tag_val": "shrimp sequencing short aligner", "full_score": 4, "title": "How to set SHRiMP parameters for best sensitivity with 35bp colorspace data?", "content": "Hi,\n\nI have 35bp Solid colorspace sequencing data, and the actual sequences to be mapped are 20-25bp after removing the linker sequence.\n\nI hope to find all the hits allowing no more than n mismatches (say n=3), not only the best hit.\n\nI know there is a -M option to specify -M sensitivity,35bp. I wonder whether this setting will guarantee the best sensitivity in this case. Since my reads are only 20-25bp long, should I changed the default 4 spaced seeds to 3?\n\nI'm new to SHRiMP, so I'd like to hear some suggestions on setting the parameters of SHRiMP to achieve the best sensitivity.\n\nThank you!", "comment_count": 0, "score": 1, "type": 1, "status": 100, "revision_count": 0, "parent": 24, "views": 32, "answer_count": 2, "accepted": false, "slug": "how-to-set-shrimp-parameters-for-best-sensitivity-with-35bp-colorspace-data", "lastedit_date": "2009-12-01 07:13:53", "url": "", "author": 14, "html": "<p>Hi,</p>\n<p>I have 35bp Solid colorspace sequencing data, and the actual sequences to be mapped are 20-25bp after removing the linker sequence.</p>\n<p>I hope to find all the hits allowing no more than n mismatches (say n=3), not only the best hit.</p>\n<p>I know there is a -M option to specify -M sensitivity,35bp. I wonder whether this setting will guarantee the best sensitivity in this case. Since my reads are only 20-25bp long, should I changed the default 4 spaced seeds to 3?</p>\n<p>I'm new to SHRiMP, so I'd like to hear some suggestions on setting the parameters of SHRiMP to achieve the best sensitivity.</p>\n<p>Thank you!</p>", "tag_set": [20, 21, 22, 23], "lastedit_user": 14, "root": 24}}, {"pk": 25, "model": "server.post", "fields": {"rank": 1259708255.0, "creation_date": "2009-12-01 14:57:35", "tag_val": "", "full_score": 0, "title": "A: How to set SHRiMP parameters for best sensitivity with 35bp colorspace data?", "content": "I just read the SHRiMP manual again, but I think that their explanation about -M option may not be enough to answer your question. I usually use the \"seed\" mode by using -s, -n, and -w and the option -M is a new feature of the version 1.3.1, which I have never tried before.\n\nI recommend for you to use the \"seed\" mode--the default would be good, but please adjust the -s option if you want more sensitivity. Always fast speed compensates sensitivity and the -M option seems to exist for this purpose.\n\nHope my message to be helpful for your project.", "comment_count": 0, "score": 2, "type": 2, "status": 100, "revision_count": 0, "parent": 24, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-how-to-set-shrimp-parameters-for-best-sensitivity-with-35bp-colorspace-data", "lastedit_date": "2009-12-01 14:57:35", "url": "", "author": 18, "html": "<p>I just read the SHRiMP manual again, but I think that their explanation about -M option may not be enough to answer your question. I usually use the \"seed\" mode by using -s, -n, and -w and the option -M is a new feature of the version 1.3.1, which I have never tried before.</p>\n<p>I recommend for you to use the \"seed\" mode--the default would be good, but please adjust the -s option if you want more sensitivity. Always fast speed compensates sensitivity and the -M option seems to exist for this purpose.</p>\n<p>Hope my message to be helpful for your project.</p>", "tag_set": [], "lastedit_user": 18, "root": 24}}, {"pk": 26, "model": "server.post", "fields": {"rank": 1259715653.0, "creation_date": "2009-12-01 18:00:53", "tag_val": "", "full_score": 0, "title": "A: How to set SHRiMP parameters for best sensitivity with 35bp colorspace data?", "content": "> Since my reads are only 20-25bp long,\n> should I changed the default 4 spaced\n> seeds to 3?\n\nwhile the shrimp manual says:\n\n- We recommend using the default 4 seeds of weight 12 in most cases.\n\nyou could try running on a smaller sample and see what happens. ", "comment_count": 0, "score": 1, "type": 2, "status": 100, "revision_count": 0, "parent": 24, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-how-to-set-shrimp-parameters-for-best-sensitivity-with-35bp-colorspace-data", "lastedit_date": "2009-12-01 18:00:53", "url": "", "author": 2, "html": "<blockquote>\n<p>Since my reads are only 20-25bp long,\nshould I changed the default 4 spaced\nseeds to 3?</p>\n</blockquote>\n<p>while the shrimp manual says:</p>\n<ul>\n<li>We recommend using the default 4 seeds of weight 12 in most cases.</li>\n</ul>\n<p>you could try running on a smaller sample and see what happens.</p>", "tag_set": [], "lastedit_user": 2, "root": 24}}, {"pk": 27, "model": "server.post", "fields": {"rank": 1260337554.0, "creation_date": "2009-12-08 21:45:54", "tag_val": "", "full_score": 0, "title": "A: Gene ID conversion tool", "content": "The following link has a list of ID conversion tools:\n\nhttp://hum-molgen.org/NewsGen/08-2009/000020.html", "comment_count": 0, "score": 2, "type": 2, "status": 100, "revision_count": 0, "parent": 22, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-gene-id-conversion-tool", "lastedit_date": "2009-12-08 21:45:54", "url": "", "author": 19, "html": "<p>The following link has a list of ID conversion tools:</p>\n<p><a href=\"http://hum\">http://hum</a>-molgen.org/NewsGen/08-2009/000020.html</p>", "tag_set": [], "lastedit_user": 19, "root": 22}}, {"pk": 28, "model": "server.post", "fields": {"rank": 1263412762.0, "creation_date": "2010-01-13 12:59:22", "tag_val": "meme sge motif motif compilation", "full_score": 1, "title": "Tips on compiling and using MEME 4.3 with a Sun Grid Engine computation cluster", "content": "Has anyone compiled and used MEME 4.x for use in a parallel computation environment, based upon operation with a Sun Grid Engine (SGE) cluster?\n\nI can compile the suite and its tests pass. However, when I attempt to use the `-p n` option, to specify `n` computation nodes, I get several error messages:\n\n    /gridware/codine/util/arch: Command not found.\n    /gridware/codine/util/arch: Command not found.\n    /gridware/codine/util/arch: Command not found.\n    /gridware/codine/util/arch: Command not found.\n    1: Command not found.\n\nWe do not have `/gridware/codine/util/arch`, but we do have `/gridengine/sgi/util/arch`.\n\nI tried looking around MEME's source code, particularly at `meme.c` and `mp.h`, but there are no references to these paths.\n\nI'm wondering if I am missing makefile directives. Here is my `./configure` statement:\n\n    ./configure --prefix=/home/areynolds/proj/meme/meme_4.3.0_build --with-url=\"http://meme.nbcr.net/meme\" --enable-openmp --enable-debug\n\nIs MPI a requirement; are there directives I am missing for MPI?\n\nThank you for any advice.", "comment_count": 0, "score": 1, "type": 1, "status": 100, "revision_count": 0, "parent": 28, "views": 59, "answer_count": 1, "accepted": true, "slug": "tips-on-compiling-and-using-meme-43-with-a-sun-grid-engine-computation-cluster", "lastedit_date": "2010-01-13 12:59:22", "url": "", "author": 20, "html": "<p>Has anyone compiled and used MEME 4.x for use in a parallel computation environment, based upon operation with a Sun Grid Engine (SGE) cluster?</p>\n<p>I can compile the suite and its tests pass. However, when I attempt to use the <code>-p n</code> option, to specify <code>n</code> computation nodes, I get several error messages:</p>\n<pre><code>/gridware/codine/util/arch: Command not found.\n/gridware/codine/util/arch: Command not found.\n/gridware/codine/util/arch: Command not found.\n/gridware/codine/util/arch: Command not found.\n1: Command not found.\n</code></pre>\n<p>We do not have <code>/gridware/codine/util/arch</code>, but we do have <code>/gridengine/sgi/util/arch</code>.</p>\n<p>I tried looking around MEME's source code, particularly at <code>meme.c</code> and <code>mp.h</code>, but there are no references to these paths.</p>\n<p>I'm wondering if I am missing makefile directives. Here is my <code>./configure</code> statement:</p>\n<pre><code>./configure --prefix=/home/areynolds/proj/meme/meme_4.3.0_build --with-url=\"http://meme.nbcr.net/meme\" --enable-openmp --enable-debug\n</code></pre>\n<p>Is MPI a requirement; are there directives I am missing for MPI?</p>\n<p>Thank you for any advice.</p>", "tag_set": [6, 24, 25, 26], "lastedit_user": 20, "root": 28}}, {"pk": 29, "model": "server.post", "fields": {"rank": 1263439070.0, "creation_date": "2010-01-13 21:17:50", "tag_val": "", "full_score": 0, "title": "A: Tips on compiling and using MEME 4.3 with a Sun Grid Engine computation cluster", "content": "This may not be overly useful but it very much sounds like a configuration problem.\n\nUsually there is  configure flag that needs to be set to point to the libraries, something like:\n\n    --with-mpidir=MPIDIR\n    --with-mpicc=MPICC\n\nIt also appears that the MEME suite does not support Open MPI (as per [install notes][1]).\n\nI would also recommend posting on the MEME user forum:\n\n[https://www.nbcr.net/forum/viewforum.php?f=5][2]\n\n\n  [1]: http://meme.sdsc.edu/meme4/meme-install.html\n  [2]: https://www.nbcr.net/forum/viewforum.php?f=5", "comment_count": 0, "score": 0, "type": 2, "status": 100, "revision_count": 0, "parent": 28, "views": 0, "answer_count": 0, "accepted": true, "slug": "a-tips-on-compiling-and-using-meme-43-with-a-sun-grid-engine-computation-cluster", "lastedit_date": "2010-01-13 21:17:50", "url": "", "author": 2, "html": "<p>This may not be overly useful but it very much sounds like a configuration problem.</p>\n<p>Usually there is  configure flag that needs to be set to point to the libraries, something like:</p>\n<pre><code>--with-mpidir=MPIDIR\n--with-mpicc=MPICC\n</code></pre>\n<p>It also appears that the MEME suite does not support Open MPI (as per <a href=\"http://meme.sdsc.edu/meme4/meme-install.html\">install notes</a>).</p>\n<p>I would also recommend posting on the MEME user forum:</p>\n<p><a href=\"https://www.nbcr.net/forum/viewforum.php?f=5\"><a href=\"https://www.nbcr.net/forum/viewforum.php?f=5\"><a href=\"https://www.nbcr.net/forum/viewforum.php?f=5\">https://www.nbcr.net/forum/viewforum.php?f=5</a></a></a></p>", "tag_set": [], "lastedit_user": 2, "root": 28}}, {"pk": 30, "model": "server.post", "fields": {"rank": 1263570494.0, "creation_date": "2010-01-15 08:48:14", "tag_val": "", "full_score": 0, "title": "A: How do I convert from BED format to GFF format?", "content": "Here's a Perl script I wrote if you wanted to do something local. \n\nThere's some code in there for translating yeast chromosome names that can be removed, if not needed. I also used a `Site` feature in the GFF file as the region ID, which might also need tweaking, depending on what features you're interested in.\n\n    #!/usr/bin/perl -w\n\n    use strict;\n    use Bio::Tools::GFF;\n    use feature qw(say switch);\n\n    my $gffio = Bio::Tools::GFF->new(-fh => \\*STDIN, -gff_version => 2);\n    my $feature;\n\n    while ($feature = $gffio->next_feature()) {\n        # print $gffio->gff_string($feature).\"\\n\";\n\n        # cf. http://www.sanger.ac.uk/Software/formats/GFF/GFF_Spec.shtml\n        my $seq_id = $feature->seq_id();   \n        my $start = $feature->start() - 1;\n        my $end = $feature->end();\n        my $strand = $feature->strand();\n        my @sites = $feature->get_tag_values('Site');\n\n        # translate strand\n        given ( $strand ) {\n            when ($_ == 1)  { $strand = \"+\"; }\n            when ($_ == -1) { $strand = \"-\"; }\n        }\n    \n        # translate yeast chromosome to UCSC browser-readable chromosome\n        # cf. http://www.yeastgenome.org/sgdpub/Saccharomyces_cerevisiae.pdf\n        given ( $seq_id ) {\n            when ( $_ eq \"I\" )    { $seq_id = \"chr1\"; }\n            when ( $_ eq \"II\" )   { $seq_id = \"chr2\"; }\n            when ( $_ eq \"III\" )  { $seq_id = \"chr3\"; }\n            when ( $_ eq \"IV\" )   { $seq_id = \"chr4\"; }\n            when ( $_ eq \"V\" )    { $seq_id = \"chr5\"; }\n            when ( $_ eq \"VI\" )   { $seq_id = \"chr6\"; }\n            when ( $_ eq \"VII\" )  { $seq_id = \"chr7\"; }\n            when ( $_ eq \"VIII\" ) { $seq_id = \"chr8\"; }\n            when ( $_ eq \"IX\" )   { $seq_id = \"chr9\"; }\n            when ( $_ eq \"X\" )    { $seq_id = \"chr10\"; }\n            when ( $_ eq \"XI\" )   { $seq_id = \"chr11\"; }\n            when ( $_ eq \"XII\" )  { $seq_id = \"chr12\"; }\n            when ( $_ eq \"XIII\" ) { $seq_id = \"chr13\"; }\n            when ( $_ eq \"XIV\" )  { $seq_id = \"chr14\"; }\n            when ( $_ eq \"XV\" )   { $seq_id = \"chr15\"; }\n            when ( $_ eq \"XVI\" )  { $seq_id = \"chr16\"; }\n            default { }\n        }\n\n        # output\n        print \"$seq_id\\t$start\\t$end\\t$sites[0]\\t0.0\\t$strand\\n\";\n    }\n    $gffio->close();\n\nTo use it:\n\n    gff2bed.pl < data.gff > data.bed", "comment_count": 2, "score": 1, "type": 2, "status": 100, "revision_count": 0, "parent": 2, "views": 0, "answer_count": 0, "accepted": false, "slug": "a-how-do-i-convert-from-bed-format-to-gff-format", "lastedit_date": "2010-01-15 08:48:14", "url": "", "author": 20, "html": "<p>Here's a Perl script I wrote if you wanted to do something local. </p>\n<p>There's some code in there for translating yeast chromosome names that can be removed, if not needed. I also used a <code>Site</code> feature in the GFF file as the region ID, which might also need tweaking, depending on what features you're interested in.</p>\n<pre><code>#!/usr/bin/perl -w\n\nuse strict;\nuse Bio::Tools::GFF;\nuse feature qw(say switch);\n\nmy $gffio = Bio::Tools::GFF-&gt;new(-fh =&gt; \\*STDIN, -gff_version =&gt; 2);\nmy $feature;\n\nwhile ($feature = $gffio-&gt;next_feature()) {\n    # print $gffio-&gt;gff_string($feature).\"\\n\";\n\n    # cf. http://www.sanger.ac.uk/Software/formats/GFF/GFF_Spec.shtml\n    my $seq_id = $feature-&gt;seq_id();   \n    my $start = $feature-&gt;start() - 1;\n    my $end = $feature-&gt;end();\n    my $strand = $feature-&gt;strand();\n    my @sites = $feature-&gt;get_tag_values('Site');\n\n    # translate strand\n    given ( $strand ) {\n        when ($_ == 1)  { $strand = \"+\"; }\n        when ($_ == -1) { $strand = \"-\"; }\n    }\n\n    # translate yeast chromosome to UCSC browser-readable chromosome\n    # cf. http://www.yeastgenome.org/sgdpub/Saccharomyces_cerevisiae.pdf\n    given ( $seq_id ) {\n        when ( $_ eq \"I\" )    { $seq_id = \"chr1\"; }\n        when ( $_ eq \"II\" )   { $seq_id = \"chr2\"; }\n        when ( $_ eq \"III\" )  { $seq_id = \"chr3\"; }\n        when ( $_ eq \"IV\" )   { $seq_id = \"chr4\"; }\n        when ( $_ eq \"V\" )    { $seq_id = \"chr5\"; }\n        when ( $_ eq \"VI\" )   { $seq_id = \"chr6\"; }\n        when ( $_ eq \"VII\" )  { $seq_id = \"chr7\"; }\n        when ( $_ eq \"VIII\" ) { $seq_id = \"chr8\"; }\n        when ( $_ eq \"IX\" )   { $seq_id = \"chr9\"; }\n        when ( $_ eq \"X\" )    { $seq_id = \"chr10\"; }\n        when ( $_ eq \"XI\" )   { $seq_id = \"chr11\"; }\n        when ( $_ eq \"XII\" )  { $seq_id = \"chr12\"; }\n        when ( $_ eq \"XIII\" ) { $seq_id = \"chr13\"; }\n        when ( $_ eq \"XIV\" )  { $seq_id = \"chr14\"; }\n        when ( $_ eq \"XV\" )   { $seq_id = \"chr15\"; }\n        when ( $_ eq \"XVI\" )  { $seq_id = \"chr16\"; }\n        default { }\n    }\n\n    # output\n    print \"$seq_id\\t$start\\t$end\\t$sites[0]\\t0.0\\t$strand\\n\";\n}\n$gffio-&gt;close();\n</code></pre>\n<p>To use it:</p>\n<pre><code>gff2bed.pl &lt; data.gff &gt; data.bed\n</code></pre>", "tag_set": [], "lastedit_user": 20, "root": 2}}, {"pk": 31, "model": "server.post", "fields": {"rank": 1254340608.0, "creation_date": "2009-09-30 14:56:48", "tag_val": "", "full_score": 0, "title": "C: How do I convert from BED format to GFF format?", "content": "I'll answer my own question here as it is a demo for now", "comment_count": 0, "score": 0, "type": 3, "status": 100, "revision_count": 0, "parent": 2, "views": 0, "answer_count": 0, "accepted": false, "slug": "c-how-do-i-convert-from-bed-format-to-gff-format", "lastedit_date": "2012-03-28 08:29:12", "url": "", "author": 2, "html": "<p>I'll answer my own question here as it is a demo for now</p>", "tag_set": [], "lastedit_user": 2, "root": 2}}, {"pk": 32, "model": "server.post", "fields": {"rank": 1254357838.0, "creation_date": "2009-09-30 19:43:58", "tag_val": "", "full_score": 0, "title": "C: A: Finding common motifs in sequences", "content": "post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right)", "comment_count": 0, "score": 0, "type": 3, "status": 100, "revision_count": 0, "parent": 9, "views": 0, "answer_count": 0, "accepted": false, "slug": "c-a-finding-common-motifs-in-sequences", "lastedit_date": "2012-03-28 08:29:12", "url": "", "author": 2, "html": "<p>post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right)</p>", "tag_set": [], "lastedit_user": 2, "root": 4}}, {"pk": 33, "model": "server.post", "fields": {"rank": 1255125443.0, "creation_date": "2009-10-09 16:57:23", "tag_val": "", "full_score": 0, "title": "C: A: Site use guidelines", "content": "I see, then I guess they will need to contribute first in some way to gain some reputation first. ", "comment_count": 0, "score": 0, "type": 3, "status": 100, "revision_count": 0, "parent": 18, "views": 0, "answer_count": 0, "accepted": false, "slug": "c-a-site-use-guidelines", "lastedit_date": "2012-03-28 08:29:12", "url": "", "author": 2, "html": "<p>I see, then I guess they will need to contribute first in some way to gain some reputation first.</p>", "tag_set": [], "lastedit_user": 2, "root": 1}}, {"pk": 34, "model": "server.post", "fields": {"rank": 1256765920.0, "creation_date": "2009-10-28 16:38:40", "tag_val": "", "full_score": 0, "title": "C: A: Gene ID conversion tool", "content": "Thanks! That's great! But I'm not student there...Can I access to that anyway? I am using Human whole genome Agilent array. Thank you so much. ", "comment_count": 0, "score": 0, "type": 3, "status": 100, "revision_count": 0, "parent": 23, "views": 0, "answer_count": 0, "accepted": false, "slug": "c-a-gene-id-conversion-tool", "lastedit_date": "2012-03-28 08:29:12", "url": "", "author": 16, "html": "<p>Thanks! That's great! But I'm not student there...Can I access to that anyway? I am using Human whole genome Agilent array. Thank you so much.</p>", "tag_set": [], "lastedit_user": 16, "root": 22}}, {"pk": 35, "model": "server.post", "fields": {"rank": 1264194898.0, "creation_date": "2010-01-22 15:14:58", "tag_val": "", "full_score": 0, "title": "C: A: How do I convert from BED format to GFF format?", "content": "Just a note: code above need bioperl", "comment_count": 0, "score": 0, "type": 3, "status": 100, "revision_count": 0, "parent": 30, "views": 0, "answer_count": 0, "accepted": false, "slug": "c-a-how-do-i-convert-from-bed-format-to-gff-format", "lastedit_date": "2012-03-28 08:29:12", "url": "", "author": 2, "html": "<p>Just a note: code above need bioperl</p>", "tag_set": [], "lastedit_user": 2, "root": 2}}, {"pk": 36, "model": "server.post", "fields": {"rank": 1264623622.0, "creation_date": "2010-01-27 14:20:22", "tag_val": "", "full_score": 0, "title": "C: A: Gene ID conversion tool", "content": "missed this comment, sorry about it!", "comment_count": 0, "score": 0, "type": 3, "status": 100, "revision_count": 0, "parent": 23, "views": 0, "answer_count": 0, "accepted": false, "slug": "c-a-gene-id-conversion-tool", "lastedit_date": "2012-03-28 08:29:12", "url": "", "author": 2, "html": "<p>missed this comment, sorry about it!</p>", "tag_set": [], "lastedit_user": 2, "root": 22}}, {"pk": 37, "model": "server.post", "fields": {"rank": 1264803464.0, "creation_date": "2010-01-29 16:17:44", "tag_val": "", "full_score": 0, "title": "C: A: How do I convert from BED format to GFF format?", "content": "Hi Alex, one thing you could do it replace the case block with a hash map that remaps chromosomes. That way it is a lot easier to add other entries withouth make the code longer and longer...", "comment_count": 0, "score": 0, "type": 3, "status": 100, "revision_count": 0, "parent": 30, "views": 0, "answer_count": 0, "accepted": false, "slug": "c-a-how-do-i-convert-from-bed-format-to-gff-format", "lastedit_date": "2012-03-28 08:29:12", "url": "", "author": 2, "html": "<p>Hi Alex, one thing you could do it replace the case block with a hash map that remaps chromosomes. That way it is a lot easier to add other entries withouth make the code longer and longer...</p>", "tag_set": [], "lastedit_user": 2, "root": 2}}, {"pk": 38, "model": "server.post", "fields": {"rank": 1332941353.0, "creation_date": "2012-03-28 08:29:13", "tag_val": "forum guidelines", "full_score": 0, "title": "Biostar forum posting guidelines", "content": "Biostar is divided into several sections:\n\nThe **Questions** tab is meant to help users find answers to well defined and specific bioinformatics questions. Most importantly the tile must be formulated as a question.\n\nThe **Forum** tab displays all other generic posts that do not have a designated location. Note that while the **Forum** post requirements are more relaxed the topic of the posts must still be _bioinformatics_ or _biological data analysis_\n\nThe **Planet** tab contains blog posts by existing users as well as short summaries and links to external blog sites. For external blog posts only a short description is shown, to read the full post follow the link. \n\nThe **Guide** tab contains guides and tutorial posts that describe a particular technique or methodology. \n\nModerators may reclassify posts if they believe to be better suited to a different section. To reach the moderators you may use the comments below each post or  see the [About](/about/ \"About page\") page for contact information.\n\nLogged in users get small notification in the tab displaying the number of new posts in a tab since their last visited.", "comment_count": 0, "score": 0, "type": 6, "status": 100, "revision_count": 1, "parent": 38, "views": 0, "answer_count": 0, "accepted": false, "slug": "biostar-forum-posting-guidelines", "lastedit_date": "2012-03-28 08:29:13", "url": "", "author": 31, "html": "<p>Biostar is divided into several sections:</p>\n<p>The <strong>Questions</strong> tab is meant to help users find answers to well defined and specific bioinformatics questions. Most importantly the tile must be formulated as a question.</p>\n<p>The <strong>Forum</strong> tab displays all other generic posts that do not have a designated location. Note that while the <strong>Forum</strong> post requirements are more relaxed the topic of the posts must still be <em>bioinformatics</em> or <em>biological data analysis</em></p>\n<p>The <strong>Planet</strong> tab contains blog posts by existing users as well as short summaries and links to external blog sites. For external blog posts only a short description is shown, to read the full post follow the link. </p>\n<p>The <strong>Guide</strong> tab contains guides and tutorial posts that describe a particular technique or methodology. </p>\n<p>Moderators may reclassify posts if they believe to be better suited to a different section. To reach the moderators you may use the comments below each post or  see the <a href=\"/about/\" title=\"About page\">About</a> page for contact information.</p>\n<p>Logged in users get small notification in the tab displaying the number of new posts in a tab since their last visited.</p>", "tag_set": [1, 27], "lastedit_user": 31, "root": 38}}, {"pk": 39, "model": "server.post", "fields": {"rank": 1332941353.0, "creation_date": "2012-03-28 08:29:13", "tag_val": "blast tutorial MSU-NGS-2011", "full_score": 0, "title": "Installing and Running NCBI BLAST", "content": "##rest\n\n\"Installing and Running NCBI BLAST\" tutorial imported from the MSU course **Analyzing Next Generation Sequencing Data** (http://bioinformatics.msu.edu/ngs-summer-course-2011)\n\nYou should start this tutorial at a prompt that looks something like this::\n\n   root@ip-10-82-233-6:~#\n\nType 'cd' to go to your home directory on your EC2 machine.\n\nNow, use your Web browser on your laptop to go to:\n\n   ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST\n\nright- or control-click on the file ending with 'x64-linux.tar.gz', and\n\"copy link URL\".  This is the file for 64-bit (large) Linux machines, which\nis what our EC2 instance is.  (The current URL is: ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.2.25+-x64-linux.tar.gz)\n\nNow use the 'curl' program to download it to your Amazon computer::\n\n %% curl -O ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.2.25+-x64-linux.tar.gz\n\nHere, 'curl' is a program that takes a Web link and downloads it via the\ncommand line; in this case, it's grabbing that file and saving it into your\ncurrent directory.\n\nAfter it completes, you should see the file in your local directory::\n\n %% ls ncbi-*.tar.gz\n\nThis is a .tar.gz file, which is kind of like a zip file.  You need to\nuse the 'tar' program to unpack it (you could use 'unzip' if it were a .zip\nfile)::\n\n %% tar xzf ncbi-*.tar.gz\n\nThis will create a new subdirectory, 'ncbi-blast-2.2.25+'::\n\n %% ls\n Dropbox  ncbi-blast-2.2.25+  ncbi-blast-2.2.25+-x64-linux.tar.gz\n\nIf you look in the blast subdirectory, you will see a few more files, most of\nwhich are directories::\n\n %% ls ncbi-blast-2.2.25+\n bin  ChangeLog  doc  LICENSE  ncbi_package_info  README\n\nIn this case, we want to put everything in that bin/ directory into\na common place where UNIX knows to look for programs to run.  One such\nplace (that, by convention, is a good place to install things that don't\ncome with the computer) is /usr/local/bin::\n\n %% cp ncbi-blast-2.2.25+/bin/* /usr/local/bin\n\nNow, let's go to a new section of the machine. ::\n\n %% cd /mnt\n\nThis goes to the folder named '/mnt', which is on another (bigger)\ndisk.  We'll explain this more tomorrow.\n\nNow lets grab some biggish files to work with... the mouse and\nzebrafish reference proteomes!\n\nGo to ftp://ftp.ncbi.nlm.nih.gov/refseq/ in your browser and explore a\nbit.  You'll see there's a bunch of files and directories; in this\ncase, we want to go grab the mouse and zebrafish protein sets. So,\ngrab\nftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/mRNA_Prot/mouse.protein.faa.gz\nand\nftp://ftp.ncbi.nlm.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.protein.faa.gz::\n\n %% curl -O ftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/mRNA_Prot/mouse.protein.faa.gz\n %% curl -O ftp://ftp.ncbi.nlm.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.protein.faa.gz\n\nThese files aren't .tar.gz files, they're just .gz files -- the .faa means\n\"fasta\".  'gz' is a compression scheme for single files; to get at the\ncontents, do uncompress both of them with this command: ::\n\n %% gunzip *.gz\n\nIf you use 'ls', you'll see that the files have turned into 'mouse.protein.faa'\nand 'zebrafish.protein.faa'::\n\n %% ls\n\nYou can also take a look at the contents of the files with the 'more'\nprogram, which pages through the files. ::\n\n %% more mouse.protein.faa\n\nUse the spacebar to scroll down, and 'q' to exit before reaching the\nend of the file.  You can also look at the zebrafish file::\n\n %% more zebrafish.protein.faa\n\nNow, let's convert them into BLAST databases::\n\n %% makeblastdb -in mouse.protein.faa -dbtype prot\n %% makeblastdb -in zebrafish.protein.faa -dbtype prot\n\nThis lets us use BLAST to query the databases for matches.\n\nBefore we do a *big* BLAST, let's start by doing a small one, just to\ncheck that it's all working.  To do that, we'll skim off some\nsequences from the top of the file::\n\n %% head zebrafish.protein.faa\n\nThe problem here is that 'head' by default only selects the first 10\nlines of a file, which may not be a complete set of FASTA records -- so you\nmay have to tweak things.  In this case, the first 14 lines are complete::\n\n %% head -14 zebrafish.protein.faa\n\nLet's take the output of 'head' and put it in a file, 'zebrafish.top',\nthat we can use for other purposes::\n\n %% head -14 zebrafish.protein.faa > zebrafish.top\n\nOK, great!  Now let's run a BLASTP comparing these zebrafish sequences\nto the mouse proteins, and we'll put the results in a file 'xxx.txt'::\n\n %% blastp -query zebrafish.top -db mouse.protein.faa -out xxx.txt\n\n(The file name 'xxx.txt' is just a throwaway file name, something\nthat we can look at and see is a test file.  You can use your own\nconvention; I usually go with something short and recognizably\nsilly, like 'xxx', 'yyy', 'foo', etc.)\n\nOK, now take a look at that file with 'more'::\n\n %% more xxx.txt\n\nYep, looks like BLAST output to me!\n\nThere's all sorts of things you can do to alter the BLAST output; run\n'blastp' to get a list of those options.  For example, '-evalue 1e-6'\nwill set the e-value cutoff at 1e-6, above which nothing will be\ndisplayed.\n\nNow let's run a bigger BLAST, all zebrafish proteins against all mouse\nproteins::\n\n %% blastp -query zebrafish.protein.faa -db mouse.protein.faa -out zebrafish.x.mouse &\n\nThis is going to take a while, which is why we told the computer to\ngive us back a command prompt while blastp runs (that's what the &\ndoes).\n\nSo, how long is it going to take?  We can guesstimate by looking at\nhow many sequences have been processed since we started.  To do that, run ::\n\n %% grep Query= zebrafish.x.mouse\n\nOK, that gives us all the query lines -- now what?  Let's count them with\n'wc -l'::\n\n %% grep Query= zebrafish.x.mouse | wc -l\n\nHere, | is what's known as a 'pipe', telling the command line to take\nthe output of 'grep' and send it to the command 'wc', which counts\nwords, lines, and paragraphs.  The '-l' tells wc to count the lines\nonly.\n\nCompare that number to the number of sequences in the zebrafish protein database::\n\n %% grep ^'>' zebrafish.protein.faa | more\n\nto see the FASTA headers, and ::\n\n %% grep ^'>' zebrafish.protein.faa | wc -l\n\nto count all the sequences.\n\nLast, but not least -- let's run a quick script to convert the file into\na set of CSV matches::\n\n %% python ~/Dropbox/ngs-scripts/blast/blast-to-csv.py zebrafish.x.mouse > ~/Dropbox/zebrafish-mouse.csv\n\nTake a look at the script and see if you can understand what it does::\n\n %% more ~/Dropbox/ngs-scripts/blast/blast-to-csv.py\n\nBefore you leave for lunch:\n---------------------------\n\nLet's start a *second* BLAST, all of mouse against all of zebrafish::\n\n  %% blastp -query mouse.protein.faa -db zebrafish.protein.faa -out mouse.x.zebrafish &\n\n...now the computer can work while we eat!\n\nWhen we come back, we can work through a reciprocal BLAST example.\n\n.. @@ save these files\n\n.. # scripting etc.\n.. @@ install biopython, ez_seutp, blastkit??", "comment_count": 0, "score": 0, "type": 4, "status": 100, "revision_count": 1, "parent": 39, "views": 0, "answer_count": 0, "accepted": false, "slug": "installing-and-running-ncbi-blast", "lastedit_date": "2012-03-28 08:29:13", "url": "", "author": 32, "html": "<div class=\"document\">\n<p>&quot;Installing and Running NCBI BLAST&quot; tutorial imported from the MSU course <strong>Analyzing Next Generation Sequencing Data</strong> (<a class=\"reference external\" href=\"http://bioinformatics.msu.edu/ngs-summer-course-2011\">http://bioinformatics.msu.edu/ngs-summer-course-2011</a>)</p>\n<p>You should start this tutorial at a prompt that looks something like this:</p>\n<pre class=\"literal-block\">\nroot&#64;ip-10-82-233-6:~#\n</pre>\n<p>Type 'cd' to go to your home directory on your EC2 machine.</p>\n<p>Now, use your Web browser on your laptop to go to:</p>\n<blockquote>\n<a class=\"reference external\" href=\"ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST\">ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST</a></blockquote>\n<p>right- or control-click on the file ending with 'x64-linux.tar.gz', and\n&quot;copy link URL&quot;.  This is the file for 64-bit (large) Linux machines, which\nis what our EC2 instance is.  (The current URL is: <a class=\"reference external\" href=\"ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.2.25+-x64-linux.tar.gz\">ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.2.25+-x64-linux.tar.gz</a>)</p>\n<p>Now use the 'curl' program to download it to your Amazon computer:</p>\n<pre class=\"literal-block\">\n%% curl -O ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.2.25+-x64-linux.tar.gz\n</pre>\n<p>Here, 'curl' is a program that takes a Web link and downloads it via the\ncommand line; in this case, it's grabbing that file and saving it into your\ncurrent directory.</p>\n<p>After it completes, you should see the file in your local directory:</p>\n<pre class=\"literal-block\">\n%% ls ncbi-*.tar.gz\n</pre>\n<p>This is a .tar.gz file, which is kind of like a zip file.  You need to\nuse the 'tar' program to unpack it (you could use 'unzip' if it were a .zip\nfile):</p>\n<pre class=\"literal-block\">\n%% tar xzf ncbi-*.tar.gz\n</pre>\n<p>This will create a new subdirectory, 'ncbi-blast-2.2.25+':</p>\n<pre class=\"literal-block\">\n%% ls\nDropbox  ncbi-blast-2.2.25+  ncbi-blast-2.2.25+-x64-linux.tar.gz\n</pre>\n<p>If you look in the blast subdirectory, you will see a few more files, most of\nwhich are directories:</p>\n<pre class=\"literal-block\">\n%% ls ncbi-blast-2.2.25+\nbin  ChangeLog  doc  LICENSE  ncbi_package_info  README\n</pre>\n<p>In this case, we want to put everything in that bin/ directory into\na common place where UNIX knows to look for programs to run.  One such\nplace (that, by convention, is a good place to install things that don't\ncome with the computer) is /usr/local/bin:</p>\n<pre class=\"literal-block\">\n%% cp ncbi-blast-2.2.25+/bin/* /usr/local/bin\n</pre>\n<p>Now, let's go to a new section of the machine.</p>\n<pre class=\"literal-block\">\n%% cd /mnt\n</pre>\n<p>This goes to the folder named '/mnt', which is on another (bigger)\ndisk.  We'll explain this more tomorrow.</p>\n<p>Now lets grab some biggish files to work with... the mouse and\nzebrafish reference proteomes!</p>\n<p>Go to <a class=\"reference external\" href=\"ftp://ftp.ncbi.nlm.nih.gov/refseq/\">ftp://ftp.ncbi.nlm.nih.gov/refseq/</a> in your browser and explore a\nbit.  You'll see there's a bunch of files and directories; in this\ncase, we want to go grab the mouse and zebrafish protein sets. So,\ngrab\n<a class=\"reference external\" href=\"ftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/mRNA_Prot/mouse.protein.faa.gz\">ftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/mRNA_Prot/mouse.protein.faa.gz</a>\nand\n<a class=\"reference external\" href=\"ftp://ftp.ncbi.nlm.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.protein.faa.gz\">ftp://ftp.ncbi.nlm.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.protein.faa.gz</a>:</p>\n<pre class=\"literal-block\">\n%% curl -O ftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/mRNA_Prot/mouse.protein.faa.gz\n%% curl -O ftp://ftp.ncbi.nlm.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.protein.faa.gz\n</pre>\n<p>These files aren't .tar.gz files, they're just .gz files -- the .faa means\n&quot;fasta&quot;.  'gz' is a compression scheme for single files; to get at the\ncontents, do uncompress both of them with this command:</p>\n<pre class=\"literal-block\">\n%% gunzip *.gz\n</pre>\n<p>If you use 'ls', you'll see that the files have turned into 'mouse.protein.faa'\nand 'zebrafish.protein.faa':</p>\n<pre class=\"literal-block\">\n%% ls\n</pre>\n<p>You can also take a look at the contents of the files with the 'more'\nprogram, which pages through the files.</p>\n<pre class=\"literal-block\">\n%% more mouse.protein.faa\n</pre>\n<p>Use the spacebar to scroll down, and 'q' to exit before reaching the\nend of the file.  You can also look at the zebrafish file:</p>\n<pre class=\"literal-block\">\n%% more zebrafish.protein.faa\n</pre>\n<p>Now, let's convert them into BLAST databases:</p>\n<pre class=\"literal-block\">\n%% makeblastdb -in mouse.protein.faa -dbtype prot\n%% makeblastdb -in zebrafish.protein.faa -dbtype prot\n</pre>\n<p>This lets us use BLAST to query the databases for matches.</p>\n<p>Before we do a <em>big</em> BLAST, let's start by doing a small one, just to\ncheck that it's all working.  To do that, we'll skim off some\nsequences from the top of the file:</p>\n<pre class=\"literal-block\">\n%% head zebrafish.protein.faa\n</pre>\n<p>The problem here is that 'head' by default only selects the first 10\nlines of a file, which may not be a complete set of FASTA records -- so you\nmay have to tweak things.  In this case, the first 14 lines are complete:</p>\n<pre class=\"literal-block\">\n%% head -14 zebrafish.protein.faa\n</pre>\n<p>Let's take the output of 'head' and put it in a file, 'zebrafish.top',\nthat we can use for other purposes:</p>\n<pre class=\"literal-block\">\n%% head -14 zebrafish.protein.faa &gt; zebrafish.top\n</pre>\n<p>OK, great!  Now let's run a BLASTP comparing these zebrafish sequences\nto the mouse proteins, and we'll put the results in a file 'xxx.txt':</p>\n<pre class=\"literal-block\">\n%% blastp -query zebrafish.top -db mouse.protein.faa -out xxx.txt\n</pre>\n<p>(The file name 'xxx.txt' is just a throwaway file name, something\nthat we can look at and see is a test file.  You can use your own\nconvention; I usually go with something short and recognizably\nsilly, like 'xxx', 'yyy', 'foo', etc.)</p>\n<p>OK, now take a look at that file with 'more':</p>\n<pre class=\"literal-block\">\n%% more xxx.txt\n</pre>\n<p>Yep, looks like BLAST output to me!</p>\n<p>There's all sorts of things you can do to alter the BLAST output; run\n'blastp' to get a list of those options.  For example, '-evalue 1e-6'\nwill set the e-value cutoff at 1e-6, above which nothing will be\ndisplayed.</p>\n<p>Now let's run a bigger BLAST, all zebrafish proteins against all mouse\nproteins:</p>\n<pre class=\"literal-block\">\n%% blastp -query zebrafish.protein.faa -db mouse.protein.faa -out zebrafish.x.mouse &amp;\n</pre>\n<p>This is going to take a while, which is why we told the computer to\ngive us back a command prompt while blastp runs (that's what the &amp;\ndoes).</p>\n<p>So, how long is it going to take?  We can guesstimate by looking at\nhow many sequences have been processed since we started.  To do that, run</p>\n<pre class=\"literal-block\">\n%% grep Query= zebrafish.x.mouse\n</pre>\n<p>OK, that gives us all the query lines -- now what?  Let's count them with\n'wc -l':</p>\n<pre class=\"literal-block\">\n%% grep Query= zebrafish.x.mouse | wc -l\n</pre>\n<p>Here, | is what's known as a 'pipe', telling the command line to take\nthe output of 'grep' and send it to the command 'wc', which counts\nwords, lines, and paragraphs.  The '-l' tells wc to count the lines\nonly.</p>\n<p>Compare that number to the number of sequences in the zebrafish protein database:</p>\n<pre class=\"literal-block\">\n%% grep ^'&gt;' zebrafish.protein.faa | more\n</pre>\n<p>to see the FASTA headers, and</p>\n<pre class=\"literal-block\">\n%% grep ^'&gt;' zebrafish.protein.faa | wc -l\n</pre>\n<p>to count all the sequences.</p>\n<p>Last, but not least -- let's run a quick script to convert the file into\na set of CSV matches:</p>\n<pre class=\"literal-block\">\n%% python ~/Dropbox/ngs-scripts/blast/blast-to-csv.py zebrafish.x.mouse &gt; ~/Dropbox/zebrafish-mouse.csv\n</pre>\n<p>Take a look at the script and see if you can understand what it does:</p>\n<pre class=\"literal-block\">\n%% more ~/Dropbox/ngs-scripts/blast/blast-to-csv.py\n</pre>\n<div class=\"section\" id=\"before-you-leave-for-lunch\">\n<h1>Before you leave for lunch:</h1>\n<p>Let's start a <em>second</em> BLAST, all of mouse against all of zebrafish:</p>\n<pre class=\"literal-block\">\n%% blastp -query mouse.protein.faa -db zebrafish.protein.faa -out mouse.x.zebrafish &amp;\n</pre>\n<p>...now the computer can work while we eat!</p>\n<p>When we come back, we can work through a reciprocal BLAST example.</p>\n<!-- @@ save these files -->\n<!-- # scripting etc. -->\n<!-- @@ install biopython, ez_seutp, blastkit?? -->\n</div>\n</div>\n", "tag_set": [28, 29, 30], "lastedit_user": 32, "root": 39}}, {"pk": 40, "model": "server.post", "fields": {"rank": 1332941353.0, "creation_date": "2012-03-28 08:29:13", "tag_val": "bowtie bwa tutorial MSU-NGS-2011", "full_score": 0, "title": "Mapping reads with bwa and bowtie", "content": "##rest\n\n\"Mapping reads with bwa and bowtie tutorial\" imported from the MSU course **Analyzing Next Generation Sequencing Data** (http://bioinformatics.msu.edu/ngs-summer-course-2011)\n\nIn this tutorial, we're going to take a set of Illumina reads from an inbred Drosophila melanogaster line, \nand map them back to the reference genome. (After these steps, we could do things like generate a list of SNPs \nat which this line differs from the reference strain, or generate a genome sequence for this fly strain, \nbut we'll get to that later on in the course.) We are also going to use two different (but popular) mapping tools, bwa and bowtie. \nAmong their differences is that bowtie (while smokin' fast) does not deal with \"gapped\" alignments, i.e. it \ndoes not handle insertion/deletions well. \n\nGetting the data\n----------------\n\nIf you just finished the FastQC tutorial, you can keep working on the same machine. Otherwise, launch an EC2 instance, make a volume out of the \nsame snapshot that we did before, and mount it. The data for this tutorial are in /data/drosophila::\n\n  %% cd /data/drosophila/\n  \nFor this example, you'll also need the Drosophila reference genome::\n\n  %% curl -O ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r5.37_FB2011_05/fasta/dmel-all-chromosome-r5.37.fasta.gz\n  %% gunzip dmel-all-chromosome-r5.37.fasta.gz\n\nInstalling and running bwa\n--------------------------\n  \nTo actually do the mapping, we need to download and install bwa.\n\nFirst we are going to grab the source files for bwa from sourceforge, using curl. It is important to \nknow that we need to specify a few flags to let the program know that we want to leep the name (and filetype) for \nthe file (-O) and that curl should follow relative hyperlinks (-L), to deal with redirection of the file site \n(or else curl won't work with sourceforge).::\n\n  %% curl -O -L http://sourceforge.net/projects/bio-bwa/files/bwa-0.5.9.tar.bz2\n\nNow we want to uncompress the tarball file using \"tar\". x extracts, v is verbose (telling you what it is doing), \nf skips prompting for each individual file, and j tells it to unzip .bz2 files.::\n\n  %% tar xvfj bwa-0.5.9.tar.bz2\n  %% cd bwa-0.5.9\n\nThe make command calls a program that helps to automate the compiling process for the program.::\n\n  %% make\n\n(Note: If your system doesn't have make installed already, you'll need to run \"apt-get install make\" before you \ncan build bwa -- but if you are using the right AMI, it should be pre-installed.)\n\nCopy the executable for bwa to a directory for binaries which is in your shell search path::  \n\n  %% cp bwa /usr/local/bin\n  %% cd ..\n\nIf you want to see what is in your shell search path (which can be modified later), run::\n\n  %% echo $PATH\n\nNow there are several steps involved in mapping our sequence reads and getting the output into a usable form. \nFirst we need to tell bwa to make an index of the reference genome; this will take a few minutes, so we've already\ngot the index already generated in the data directory, but if you want to try it yourself, you can run (but see the note below first!!)::\n\n  %% bwa index dmel-all-chromosome-r5.37.fasta\n\nNote: This step takes several minutes. If you run it, it will overwrite the index files we have already made, \nso don't run the above line exactly; instead, create a copy of the reference genome and then index the copy instead, \nso that we can preserve our pre-computed reference index::\n\n  %% cp dmel-all-chromosome-r5.37.fasta dmel-all-chromosome-r5.37.copy.fasta\n  %% bwa index dmel-all-chromosome-r5.37.copy.fasta\n \nNext, we do the actual mapping. These were paired-end reads, which means that for each DNA fragment, we have sequence \ndata from both ends. The sequences are therefore stored in two separate files (one for the data from each end), so we \nhave two mapping steps to perform. For now, we'll use bwa's default settings. The files you'll be running this on are \ndatasets that have been trimmed down to just the first 1 million sequence reads to speed things up, but at the end you'll \nbe able to work with the final product from an analysis of the full dataset that we ran earlier (some of these steps take \nupwards of an hour on the full dataset, but just a couple minutes on the trimmed dataset). Run::\n\n  %% bwa aln dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n  %% bwa aln dmel-all-chromosome-r5.37.fasta RAL357_2.fastq > RAL357_2.sai\n\nThese .sai files aren't very useful to us, so we need to convert them into SAM files. In this step, bwa takes the \ninformation from the two separate ends of each sequence and combines everything together. Here's how you do it \n(this may take around 10 minutes)::\n\n  %% bwa sampe dmel-all-chromosome-r5.37.fasta RAL357_1.sai RAL357_2.sai RAL357_1.fastq RAL357_2.fastq > RAL357_bwa.sam\n  \nThe SAM file is technically human-readable; take a look at it with::\n\n  %% more RAL357_bwa.sam\n  \nIt's not very easy to understand (if you are really curious about the SAM format, there is a 12-page \nmanual at http://samtools.sourceforge.net/SAM1.pdf). For now we'll use bowtie to map the same reads, \nand we'll use another tool to visualize these mappings in a more intuitive way. \n\nbwa options\n-----------\n\nThere are several options you can configure in bwa. Probably one of the most important is how many mismatches you \nwill allow between a read and a potential mapping location for that location to be considered a match. \nThe default is 4% of the read length, but you can set this to be either another proportion of the read length, or a fixed integer. \nFor example, if you ran::\n\n  %% bwa aln -n 4 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n  \nThis would do almost the same thing as above, except this time, all locations in the reference genome that contain \nfour or fewer mismatches to a given sequence read would be considered a match to that read.\n\nAlternatively, you could do::\n\n  %% bwa aln -n 0.01 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n  \nThis would only allow reads to be mapped to locations at which the reference genome differs by 1% or less from a given read fragment.\n\nIf you want to speed things up, you can tell it to run the alignment on multiple threads at once (this will only work \nif your computer has a multi-core processor, which our Amazon image does). To do so, use the -t option to specify the \nnumber of threads. For example, the following line would run in two simultaneous threads::\n\n  %% bwa aln -t 2 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n\nbwa can also handle single-end reads. The only difference is that you would use samse instead of sampe to generate your SAM file::\n\n  %% bwa samse dmel-all-chromosome-r5.37.fasta RAL357_1.sai RAL357_1.fastq > RAL357_1.sam\n  \n   \nNow let us align our reads using bowtie \n---------------------------------------\n(Note: For simplicity we are going to put all of the bowtie related files into the same directory. \nFor your own work, you may want to organize your file structure better than we have).\n\nLet's get bowtie from Sourceforge::\n\n  %% curl -O -L http://sourceforge.net/projects/bowtie-bio/files/bowtie/0.12.7/bowtie-0.12.7-linux-x86_64.zip\n\nunzip the file, and create a directory for bowtie. In this case, the program is precompiled so it comes as a binary executable::\n\n  %% unzip bowtie-0.12.7-linux-x86_64.zip\n  \nChange directory::\n\n  %% cd bowtie-0.12.7  \n\nCopy the bowtie files to a directory in you shell search path, and then move back to the parent directory (/data/drosophila)::\n\n  %% cp bowtie bowtie-build bowtie-inspect /usr/local/bin\n\nLet's create a new directory, \"drosophila_bowtie\" where we are going to place all the bowtie results::\n\n  %% cd ..\n  %% mkdir drosophila_bowtie\n  %% cd drosophila_bowtie\n  \nNow we are going to build an index of the Drosophila genome using bowtie just like we did with bwa. The original Drosophila reference genome is in the same location as we used before. Again, we have already performed the indexing step (it takes about 7 minutes), so if you want to try it yourself, index a copy so you don't over-write the one we've pre-run for you::\n\n%%  bowtie-build /data/drosophila/dmel-all-chromosome-r5.37.fasta  drosophila_bowtie  \n\nNow we get to map! We are going to use the default options for bowtie for the moment.  Let's go through this. there are a couple of flags that we have set, since We have paired end reads for these samples, and multiple processors. The general format for bowtie is (don't run this)::\n\n  %% bowtie indexFile fastqFile outputFile\n\nHowever we have some more details we want to include, so there are a couple of flags that we have to set.\n-S means that we want the output in SAM format.\n-p 2 is for multithreading (using more than one processor). In this case we have two to use.\n-1 -2 tells bowtie that these are paired end reads (the .fastq), and specifies which one is which.\n  \nThis should take 35-40 minutes to run on the full dataset so we'll run it on a trimmed version (should take about 3 minutes; later we'll give you pre-computed results for the full set.)::\n\n  %% bowtie -S -p 2 drosophila_bowtie -1 /data/drosophila/RAL357_1.fastq -2 /data/drosophila/RAL357_2.fastq RAL357_bowtie.sam\n\nYou may see warning messages like::\n\n  Warning: Exhausted best-first chunk memory for read SRR018286.1830915 USI-EAS034_2_PE_FC304DDAAXX:8:21:450:1640 length=45/1 (patid 1830914); skipping read\n\nWe will talk about some options you can set to deal with this.\n\nThe bowtie manual can be found here: http://bowtie-bio.sourceforge.net/manual.shtml\n\nSome additional useful arguments/options (at least for me)\n-m  # Suppresses all alignments for a particular read if more than m reportable alignments exist.\n-v  # no more than v mismatches in the entire length of the read\n-n -l # max number of mismatches in the high quality \"seed\", which is the the first l base pairs of a read.\n-chunkmbs  # number of mb of memory a thread is given to store path. Useful when you get warnings like above\n--best # make Bowtie \"guarantee\" that reported singleton alignments are \"best\" given the options\n--tryhard  # try  hard to find valid alignments, when they exit. VERY SLOW.\n \nProcessing the output for use with Samtools\n-------------------------------------------\n  \nEven the SAM file isn't very useful unless we can get it into a program that generates more readable output or lets us visualize things in a more intuitive way. For now, we'll get the output into a sorted BAM file so we can look at it using Samtools later.\n\nDownload and install Samtools::\n\n  %% cd /data/drosophila\n  %% curl -O -L http://sourceforge.net/projects/samtools/files/samtools/0.1.16/samtools-0.1.16.tar.bz2\n  %% tar xvfj samtools-0.1.16.tar.bz2\n  %% cd samtools-0.1.16\n  %% make\n  %% cp samtools /usr/local/bin\n  %% cd misc/\n  %% cp *.pl maq2sam-long maq2sam-short md5fa md5sum-lite wgsim /usr/local/bin/\n  %% cd /data/drosophila\n  \nLike bwa, Samtools also requires us to go through several steps before we have our data in usable form. First, we need to have Samtools generate its own index of the reference genome::\n\n  %% samtools faidx dmel-all-chromosome-r5.37.fasta\n\nNext, we need to convert the SAM file into a BAM file. (A BAM file is just a binary version of a SAM file.)::\n\n  %% samtools import dmel-all-chromosome-r5.37.fasta.fai RAL357_bwa.sam RAL357_bwa.bam\n\nNow, we need to sort the BAM file::\n\n  %% samtools sort RAL357_bwa.bam RAL357_bwa.sorted\n  \nAnd last, we need Samtools to index the BAM file::\n\n  %% samtools index RAL357_bwa.sorted.bam\n\nLet us do this again for the bowtie output. We move back to the drosophila_bowtie directory (you could do this all from the other directory, but it gets harder to read the command with long pathnames)::\n\n  %% cd drosophila_bowtie\n  %% samtools import ../dmel-all-chromosome-r5.37.fasta.fai RAL357_bowtie.sam RAL357_bowtie.bam\n\nNow, we need to sort the BAM file (also slow)::\n\n  %% samtools sort RAL357_bowtie.bam RAL357_bowtie.sorted\n  \nAnd last, we need Samtools to index the BAM file::\n\n  %% samtools index RAL357_bowtie.sorted.bam\n  \nAll done! Now we can use the sorted BAM file in Samtools to visualize our mappings, generate lists of SNPs, and call consensus sequences. We'll get to all of that later on today and in the rest of the course.\n\nViewing the output with TView\n-----------------------------\n\nBefore we can use TView to compare Bowtie and BWA mappings, we need to sort the Bowtie BAM file, and generate an index for it.::\n\n  %% cd drosophila_bowtie\n  %% samtools sort RAL357_full_bowtie.bam RAL357_full_bowtie.sorted\n  %% samtools index RAL357_full_bowtie.sorted.bam\n\nNow that we've generated the files, we can view the output with TView. We'll compare two different sorted::\n\n  %% cd ..\n  %% samtools tview RAL357_full_bwa.sorted.bam\n\nNow open an additional terminal window, and load the Bowtie mapping file there as well.::\n  \n  %% cd /data/drosophila/drosophila_bowtie\n  %% samtools tview RAL357_full_bowtie.sorted.bam ../dmel-all-chromosome-r5.37.fasta\n\nTo view the tview help, type '?'", "comment_count": 0, "score": 0, "type": 4, "status": 100, "revision_count": 1, "parent": 40, "views": 0, "answer_count": 0, "accepted": false, "slug": "mapping-reads-with-bwa-and-bowtie", "lastedit_date": "2012-03-28 08:29:13", "url": "", "author": 32, "html": "<div class=\"document\">\n<p>&quot;Mapping reads with bwa and bowtie tutorial&quot; imported from the MSU course <strong>Analyzing Next Generation Sequencing Data</strong> (<a class=\"reference external\" href=\"http://bioinformatics.msu.edu/ngs-summer-course-2011\">http://bioinformatics.msu.edu/ngs-summer-course-2011</a>)</p>\n<p>In this tutorial, we're going to take a set of Illumina reads from an inbred Drosophila melanogaster line,\nand map them back to the reference genome. (After these steps, we could do things like generate a list of SNPs\nat which this line differs from the reference strain, or generate a genome sequence for this fly strain,\nbut we'll get to that later on in the course.) We are also going to use two different (but popular) mapping tools, bwa and bowtie.\nAmong their differences is that bowtie (while smokin' fast) does not deal with &quot;gapped&quot; alignments, i.e. it\ndoes not handle insertion/deletions well.</p>\n<div class=\"section\" id=\"getting-the-data\">\n<h1>Getting the data</h1>\n<p>If you just finished the FastQC tutorial, you can keep working on the same machine. Otherwise, launch an EC2 instance, make a volume out of the\nsame snapshot that we did before, and mount it. The data for this tutorial are in /data/drosophila:</p>\n<pre class=\"literal-block\">\n%% cd /data/drosophila/\n</pre>\n<p>For this example, you'll also need the Drosophila reference genome:</p>\n<pre class=\"literal-block\">\n%% curl -O ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r5.37_FB2011_05/fasta/dmel-all-chromosome-r5.37.fasta.gz\n%% gunzip dmel-all-chromosome-r5.37.fasta.gz\n</pre>\n</div>\n<div class=\"section\" id=\"installing-and-running-bwa\">\n<h1>Installing and running bwa</h1>\n<p>To actually do the mapping, we need to download and install bwa.</p>\n<p>First we are going to grab the source files for bwa from sourceforge, using curl. It is important to\nknow that we need to specify a few flags to let the program know that we want to leep the name (and filetype) for\nthe file (-O) and that curl should follow relative hyperlinks (-L), to deal with redirection of the file site\n(or else curl won't work with sourceforge).:</p>\n<pre class=\"literal-block\">\n%% curl -O -L http://sourceforge.net/projects/bio-bwa/files/bwa-0.5.9.tar.bz2\n</pre>\n<p>Now we want to uncompress the tarball file using &quot;tar&quot;. x extracts, v is verbose (telling you what it is doing),\nf skips prompting for each individual file, and j tells it to unzip .bz2 files.:</p>\n<pre class=\"literal-block\">\n%% tar xvfj bwa-0.5.9.tar.bz2\n%% cd bwa-0.5.9\n</pre>\n<p>The make command calls a program that helps to automate the compiling process for the program.:</p>\n<pre class=\"literal-block\">\n%% make\n</pre>\n<p>(Note: If your system doesn't have make installed already, you'll need to run &quot;apt-get install make&quot; before you\ncan build bwa -- but if you are using the right AMI, it should be pre-installed.)</p>\n<p>Copy the executable for bwa to a directory for binaries which is in your shell search path:</p>\n<pre class=\"literal-block\">\n%% cp bwa /usr/local/bin\n%% cd ..\n</pre>\n<p>If you want to see what is in your shell search path (which can be modified later), run:</p>\n<pre class=\"literal-block\">\n%% echo $PATH\n</pre>\n<p>Now there are several steps involved in mapping our sequence reads and getting the output into a usable form.\nFirst we need to tell bwa to make an index of the reference genome; this will take a few minutes, so we've already\ngot the index already generated in the data directory, but if you want to try it yourself, you can run (but see the note below first!!):</p>\n<pre class=\"literal-block\">\n%% bwa index dmel-all-chromosome-r5.37.fasta\n</pre>\n<p>Note: This step takes several minutes. If you run it, it will overwrite the index files we have already made,\nso don't run the above line exactly; instead, create a copy of the reference genome and then index the copy instead,\nso that we can preserve our pre-computed reference index:</p>\n<pre class=\"literal-block\">\n%% cp dmel-all-chromosome-r5.37.fasta dmel-all-chromosome-r5.37.copy.fasta\n%% bwa index dmel-all-chromosome-r5.37.copy.fasta\n</pre>\n<p>Next, we do the actual mapping. These were paired-end reads, which means that for each DNA fragment, we have sequence\ndata from both ends. The sequences are therefore stored in two separate files (one for the data from each end), so we\nhave two mapping steps to perform. For now, we'll use bwa's default settings. The files you'll be running this on are\ndatasets that have been trimmed down to just the first 1 million sequence reads to speed things up, but at the end you'll\nbe able to work with the final product from an analysis of the full dataset that we ran earlier (some of these steps take\nupwards of an hour on the full dataset, but just a couple minutes on the trimmed dataset). Run:</p>\n<pre class=\"literal-block\">\n%% bwa aln dmel-all-chromosome-r5.37.fasta RAL357_1.fastq &gt; RAL357_1.sai\n%% bwa aln dmel-all-chromosome-r5.37.fasta RAL357_2.fastq &gt; RAL357_2.sai\n</pre>\n<p>These .sai files aren't very useful to us, so we need to convert them into SAM files. In this step, bwa takes the\ninformation from the two separate ends of each sequence and combines everything together. Here's how you do it\n(this may take around 10 minutes):</p>\n<pre class=\"literal-block\">\n%% bwa sampe dmel-all-chromosome-r5.37.fasta RAL357_1.sai RAL357_2.sai RAL357_1.fastq RAL357_2.fastq &gt; RAL357_bwa.sam\n</pre>\n<p>The SAM file is technically human-readable; take a look at it with:</p>\n<pre class=\"literal-block\">\n%% more RAL357_bwa.sam\n</pre>\n<p>It's not very easy to understand (if you are really curious about the SAM format, there is a 12-page\nmanual at <a class=\"reference external\" href=\"http://samtools.sourceforge.net/SAM1.pdf\">http://samtools.sourceforge.net/SAM1.pdf</a>). For now we'll use bowtie to map the same reads,\nand we'll use another tool to visualize these mappings in a more intuitive way.</p>\n</div>\n<div class=\"section\" id=\"bwa-options\">\n<h1>bwa options</h1>\n<p>There are several options you can configure in bwa. Probably one of the most important is how many mismatches you\nwill allow between a read and a potential mapping location for that location to be considered a match.\nThe default is 4% of the read length, but you can set this to be either another proportion of the read length, or a fixed integer.\nFor example, if you ran:</p>\n<pre class=\"literal-block\">\n%% bwa aln -n 4 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq &gt; RAL357_1.sai\n</pre>\n<p>This would do almost the same thing as above, except this time, all locations in the reference genome that contain\nfour or fewer mismatches to a given sequence read would be considered a match to that read.</p>\n<p>Alternatively, you could do:</p>\n<pre class=\"literal-block\">\n%% bwa aln -n 0.01 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq &gt; RAL357_1.sai\n</pre>\n<p>This would only allow reads to be mapped to locations at which the reference genome differs by 1% or less from a given read fragment.</p>\n<p>If you want to speed things up, you can tell it to run the alignment on multiple threads at once (this will only work\nif your computer has a multi-core processor, which our Amazon image does). To do so, use the -t option to specify the\nnumber of threads. For example, the following line would run in two simultaneous threads:</p>\n<pre class=\"literal-block\">\n%% bwa aln -t 2 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq &gt; RAL357_1.sai\n</pre>\n<p>bwa can also handle single-end reads. The only difference is that you would use samse instead of sampe to generate your SAM file:</p>\n<pre class=\"literal-block\">\n%% bwa samse dmel-all-chromosome-r5.37.fasta RAL357_1.sai RAL357_1.fastq &gt; RAL357_1.sam\n</pre>\n</div>\n<div class=\"section\" id=\"now-let-us-align-our-reads-using-bowtie\">\n<h1>Now let us align our reads using bowtie</h1>\n<p>(Note: For simplicity we are going to put all of the bowtie related files into the same directory.\nFor your own work, you may want to organize your file structure better than we have).</p>\n<p>Let's get bowtie from Sourceforge:</p>\n<pre class=\"literal-block\">\n%% curl -O -L http://sourceforge.net/projects/bowtie-bio/files/bowtie/0.12.7/bowtie-0.12.7-linux-x86_64.zip\n</pre>\n<p>unzip the file, and create a directory for bowtie. In this case, the program is precompiled so it comes as a binary executable:</p>\n<pre class=\"literal-block\">\n%% unzip bowtie-0.12.7-linux-x86_64.zip\n</pre>\n<p>Change directory:</p>\n<pre class=\"literal-block\">\n%% cd bowtie-0.12.7\n</pre>\n<p>Copy the bowtie files to a directory in you shell search path, and then move back to the parent directory (/data/drosophila):</p>\n<pre class=\"literal-block\">\n%% cp bowtie bowtie-build bowtie-inspect /usr/local/bin\n</pre>\n<p>Let's create a new directory, &quot;drosophila_bowtie&quot; where we are going to place all the bowtie results:</p>\n<pre class=\"literal-block\">\n%% cd ..\n%% mkdir drosophila_bowtie\n%% cd drosophila_bowtie\n</pre>\n<p>Now we are going to build an index of the Drosophila genome using bowtie just like we did with bwa. The original Drosophila reference genome is in the same location as we used before. Again, we have already performed the indexing step (it takes about 7 minutes), so if you want to try it yourself, index a copy so you don't over-write the one we've pre-run for you:</p>\n<pre class=\"literal-block\">\n%%  bowtie-build /data/drosophila/dmel-all-chromosome-r5.37.fasta  drosophila_bowtie\n</pre>\n<p>Now we get to map! We are going to use the default options for bowtie for the moment.  Let's go through this. there are a couple of flags that we have set, since We have paired end reads for these samples, and multiple processors. The general format for bowtie is (don't run this):</p>\n<pre class=\"literal-block\">\n%% bowtie indexFile fastqFile outputFile\n</pre>\n<p>However we have some more details we want to include, so there are a couple of flags that we have to set.\n-S means that we want the output in SAM format.\n-p 2 is for multithreading (using more than one processor). In this case we have two to use.\n-1 -2 tells bowtie that these are paired end reads (the .fastq), and specifies which one is which.</p>\n<p>This should take 35-40 minutes to run on the full dataset so we'll run it on a trimmed version (should take about 3 minutes; later we'll give you pre-computed results for the full set.):</p>\n<pre class=\"literal-block\">\n%% bowtie -S -p 2 drosophila_bowtie -1 /data/drosophila/RAL357_1.fastq -2 /data/drosophila/RAL357_2.fastq RAL357_bowtie.sam\n</pre>\n<p>You may see warning messages like:</p>\n<pre class=\"literal-block\">\nWarning: Exhausted best-first chunk memory for read SRR018286.1830915 USI-EAS034_2_PE_FC304DDAAXX:8:21:450:1640 length=45/1 (patid 1830914); skipping read\n</pre>\n<p>We will talk about some options you can set to deal with this.</p>\n<p>The bowtie manual can be found here: <a class=\"reference external\" href=\"http://bowtie-bio.sourceforge.net/manual.shtml\">http://bowtie-bio.sourceforge.net/manual.shtml</a></p>\n<p>Some additional useful arguments/options (at least for me)\n-m  # Suppresses all alignments for a particular read if more than m reportable alignments exist.\n-v  # no more than v mismatches in the entire length of the read\n-n -l # max number of mismatches in the high quality &quot;seed&quot;, which is the the first l base pairs of a read.\n-chunkmbs  # number of mb of memory a thread is given to store path. Useful when you get warnings like above\n--best # make Bowtie &quot;guarantee&quot; that reported singleton alignments are &quot;best&quot; given the options\n--tryhard  # try  hard to find valid alignments, when they exit. VERY SLOW.</p>\n</div>\n<div class=\"section\" id=\"processing-the-output-for-use-with-samtools\">\n<h1>Processing the output for use with Samtools</h1>\n<p>Even the SAM file isn't very useful unless we can get it into a program that generates more readable output or lets us visualize things in a more intuitive way. For now, we'll get the output into a sorted BAM file so we can look at it using Samtools later.</p>\n<p>Download and install Samtools:</p>\n<pre class=\"literal-block\">\n%% cd /data/drosophila\n%% curl -O -L http://sourceforge.net/projects/samtools/files/samtools/0.1.16/samtools-0.1.16.tar.bz2\n%% tar xvfj samtools-0.1.16.tar.bz2\n%% cd samtools-0.1.16\n%% make\n%% cp samtools /usr/local/bin\n%% cd misc/\n%% cp *.pl maq2sam-long maq2sam-short md5fa md5sum-lite wgsim /usr/local/bin/\n%% cd /data/drosophila\n</pre>\n<p>Like bwa, Samtools also requires us to go through several steps before we have our data in usable form. First, we need to have Samtools generate its own index of the reference genome:</p>\n<pre class=\"literal-block\">\n%% samtools faidx dmel-all-chromosome-r5.37.fasta\n</pre>\n<p>Next, we need to convert the SAM file into a BAM file. (A BAM file is just a binary version of a SAM file.):</p>\n<pre class=\"literal-block\">\n%% samtools import dmel-all-chromosome-r5.37.fasta.fai RAL357_bwa.sam RAL357_bwa.bam\n</pre>\n<p>Now, we need to sort the BAM file:</p>\n<pre class=\"literal-block\">\n%% samtools sort RAL357_bwa.bam RAL357_bwa.sorted\n</pre>\n<p>And last, we need Samtools to index the BAM file:</p>\n<pre class=\"literal-block\">\n%% samtools index RAL357_bwa.sorted.bam\n</pre>\n<p>Let us do this again for the bowtie output. We move back to the drosophila_bowtie directory (you could do this all from the other directory, but it gets harder to read the command with long pathnames):</p>\n<pre class=\"literal-block\">\n%% cd drosophila_bowtie\n%% samtools import ../dmel-all-chromosome-r5.37.fasta.fai RAL357_bowtie.sam RAL357_bowtie.bam\n</pre>\n<p>Now, we need to sort the BAM file (also slow):</p>\n<pre class=\"literal-block\">\n%% samtools sort RAL357_bowtie.bam RAL357_bowtie.sorted\n</pre>\n<p>And last, we need Samtools to index the BAM file:</p>\n<pre class=\"literal-block\">\n%% samtools index RAL357_bowtie.sorted.bam\n</pre>\n<p>All done! Now we can use the sorted BAM file in Samtools to visualize our mappings, generate lists of SNPs, and call consensus sequences. We'll get to all of that later on today and in the rest of the course.</p>\n</div>\n<div class=\"section\" id=\"viewing-the-output-with-tview\">\n<h1>Viewing the output with TView</h1>\n<p>Before we can use TView to compare Bowtie and BWA mappings, we need to sort the Bowtie BAM file, and generate an index for it.:</p>\n<pre class=\"literal-block\">\n%% cd drosophila_bowtie\n%% samtools sort RAL357_full_bowtie.bam RAL357_full_bowtie.sorted\n%% samtools index RAL357_full_bowtie.sorted.bam\n</pre>\n<p>Now that we've generated the files, we can view the output with TView. We'll compare two different sorted:</p>\n<pre class=\"literal-block\">\n%% cd ..\n%% samtools tview RAL357_full_bwa.sorted.bam\n</pre>\n<p>Now open an additional terminal window, and load the Bowtie mapping file there as well.:</p>\n<pre class=\"literal-block\">\n%% cd /data/drosophila/drosophila_bowtie\n%% samtools tview RAL357_full_bowtie.sorted.bam ../dmel-all-chromosome-r5.37.fasta\n</pre>\n<p>To view the tview help, type '?'</p>\n</div>\n</div>\n", "tag_set": [29, 30, 31, 32], "lastedit_user": 32, "root": 40}}, {"pk": 1, "model": "server.postrevision", "fields": {"content": "TITLE:Biostar forum posting guidelines\nBiostar is divided into several sections:\n\nThe **Questions** tab is meant to help users find answers to well defined and specific bioinformatics questions. Most importantly the tile must be formulated as a question.\n\nThe **Forum** tab displays all other generic posts that do not have a designated location. Note that while the **Forum** post requirements are more relaxed the topic of the posts must still be _bioinformatics_ or _biological data analysis_\n\nThe **Planet** tab contains blog posts by existing users as well as short summaries and links to external blog sites. For external blog posts only a short description is shown, to read the full post follow the link. \n\nThe **Guide** tab contains guides and tutorial posts that describe a particular technique or methodology. \n\nModerators may reclassify posts if they believe to be better suited to a different section. To reach the moderators you may use the comments below each post or  see the [About](/about/ \"About page\") page for contact information.\n\nLogged in users get small notification in the tab displaying the number of new posts in a tab since their last visited.\nTAGS:forum guidelines", "diff": "--- \n+++ \n@@ -1,0 +1,15 @@\n+TITLE:Biostar forum posting guidelines\n+Biostar is divided into several sections:\n+\n+The **Questions** tab is meant to help users find answers to well defined and specific bioinformatics questions. Most importantly the tile must be formulated as a question.\n+\n+The **Forum** tab displays all other generic posts that do not have a designated location. Note that while the **Forum** post requirements are more relaxed the topic of the posts must still be _bioinformatics_ or _biological data analysis_\n+\n+The **Planet** tab contains blog posts by existing users as well as short summaries and links to external blog sites. For external blog posts only a short description is shown, to read the full post follow the link. \n+\n+The **Guide** tab contains guides and tutorial posts that describe a particular technique or methodology. \n+\n+Moderators may reclassify posts if they believe to be better suited to a different section. To reach the moderators you may use the comments below each post or  see the [About](/about/ \"About page\") page for contact information.\n+\n+Logged in users get small notification in the tab displaying the number of new posts in a tab since their last visited.\n+TAGS:forum guidelines", "post": 38, "date": "2012-03-28 08:29:13", "author": 31}}, {"pk": 2, "model": "server.postrevision", "fields": {"content": "TITLE:Installing and Running NCBI BLAST\n##rest\n\n\"Installing and Running NCBI BLAST\" tutorial imported from the MSU course **Analyzing Next Generation Sequencing Data** (http://bioinformatics.msu.edu/ngs-summer-course-2011)\n\nYou should start this tutorial at a prompt that looks something like this::\n\n   root@ip-10-82-233-6:~#\n\nType 'cd' to go to your home directory on your EC2 machine.\n\nNow, use your Web browser on your laptop to go to:\n\n   ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST\n\nright- or control-click on the file ending with 'x64-linux.tar.gz', and\n\"copy link URL\".  This is the file for 64-bit (large) Linux machines, which\nis what our EC2 instance is.  (The current URL is: ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.2.25+-x64-linux.tar.gz)\n\nNow use the 'curl' program to download it to your Amazon computer::\n\n %% curl -O ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.2.25+-x64-linux.tar.gz\n\nHere, 'curl' is a program that takes a Web link and downloads it via the\ncommand line; in this case, it's grabbing that file and saving it into your\ncurrent directory.\n\nAfter it completes, you should see the file in your local directory::\n\n %% ls ncbi-*.tar.gz\n\nThis is a .tar.gz file, which is kind of like a zip file.  You need to\nuse the 'tar' program to unpack it (you could use 'unzip' if it were a .zip\nfile)::\n\n %% tar xzf ncbi-*.tar.gz\n\nThis will create a new subdirectory, 'ncbi-blast-2.2.25+'::\n\n %% ls\n Dropbox  ncbi-blast-2.2.25+  ncbi-blast-2.2.25+-x64-linux.tar.gz\n\nIf you look in the blast subdirectory, you will see a few more files, most of\nwhich are directories::\n\n %% ls ncbi-blast-2.2.25+\n bin  ChangeLog  doc  LICENSE  ncbi_package_info  README\n\nIn this case, we want to put everything in that bin/ directory into\na common place where UNIX knows to look for programs to run.  One such\nplace (that, by convention, is a good place to install things that don't\ncome with the computer) is /usr/local/bin::\n\n %% cp ncbi-blast-2.2.25+/bin/* /usr/local/bin\n\nNow, let's go to a new section of the machine. ::\n\n %% cd /mnt\n\nThis goes to the folder named '/mnt', which is on another (bigger)\ndisk.  We'll explain this more tomorrow.\n\nNow lets grab some biggish files to work with... the mouse and\nzebrafish reference proteomes!\n\nGo to ftp://ftp.ncbi.nlm.nih.gov/refseq/ in your browser and explore a\nbit.  You'll see there's a bunch of files and directories; in this\ncase, we want to go grab the mouse and zebrafish protein sets. So,\ngrab\nftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/mRNA_Prot/mouse.protein.faa.gz\nand\nftp://ftp.ncbi.nlm.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.protein.faa.gz::\n\n %% curl -O ftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/mRNA_Prot/mouse.protein.faa.gz\n %% curl -O ftp://ftp.ncbi.nlm.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.protein.faa.gz\n\nThese files aren't .tar.gz files, they're just .gz files -- the .faa means\n\"fasta\".  'gz' is a compression scheme for single files; to get at the\ncontents, do uncompress both of them with this command: ::\n\n %% gunzip *.gz\n\nIf you use 'ls', you'll see that the files have turned into 'mouse.protein.faa'\nand 'zebrafish.protein.faa'::\n\n %% ls\n\nYou can also take a look at the contents of the files with the 'more'\nprogram, which pages through the files. ::\n\n %% more mouse.protein.faa\n\nUse the spacebar to scroll down, and 'q' to exit before reaching the\nend of the file.  You can also look at the zebrafish file::\n\n %% more zebrafish.protein.faa\n\nNow, let's convert them into BLAST databases::\n\n %% makeblastdb -in mouse.protein.faa -dbtype prot\n %% makeblastdb -in zebrafish.protein.faa -dbtype prot\n\nThis lets us use BLAST to query the databases for matches.\n\nBefore we do a *big* BLAST, let's start by doing a small one, just to\ncheck that it's all working.  To do that, we'll skim off some\nsequences from the top of the file::\n\n %% head zebrafish.protein.faa\n\nThe problem here is that 'head' by default only selects the first 10\nlines of a file, which may not be a complete set of FASTA records -- so you\nmay have to tweak things.  In this case, the first 14 lines are complete::\n\n %% head -14 zebrafish.protein.faa\n\nLet's take the output of 'head' and put it in a file, 'zebrafish.top',\nthat we can use for other purposes::\n\n %% head -14 zebrafish.protein.faa > zebrafish.top\n\nOK, great!  Now let's run a BLASTP comparing these zebrafish sequences\nto the mouse proteins, and we'll put the results in a file 'xxx.txt'::\n\n %% blastp -query zebrafish.top -db mouse.protein.faa -out xxx.txt\n\n(The file name 'xxx.txt' is just a throwaway file name, something\nthat we can look at and see is a test file.  You can use your own\nconvention; I usually go with something short and recognizably\nsilly, like 'xxx', 'yyy', 'foo', etc.)\n\nOK, now take a look at that file with 'more'::\n\n %% more xxx.txt\n\nYep, looks like BLAST output to me!\n\nThere's all sorts of things you can do to alter the BLAST output; run\n'blastp' to get a list of those options.  For example, '-evalue 1e-6'\nwill set the e-value cutoff at 1e-6, above which nothing will be\ndisplayed.\n\nNow let's run a bigger BLAST, all zebrafish proteins against all mouse\nproteins::\n\n %% blastp -query zebrafish.protein.faa -db mouse.protein.faa -out zebrafish.x.mouse &\n\nThis is going to take a while, which is why we told the computer to\ngive us back a command prompt while blastp runs (that's what the &\ndoes).\n\nSo, how long is it going to take?  We can guesstimate by looking at\nhow many sequences have been processed since we started.  To do that, run ::\n\n %% grep Query= zebrafish.x.mouse\n\nOK, that gives us all the query lines -- now what?  Let's count them with\n'wc -l'::\n\n %% grep Query= zebrafish.x.mouse | wc -l\n\nHere, | is what's known as a 'pipe', telling the command line to take\nthe output of 'grep' and send it to the command 'wc', which counts\nwords, lines, and paragraphs.  The '-l' tells wc to count the lines\nonly.\n\nCompare that number to the number of sequences in the zebrafish protein database::\n\n %% grep ^'>' zebrafish.protein.faa | more\n\nto see the FASTA headers, and ::\n\n %% grep ^'>' zebrafish.protein.faa | wc -l\n\nto count all the sequences.\n\nLast, but not least -- let's run a quick script to convert the file into\na set of CSV matches::\n\n %% python ~/Dropbox/ngs-scripts/blast/blast-to-csv.py zebrafish.x.mouse > ~/Dropbox/zebrafish-mouse.csv\n\nTake a look at the script and see if you can understand what it does::\n\n %% more ~/Dropbox/ngs-scripts/blast/blast-to-csv.py\n\nBefore you leave for lunch:\n---------------------------\n\nLet's start a *second* BLAST, all of mouse against all of zebrafish::\n\n  %% blastp -query mouse.protein.faa -db zebrafish.protein.faa -out mouse.x.zebrafish &\n\n...now the computer can work while we eat!\n\nWhen we come back, we can work through a reciprocal BLAST example.\n\n.. @@ save these files\n\n.. # scripting etc.\n.. @@ install biopython, ez_seutp, blastkit??\nTAGS:blast tutorial MSU-NGS-2011", "diff": "--- \n+++ \n@@ -1,0 +1,201 @@\n+TITLE:Installing and Running NCBI BLAST\n+##rest\n+\n+\"Installing and Running NCBI BLAST\" tutorial imported from the MSU course **Analyzing Next Generation Sequencing Data** (http://bioinformatics.msu.edu/ngs-summer-course-2011)\n+\n+You should start this tutorial at a prompt that looks something like this::\n+\n+   root@ip-10-82-233-6:~#\n+\n+Type 'cd' to go to your home directory on your EC2 machine.\n+\n+Now, use your Web browser on your laptop to go to:\n+\n+   ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST\n+\n+right- or control-click on the file ending with 'x64-linux.tar.gz', and\n+\"copy link URL\".  This is the file for 64-bit (large) Linux machines, which\n+is what our EC2 instance is.  (The current URL is: ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.2.25+-x64-linux.tar.gz)\n+\n+Now use the 'curl' program to download it to your Amazon computer::\n+\n+ %% curl -O ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.2.25+-x64-linux.tar.gz\n+\n+Here, 'curl' is a program that takes a Web link and downloads it via the\n+command line; in this case, it's grabbing that file and saving it into your\n+current directory.\n+\n+After it completes, you should see the file in your local directory::\n+\n+ %% ls ncbi-*.tar.gz\n+\n+This is a .tar.gz file, which is kind of like a zip file.  You need to\n+use the 'tar' program to unpack it (you could use 'unzip' if it were a .zip\n+file)::\n+\n+ %% tar xzf ncbi-*.tar.gz\n+\n+This will create a new subdirectory, 'ncbi-blast-2.2.25+'::\n+\n+ %% ls\n+ Dropbox  ncbi-blast-2.2.25+  ncbi-blast-2.2.25+-x64-linux.tar.gz\n+\n+If you look in the blast subdirectory, you will see a few more files, most of\n+which are directories::\n+\n+ %% ls ncbi-blast-2.2.25+\n+ bin  ChangeLog  doc  LICENSE  ncbi_package_info  README\n+\n+In this case, we want to put everything in that bin/ directory into\n+a common place where UNIX knows to look for programs to run.  One such\n+place (that, by convention, is a good place to install things that don't\n+come with the computer) is /usr/local/bin::\n+\n+ %% cp ncbi-blast-2.2.25+/bin/* /usr/local/bin\n+\n+Now, let's go to a new section of the machine. ::\n+\n+ %% cd /mnt\n+\n+This goes to the folder named '/mnt', which is on another (bigger)\n+disk.  We'll explain this more tomorrow.\n+\n+Now lets grab some biggish files to work with... the mouse and\n+zebrafish reference proteomes!\n+\n+Go to ftp://ftp.ncbi.nlm.nih.gov/refseq/ in your browser and explore a\n+bit.  You'll see there's a bunch of files and directories; in this\n+case, we want to go grab the mouse and zebrafish protein sets. So,\n+grab\n+ftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/mRNA_Prot/mouse.protein.faa.gz\n+and\n+ftp://ftp.ncbi.nlm.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.protein.faa.gz::\n+\n+ %% curl -O ftp://ftp.ncbi.nlm.nih.gov/refseq/M_musculus/mRNA_Prot/mouse.protein.faa.gz\n+ %% curl -O ftp://ftp.ncbi.nlm.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.protein.faa.gz\n+\n+These files aren't .tar.gz files, they're just .gz files -- the .faa means\n+\"fasta\".  'gz' is a compression scheme for single files; to get at the\n+contents, do uncompress both of them with this command: ::\n+\n+ %% gunzip *.gz\n+\n+If you use 'ls', you'll see that the files have turned into 'mouse.protein.faa'\n+and 'zebrafish.protein.faa'::\n+\n+ %% ls\n+\n+You can also take a look at the contents of the files with the 'more'\n+program, which pages through the files. ::\n+\n+ %% more mouse.protein.faa\n+\n+Use the spacebar to scroll down, and 'q' to exit before reaching the\n+end of the file.  You can also look at the zebrafish file::\n+\n+ %% more zebrafish.protein.faa\n+\n+Now, let's convert them into BLAST databases::\n+\n+ %% makeblastdb -in mouse.protein.faa -dbtype prot\n+ %% makeblastdb -in zebrafish.protein.faa -dbtype prot\n+\n+This lets us use BLAST to query the databases for matches.\n+\n+Before we do a *big* BLAST, let's start by doing a small one, just to\n+check that it's all working.  To do that, we'll skim off some\n+sequences from the top of the file::\n+\n+ %% head zebrafish.protein.faa\n+\n+The problem here is that 'head' by default only selects the first 10\n+lines of a file, which may not be a complete set of FASTA records -- so you\n+may have to tweak things.  In this case, the first 14 lines are complete::\n+\n+ %% head -14 zebrafish.protein.faa\n+\n+Let's take the output of 'head' and put it in a file, 'zebrafish.top',\n+that we can use for other purposes::\n+\n+ %% head -14 zebrafish.protein.faa > zebrafish.top\n+\n+OK, great!  Now let's run a BLASTP comparing these zebrafish sequences\n+to the mouse proteins, and we'll put the results in a file 'xxx.txt'::\n+\n+ %% blastp -query zebrafish.top -db mouse.protein.faa -out xxx.txt\n+\n+(The file name 'xxx.txt' is just a throwaway file name, something\n+that we can look at and see is a test file.  You can use your own\n+convention; I usually go with something short and recognizably\n+silly, like 'xxx', 'yyy', 'foo', etc.)\n+\n+OK, now take a look at that file with 'more'::\n+\n+ %% more xxx.txt\n+\n+Yep, looks like BLAST output to me!\n+\n+There's all sorts of things you can do to alter the BLAST output; run\n+'blastp' to get a list of those options.  For example, '-evalue 1e-6'\n+will set the e-value cutoff at 1e-6, above which nothing will be\n+displayed.\n+\n+Now let's run a bigger BLAST, all zebrafish proteins against all mouse\n+proteins::\n+\n+ %% blastp -query zebrafish.protein.faa -db mouse.protein.faa -out zebrafish.x.mouse &\n+\n+This is going to take a while, which is why we told the computer to\n+give us back a command prompt while blastp runs (that's what the &\n+does).\n+\n+So, how long is it going to take?  We can guesstimate by looking at\n+how many sequences have been processed since we started.  To do that, run ::\n+\n+ %% grep Query= zebrafish.x.mouse\n+\n+OK, that gives us all the query lines -- now what?  Let's count them with\n+'wc -l'::\n+\n+ %% grep Query= zebrafish.x.mouse | wc -l\n+\n+Here, | is what's known as a 'pipe', telling the command line to take\n+the output of 'grep' and send it to the command 'wc', which counts\n+words, lines, and paragraphs.  The '-l' tells wc to count the lines\n+only.\n+\n+Compare that number to the number of sequences in the zebrafish protein database::\n+\n+ %% grep ^'>' zebrafish.protein.faa | more\n+\n+to see the FASTA headers, and ::\n+\n+ %% grep ^'>' zebrafish.protein.faa | wc -l\n+\n+to count all the sequences.\n+\n+Last, but not least -- let's run a quick script to convert the file into\n+a set of CSV matches::\n+\n+ %% python ~/Dropbox/ngs-scripts/blast/blast-to-csv.py zebrafish.x.mouse > ~/Dropbox/zebrafish-mouse.csv\n+\n+Take a look at the script and see if you can understand what it does::\n+\n+ %% more ~/Dropbox/ngs-scripts/blast/blast-to-csv.py\n+\n+Before you leave for lunch:\n+---------------------------\n+\n+Let's start a *second* BLAST, all of mouse against all of zebrafish::\n+\n+  %% blastp -query mouse.protein.faa -db zebrafish.protein.faa -out mouse.x.zebrafish &\n+\n+...now the computer can work while we eat!\n+\n+When we come back, we can work through a reciprocal BLAST example.\n+\n+.. @@ save these files\n+\n+.. # scripting etc.\n+.. @@ install biopython, ez_seutp, blastkit??\n+TAGS:blast tutorial MSU-NGS-2011", "post": 39, "date": "2012-03-28 08:29:13", "author": 32}}, {"pk": 3, "model": "server.postrevision", "fields": {"content": "TITLE:Mapping reads with bwa and bowtie\n##rest\n\n\"Mapping reads with bwa and bowtie tutorial\" imported from the MSU course **Analyzing Next Generation Sequencing Data** (http://bioinformatics.msu.edu/ngs-summer-course-2011)\n\nIn this tutorial, we're going to take a set of Illumina reads from an inbred Drosophila melanogaster line, \nand map them back to the reference genome. (After these steps, we could do things like generate a list of SNPs \nat which this line differs from the reference strain, or generate a genome sequence for this fly strain, \nbut we'll get to that later on in the course.) We are also going to use two different (but popular) mapping tools, bwa and bowtie. \nAmong their differences is that bowtie (while smokin' fast) does not deal with \"gapped\" alignments, i.e. it \ndoes not handle insertion/deletions well. \n\nGetting the data\n----------------\n\nIf you just finished the FastQC tutorial, you can keep working on the same machine. Otherwise, launch an EC2 instance, make a volume out of the \nsame snapshot that we did before, and mount it. The data for this tutorial are in /data/drosophila::\n\n  %% cd /data/drosophila/\n  \nFor this example, you'll also need the Drosophila reference genome::\n\n  %% curl -O ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r5.37_FB2011_05/fasta/dmel-all-chromosome-r5.37.fasta.gz\n  %% gunzip dmel-all-chromosome-r5.37.fasta.gz\n\nInstalling and running bwa\n--------------------------\n  \nTo actually do the mapping, we need to download and install bwa.\n\nFirst we are going to grab the source files for bwa from sourceforge, using curl. It is important to \nknow that we need to specify a few flags to let the program know that we want to leep the name (and filetype) for \nthe file (-O) and that curl should follow relative hyperlinks (-L), to deal with redirection of the file site \n(or else curl won't work with sourceforge).::\n\n  %% curl -O -L http://sourceforge.net/projects/bio-bwa/files/bwa-0.5.9.tar.bz2\n\nNow we want to uncompress the tarball file using \"tar\". x extracts, v is verbose (telling you what it is doing), \nf skips prompting for each individual file, and j tells it to unzip .bz2 files.::\n\n  %% tar xvfj bwa-0.5.9.tar.bz2\n  %% cd bwa-0.5.9\n\nThe make command calls a program that helps to automate the compiling process for the program.::\n\n  %% make\n\n(Note: If your system doesn't have make installed already, you'll need to run \"apt-get install make\" before you \ncan build bwa -- but if you are using the right AMI, it should be pre-installed.)\n\nCopy the executable for bwa to a directory for binaries which is in your shell search path::  \n\n  %% cp bwa /usr/local/bin\n  %% cd ..\n\nIf you want to see what is in your shell search path (which can be modified later), run::\n\n  %% echo $PATH\n\nNow there are several steps involved in mapping our sequence reads and getting the output into a usable form. \nFirst we need to tell bwa to make an index of the reference genome; this will take a few minutes, so we've already\ngot the index already generated in the data directory, but if you want to try it yourself, you can run (but see the note below first!!)::\n\n  %% bwa index dmel-all-chromosome-r5.37.fasta\n\nNote: This step takes several minutes. If you run it, it will overwrite the index files we have already made, \nso don't run the above line exactly; instead, create a copy of the reference genome and then index the copy instead, \nso that we can preserve our pre-computed reference index::\n\n  %% cp dmel-all-chromosome-r5.37.fasta dmel-all-chromosome-r5.37.copy.fasta\n  %% bwa index dmel-all-chromosome-r5.37.copy.fasta\n \nNext, we do the actual mapping. These were paired-end reads, which means that for each DNA fragment, we have sequence \ndata from both ends. The sequences are therefore stored in two separate files (one for the data from each end), so we \nhave two mapping steps to perform. For now, we'll use bwa's default settings. The files you'll be running this on are \ndatasets that have been trimmed down to just the first 1 million sequence reads to speed things up, but at the end you'll \nbe able to work with the final product from an analysis of the full dataset that we ran earlier (some of these steps take \nupwards of an hour on the full dataset, but just a couple minutes on the trimmed dataset). Run::\n\n  %% bwa aln dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n  %% bwa aln dmel-all-chromosome-r5.37.fasta RAL357_2.fastq > RAL357_2.sai\n\nThese .sai files aren't very useful to us, so we need to convert them into SAM files. In this step, bwa takes the \ninformation from the two separate ends of each sequence and combines everything together. Here's how you do it \n(this may take around 10 minutes)::\n\n  %% bwa sampe dmel-all-chromosome-r5.37.fasta RAL357_1.sai RAL357_2.sai RAL357_1.fastq RAL357_2.fastq > RAL357_bwa.sam\n  \nThe SAM file is technically human-readable; take a look at it with::\n\n  %% more RAL357_bwa.sam\n  \nIt's not very easy to understand (if you are really curious about the SAM format, there is a 12-page \nmanual at http://samtools.sourceforge.net/SAM1.pdf). For now we'll use bowtie to map the same reads, \nand we'll use another tool to visualize these mappings in a more intuitive way. \n\nbwa options\n-----------\n\nThere are several options you can configure in bwa. Probably one of the most important is how many mismatches you \nwill allow between a read and a potential mapping location for that location to be considered a match. \nThe default is 4% of the read length, but you can set this to be either another proportion of the read length, or a fixed integer. \nFor example, if you ran::\n\n  %% bwa aln -n 4 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n  \nThis would do almost the same thing as above, except this time, all locations in the reference genome that contain \nfour or fewer mismatches to a given sequence read would be considered a match to that read.\n\nAlternatively, you could do::\n\n  %% bwa aln -n 0.01 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n  \nThis would only allow reads to be mapped to locations at which the reference genome differs by 1% or less from a given read fragment.\n\nIf you want to speed things up, you can tell it to run the alignment on multiple threads at once (this will only work \nif your computer has a multi-core processor, which our Amazon image does). To do so, use the -t option to specify the \nnumber of threads. For example, the following line would run in two simultaneous threads::\n\n  %% bwa aln -t 2 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n\nbwa can also handle single-end reads. The only difference is that you would use samse instead of sampe to generate your SAM file::\n\n  %% bwa samse dmel-all-chromosome-r5.37.fasta RAL357_1.sai RAL357_1.fastq > RAL357_1.sam\n  \n   \nNow let us align our reads using bowtie \n---------------------------------------\n(Note: For simplicity we are going to put all of the bowtie related files into the same directory. \nFor your own work, you may want to organize your file structure better than we have).\n\nLet's get bowtie from Sourceforge::\n\n  %% curl -O -L http://sourceforge.net/projects/bowtie-bio/files/bowtie/0.12.7/bowtie-0.12.7-linux-x86_64.zip\n\nunzip the file, and create a directory for bowtie. In this case, the program is precompiled so it comes as a binary executable::\n\n  %% unzip bowtie-0.12.7-linux-x86_64.zip\n  \nChange directory::\n\n  %% cd bowtie-0.12.7  \n\nCopy the bowtie files to a directory in you shell search path, and then move back to the parent directory (/data/drosophila)::\n\n  %% cp bowtie bowtie-build bowtie-inspect /usr/local/bin\n\nLet's create a new directory, \"drosophila_bowtie\" where we are going to place all the bowtie results::\n\n  %% cd ..\n  %% mkdir drosophila_bowtie\n  %% cd drosophila_bowtie\n  \nNow we are going to build an index of the Drosophila genome using bowtie just like we did with bwa. The original Drosophila reference genome is in the same location as we used before. Again, we have already performed the indexing step (it takes about 7 minutes), so if you want to try it yourself, index a copy so you don't over-write the one we've pre-run for you::\n\n%%  bowtie-build /data/drosophila/dmel-all-chromosome-r5.37.fasta  drosophila_bowtie  \n\nNow we get to map! We are going to use the default options for bowtie for the moment.  Let's go through this. there are a couple of flags that we have set, since We have paired end reads for these samples, and multiple processors. The general format for bowtie is (don't run this)::\n\n  %% bowtie indexFile fastqFile outputFile\n\nHowever we have some more details we want to include, so there are a couple of flags that we have to set.\n-S means that we want the output in SAM format.\n-p 2 is for multithreading (using more than one processor). In this case we have two to use.\n-1 -2 tells bowtie that these are paired end reads (the .fastq), and specifies which one is which.\n  \nThis should take 35-40 minutes to run on the full dataset so we'll run it on a trimmed version (should take about 3 minutes; later we'll give you pre-computed results for the full set.)::\n\n  %% bowtie -S -p 2 drosophila_bowtie -1 /data/drosophila/RAL357_1.fastq -2 /data/drosophila/RAL357_2.fastq RAL357_bowtie.sam\n\nYou may see warning messages like::\n\n  Warning: Exhausted best-first chunk memory for read SRR018286.1830915 USI-EAS034_2_PE_FC304DDAAXX:8:21:450:1640 length=45/1 (patid 1830914); skipping read\n\nWe will talk about some options you can set to deal with this.\n\nThe bowtie manual can be found here: http://bowtie-bio.sourceforge.net/manual.shtml\n\nSome additional useful arguments/options (at least for me)\n-m  # Suppresses all alignments for a particular read if more than m reportable alignments exist.\n-v  # no more than v mismatches in the entire length of the read\n-n -l # max number of mismatches in the high quality \"seed\", which is the the first l base pairs of a read.\n-chunkmbs  # number of mb of memory a thread is given to store path. Useful when you get warnings like above\n--best # make Bowtie \"guarantee\" that reported singleton alignments are \"best\" given the options\n--tryhard  # try  hard to find valid alignments, when they exit. VERY SLOW.\n \nProcessing the output for use with Samtools\n-------------------------------------------\n  \nEven the SAM file isn't very useful unless we can get it into a program that generates more readable output or lets us visualize things in a more intuitive way. For now, we'll get the output into a sorted BAM file so we can look at it using Samtools later.\n\nDownload and install Samtools::\n\n  %% cd /data/drosophila\n  %% curl -O -L http://sourceforge.net/projects/samtools/files/samtools/0.1.16/samtools-0.1.16.tar.bz2\n  %% tar xvfj samtools-0.1.16.tar.bz2\n  %% cd samtools-0.1.16\n  %% make\n  %% cp samtools /usr/local/bin\n  %% cd misc/\n  %% cp *.pl maq2sam-long maq2sam-short md5fa md5sum-lite wgsim /usr/local/bin/\n  %% cd /data/drosophila\n  \nLike bwa, Samtools also requires us to go through several steps before we have our data in usable form. First, we need to have Samtools generate its own index of the reference genome::\n\n  %% samtools faidx dmel-all-chromosome-r5.37.fasta\n\nNext, we need to convert the SAM file into a BAM file. (A BAM file is just a binary version of a SAM file.)::\n\n  %% samtools import dmel-all-chromosome-r5.37.fasta.fai RAL357_bwa.sam RAL357_bwa.bam\n\nNow, we need to sort the BAM file::\n\n  %% samtools sort RAL357_bwa.bam RAL357_bwa.sorted\n  \nAnd last, we need Samtools to index the BAM file::\n\n  %% samtools index RAL357_bwa.sorted.bam\n\nLet us do this again for the bowtie output. We move back to the drosophila_bowtie directory (you could do this all from the other directory, but it gets harder to read the command with long pathnames)::\n\n  %% cd drosophila_bowtie\n  %% samtools import ../dmel-all-chromosome-r5.37.fasta.fai RAL357_bowtie.sam RAL357_bowtie.bam\n\nNow, we need to sort the BAM file (also slow)::\n\n  %% samtools sort RAL357_bowtie.bam RAL357_bowtie.sorted\n  \nAnd last, we need Samtools to index the BAM file::\n\n  %% samtools index RAL357_bowtie.sorted.bam\n  \nAll done! Now we can use the sorted BAM file in Samtools to visualize our mappings, generate lists of SNPs, and call consensus sequences. We'll get to all of that later on today and in the rest of the course.\n\nViewing the output with TView\n-----------------------------\n\nBefore we can use TView to compare Bowtie and BWA mappings, we need to sort the Bowtie BAM file, and generate an index for it.::\n\n  %% cd drosophila_bowtie\n  %% samtools sort RAL357_full_bowtie.bam RAL357_full_bowtie.sorted\n  %% samtools index RAL357_full_bowtie.sorted.bam\n\nNow that we've generated the files, we can view the output with TView. We'll compare two different sorted::\n\n  %% cd ..\n  %% samtools tview RAL357_full_bwa.sorted.bam\n\nNow open an additional terminal window, and load the Bowtie mapping file there as well.::\n  \n  %% cd /data/drosophila/drosophila_bowtie\n  %% samtools tview RAL357_full_bowtie.sorted.bam ../dmel-all-chromosome-r5.37.fasta\n\nTo view the tview help, type '?'\nTAGS:bowtie bwa tutorial MSU-NGS-2011", "diff": "--- \n+++ \n@@ -1,0 +1,255 @@\n+TITLE:Mapping reads with bwa and bowtie\n+##rest\n+\n+\"Mapping reads with bwa and bowtie tutorial\" imported from the MSU course **Analyzing Next Generation Sequencing Data** (http://bioinformatics.msu.edu/ngs-summer-course-2011)\n+\n+In this tutorial, we're going to take a set of Illumina reads from an inbred Drosophila melanogaster line, \n+and map them back to the reference genome. (After these steps, we could do things like generate a list of SNPs \n+at which this line differs from the reference strain, or generate a genome sequence for this fly strain, \n+but we'll get to that later on in the course.) We are also going to use two different (but popular) mapping tools, bwa and bowtie. \n+Among their differences is that bowtie (while smokin' fast) does not deal with \"gapped\" alignments, i.e. it \n+does not handle insertion/deletions well. \n+\n+Getting the data\n+----------------\n+\n+If you just finished the FastQC tutorial, you can keep working on the same machine. Otherwise, launch an EC2 instance, make a volume out of the \n+same snapshot that we did before, and mount it. The data for this tutorial are in /data/drosophila::\n+\n+  %% cd /data/drosophila/\n+  \n+For this example, you'll also need the Drosophila reference genome::\n+\n+  %% curl -O ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r5.37_FB2011_05/fasta/dmel-all-chromosome-r5.37.fasta.gz\n+  %% gunzip dmel-all-chromosome-r5.37.fasta.gz\n+\n+Installing and running bwa\n+--------------------------\n+  \n+To actually do the mapping, we need to download and install bwa.\n+\n+First we are going to grab the source files for bwa from sourceforge, using curl. It is important to \n+know that we need to specify a few flags to let the program know that we want to leep the name (and filetype) for \n+the file (-O) and that curl should follow relative hyperlinks (-L), to deal with redirection of the file site \n+(or else curl won't work with sourceforge).::\n+\n+  %% curl -O -L http://sourceforge.net/projects/bio-bwa/files/bwa-0.5.9.tar.bz2\n+\n+Now we want to uncompress the tarball file using \"tar\". x extracts, v is verbose (telling you what it is doing), \n+f skips prompting for each individual file, and j tells it to unzip .bz2 files.::\n+\n+  %% tar xvfj bwa-0.5.9.tar.bz2\n+  %% cd bwa-0.5.9\n+\n+The make command calls a program that helps to automate the compiling process for the program.::\n+\n+  %% make\n+\n+(Note: If your system doesn't have make installed already, you'll need to run \"apt-get install make\" before you \n+can build bwa -- but if you are using the right AMI, it should be pre-installed.)\n+\n+Copy the executable for bwa to a directory for binaries which is in your shell search path::  \n+\n+  %% cp bwa /usr/local/bin\n+  %% cd ..\n+\n+If you want to see what is in your shell search path (which can be modified later), run::\n+\n+  %% echo $PATH\n+\n+Now there are several steps involved in mapping our sequence reads and getting the output into a usable form. \n+First we need to tell bwa to make an index of the reference genome; this will take a few minutes, so we've already\n+got the index already generated in the data directory, but if you want to try it yourself, you can run (but see the note below first!!)::\n+\n+  %% bwa index dmel-all-chromosome-r5.37.fasta\n+\n+Note: This step takes several minutes. If you run it, it will overwrite the index files we have already made, \n+so don't run the above line exactly; instead, create a copy of the reference genome and then index the copy instead, \n+so that we can preserve our pre-computed reference index::\n+\n+  %% cp dmel-all-chromosome-r5.37.fasta dmel-all-chromosome-r5.37.copy.fasta\n+  %% bwa index dmel-all-chromosome-r5.37.copy.fasta\n+ \n+Next, we do the actual mapping. These were paired-end reads, which means that for each DNA fragment, we have sequence \n+data from both ends. The sequences are therefore stored in two separate files (one for the data from each end), so we \n+have two mapping steps to perform. For now, we'll use bwa's default settings. The files you'll be running this on are \n+datasets that have been trimmed down to just the first 1 million sequence reads to speed things up, but at the end you'll \n+be able to work with the final product from an analysis of the full dataset that we ran earlier (some of these steps take \n+upwards of an hour on the full dataset, but just a couple minutes on the trimmed dataset). Run::\n+\n+  %% bwa aln dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n+  %% bwa aln dmel-all-chromosome-r5.37.fasta RAL357_2.fastq > RAL357_2.sai\n+\n+These .sai files aren't very useful to us, so we need to convert them into SAM files. In this step, bwa takes the \n+information from the two separate ends of each sequence and combines everything together. Here's how you do it \n+(this may take around 10 minutes)::\n+\n+  %% bwa sampe dmel-all-chromosome-r5.37.fasta RAL357_1.sai RAL357_2.sai RAL357_1.fastq RAL357_2.fastq > RAL357_bwa.sam\n+  \n+The SAM file is technically human-readable; take a look at it with::\n+\n+  %% more RAL357_bwa.sam\n+  \n+It's not very easy to understand (if you are really curious about the SAM format, there is a 12-page \n+manual at http://samtools.sourceforge.net/SAM1.pdf). For now we'll use bowtie to map the same reads, \n+and we'll use another tool to visualize these mappings in a more intuitive way. \n+\n+bwa options\n+-----------\n+\n+There are several options you can configure in bwa. Probably one of the most important is how many mismatches you \n+will allow between a read and a potential mapping location for that location to be considered a match. \n+The default is 4% of the read length, but you can set this to be either another proportion of the read length, or a fixed integer. \n+For example, if you ran::\n+\n+  %% bwa aln -n 4 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n+  \n+This would do almost the same thing as above, except this time, all locations in the reference genome that contain \n+four or fewer mismatches to a given sequence read would be considered a match to that read.\n+\n+Alternatively, you could do::\n+\n+  %% bwa aln -n 0.01 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n+  \n+This would only allow reads to be mapped to locations at which the reference genome differs by 1% or less from a given read fragment.\n+\n+If you want to speed things up, you can tell it to run the alignment on multiple threads at once (this will only work \n+if your computer has a multi-core processor, which our Amazon image does). To do so, use the -t option to specify the \n+number of threads. For example, the following line would run in two simultaneous threads::\n+\n+  %% bwa aln -t 2 dmel-all-chromosome-r5.37.fasta RAL357_1.fastq > RAL357_1.sai\n+\n+bwa can also handle single-end reads. The only difference is that you would use samse instead of sampe to generate your SAM file::\n+\n+  %% bwa samse dmel-all-chromosome-r5.37.fasta RAL357_1.sai RAL357_1.fastq > RAL357_1.sam\n+  \n+   \n+Now let us align our reads using bowtie \n+---------------------------------------\n+(Note: For simplicity we are going to put all of the bowtie related files into the same directory. \n+For your own work, you may want to organize your file structure better than we have).\n+\n+Let's get bowtie from Sourceforge::\n+\n+  %% curl -O -L http://sourceforge.net/projects/bowtie-bio/files/bowtie/0.12.7/bowtie-0.12.7-linux-x86_64.zip\n+\n+unzip the file, and create a directory for bowtie. In this case, the program is precompiled so it comes as a binary executable::\n+\n+  %% unzip bowtie-0.12.7-linux-x86_64.zip\n+  \n+Change directory::\n+\n+  %% cd bowtie-0.12.7  \n+\n+Copy the bowtie files to a directory in you shell search path, and then move back to the parent directory (/data/drosophila)::\n+\n+  %% cp bowtie bowtie-build bowtie-inspect /usr/local/bin\n+\n+Let's create a new directory, \"drosophila_bowtie\" where we are going to place all the bowtie results::\n+\n+  %% cd ..\n+  %% mkdir drosophila_bowtie\n+  %% cd drosophila_bowtie\n+  \n+Now we are going to build an index of the Drosophila genome using bowtie just like we did with bwa. The original Drosophila reference genome is in the same location as we used before. Again, we have already performed the indexing step (it takes about 7 minutes), so if you want to try it yourself, index a copy so you don't over-write the one we've pre-run for you::\n+\n+%%  bowtie-build /data/drosophila/dmel-all-chromosome-r5.37.fasta  drosophila_bowtie  \n+\n+Now we get to map! We are going to use the default options for bowtie for the moment.  Let's go through this. there are a couple of flags that we have set, since We have paired end reads for these samples, and multiple processors. The general format for bowtie is (don't run this)::\n+\n+  %% bowtie indexFile fastqFile outputFile\n+\n+However we have some more details we want to include, so there are a couple of flags that we have to set.\n+-S means that we want the output in SAM format.\n+-p 2 is for multithreading (using more than one processor). In this case we have two to use.\n+-1 -2 tells bowtie that these are paired end reads (the .fastq), and specifies which one is which.\n+  \n+This should take 35-40 minutes to run on the full dataset so we'll run it on a trimmed version (should take about 3 minutes; later we'll give you pre-computed results for the full set.)::\n+\n+  %% bowtie -S -p 2 drosophila_bowtie -1 /data/drosophila/RAL357_1.fastq -2 /data/drosophila/RAL357_2.fastq RAL357_bowtie.sam\n+\n+You may see warning messages like::\n+\n+  Warning: Exhausted best-first chunk memory for read SRR018286.1830915 USI-EAS034_2_PE_FC304DDAAXX:8:21:450:1640 length=45/1 (patid 1830914); skipping read\n+\n+We will talk about some options you can set to deal with this.\n+\n+The bowtie manual can be found here: http://bowtie-bio.sourceforge.net/manual.shtml\n+\n+Some additional useful arguments/options (at least for me)\n+-m  # Suppresses all alignments for a particular read if more than m reportable alignments exist.\n+-v  # no more than v mismatches in the entire length of the read\n+-n -l # max number of mismatches in the high quality \"seed\", which is the the first l base pairs of a read.\n+-chunkmbs  # number of mb of memory a thread is given to store path. Useful when you get warnings like above\n+--best # make Bowtie \"guarantee\" that reported singleton alignments are \"best\" given the options\n+--tryhard  # try  hard to find valid alignments, when they exit. VERY SLOW.\n+ \n+Processing the output for use with Samtools\n+-------------------------------------------\n+  \n+Even the SAM file isn't very useful unless we can get it into a program that generates more readable output or lets us visualize things in a more intuitive way. For now, we'll get the output into a sorted BAM file so we can look at it using Samtools later.\n+\n+Download and install Samtools::\n+\n+  %% cd /data/drosophila\n+  %% curl -O -L http://sourceforge.net/projects/samtools/files/samtools/0.1.16/samtools-0.1.16.tar.bz2\n+  %% tar xvfj samtools-0.1.16.tar.bz2\n+  %% cd samtools-0.1.16\n+  %% make\n+  %% cp samtools /usr/local/bin\n+  %% cd misc/\n+  %% cp *.pl maq2sam-long maq2sam-short md5fa md5sum-lite wgsim /usr/local/bin/\n+  %% cd /data/drosophila\n+  \n+Like bwa, Samtools also requires us to go through several steps before we have our data in usable form. First, we need to have Samtools generate its own index of the reference genome::\n+\n+  %% samtools faidx dmel-all-chromosome-r5.37.fasta\n+\n+Next, we need to convert the SAM file into a BAM file. (A BAM file is just a binary version of a SAM file.)::\n+\n+  %% samtools import dmel-all-chromosome-r5.37.fasta.fai RAL357_bwa.sam RAL357_bwa.bam\n+\n+Now, we need to sort the BAM file::\n+\n+  %% samtools sort RAL357_bwa.bam RAL357_bwa.sorted\n+  \n+And last, we need Samtools to index the BAM file::\n+\n+  %% samtools index RAL357_bwa.sorted.bam\n+\n+Let us do this again for the bowtie output. We move back to the drosophila_bowtie directory (you could do this all from the other directory, but it gets harder to read the command with long pathnames)::\n+\n+  %% cd drosophila_bowtie\n+  %% samtools import ../dmel-all-chromosome-r5.37.fasta.fai RAL357_bowtie.sam RAL357_bowtie.bam\n+\n+Now, we need to sort the BAM file (also slow)::\n+\n+  %% samtools sort RAL357_bowtie.bam RAL357_bowtie.sorted\n+  \n+And last, we need Samtools to index the BAM file::\n+\n+  %% samtools index RAL357_bowtie.sorted.bam\n+  \n+All done! Now we can use the sorted BAM file in Samtools to visualize our mappings, generate lists of SNPs, and call consensus sequences. We'll get to all of that later on today and in the rest of the course.\n+\n+Viewing the output with TView\n+-----------------------------\n+\n+Before we can use TView to compare Bowtie and BWA mappings, we need to sort the Bowtie BAM file, and generate an index for it.::\n+\n+  %% cd drosophila_bowtie\n+  %% samtools sort RAL357_full_bowtie.bam RAL357_full_bowtie.sorted\n+  %% samtools index RAL357_full_bowtie.sorted.bam\n+\n+Now that we've generated the files, we can view the output with TView. We'll compare two different sorted::\n+\n+  %% cd ..\n+  %% samtools tview RAL357_full_bwa.sorted.bam\n+\n+Now open an additional terminal window, and load the Bowtie mapping file there as well.::\n+  \n+  %% cd /data/drosophila/drosophila_bowtie\n+  %% samtools tview RAL357_full_bowtie.sorted.bam ../dmel-all-chromosome-r5.37.fasta\n+\n+To view the tview help, type '?'\n+TAGS:bowtie bwa tutorial MSU-NGS-2011", "post": 40, "date": "2012-03-28 08:29:13", "author": 32}}, {"pk": 1, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/6/test-by-zhenhai/", "content": "[Istvan Albert](/user/profile/2/) set status to Deleted on [Question:test by zhenhai]( /post/show/6/test-by-zhenhai/)", "html": "<p><a href=\"/user/profile/2/\">Istvan Albert</a> set status to Deleted on <a href=\"/post/show/6/test-by-zhenhai/\">Question:test by zhenhai</a></p>", "date": "2012-03-28 08:29:12", "unread": true, "type": 1, "target": 5}}, {"pk": 2, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/6/test-by-zhenhai/", "content": "[Istvan Albert](/user/profile/2/) set status to Deleted on [Question:test by zhenhai]( /post/show/6/test-by-zhenhai/)", "html": "<p><a href=\"/user/profile/2/\">Istvan Albert</a> set status to Deleted on <a href=\"/post/show/6/test-by-zhenhai/\">Question:test by zhenhai</a></p>", "date": "2012-03-28 08:29:12", "unread": false, "type": 2, "target": 2}}, {"pk": 3, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/20/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/", "content": "[Istvan Albert](/user/profile/2/) set status to Deleted on [Question:do you have to be a guy to dress up...]( /post/show/20/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/)", "html": "<p><a href=\"/user/profile/2/\">Istvan Albert</a> set status to Deleted on <a href=\"/post/show/20/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/\">Question:do you have to be a guy to dress up...</a></p>", "date": "2012-03-28 08:29:12", "unread": true, "type": 1, "target": 15}}, {"pk": 4, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/20/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/", "content": "[Istvan Albert](/user/profile/2/) set status to Deleted on [Question:do you have to be a guy to dress up...]( /post/show/20/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/)", "html": "<p><a href=\"/user/profile/2/\">Istvan Albert</a> set status to Deleted on <a href=\"/post/show/20/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/\">Question:do you have to be a guy to dress up...</a></p>", "date": "2012-03-28 08:29:12", "unread": false, "type": 2, "target": 2}}, {"pk": 5, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/21/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/", "content": "[Istvan Albert](/user/profile/2/) set status to Deleted on [Question:do you have to be a guy to dress up...]( /post/show/21/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/)", "html": "<p><a href=\"/user/profile/2/\">Istvan Albert</a> set status to Deleted on <a href=\"/post/show/21/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/\">Question:do you have to be a guy to dress up...</a></p>", "date": "2012-03-28 08:29:12", "unread": true, "type": 1, "target": 15}}, {"pk": 6, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/21/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/", "content": "[Istvan Albert](/user/profile/2/) set status to Deleted on [Question:do you have to be a guy to dress up...]( /post/show/21/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/)", "html": "<p><a href=\"/user/profile/2/\">Istvan Albert</a> set status to Deleted on <a href=\"/post/show/21/do-you-have-to-be-a-guy-to-dress-up-as-boy-george/\">Question:do you have to be a guy to dress up...</a></p>", "date": "2012-03-28 08:29:12", "unread": false, "type": 2, "target": 2}}, {"pk": 7, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#31", "content": "Istvan Albert: I'll answer my own question here as it is a demo for now in [Comment:How do I convert from BED format to...]( /post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#31)", "html": "<p>Istvan Albert: I'll answer my own question here as it is a demo for now in <a href=\"/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#31\">Comment:How do I convert from BED format to...</a></p>", "date": "2009-09-30 14:56:48", "unread": false, "type": 1, "target": 2}}, {"pk": 8, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#31", "content": "Istvan Albert: I'll answer my own question here as it is a demo for now in [Comment:How do I convert from BED format to...]( /post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#31)", "html": "<p>Istvan Albert: I'll answer my own question here as it is a demo for now in <a href=\"/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#31\">Comment:How do I convert from BED format to...</a></p>", "date": "2009-09-30 14:56:48", "unread": true, "type": 1, "target": 20}}, {"pk": 9, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/4/finding-common-motifs-in-sequences/#32", "content": "Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in [Comment:Finding common motifs in sequences]( /post/show/4/finding-common-motifs-in-sequences/#32)", "html": "<p>Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in <a href=\"/post/show/4/finding-common-motifs-in-sequences/#32\">Comment:Finding common motifs in sequences</a></p>", "date": "2009-09-30 19:43:58", "unread": false, "type": 1, "target": 2}}, {"pk": 10, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/4/finding-common-motifs-in-sequences/#32", "content": "Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in [Comment:Finding common motifs in sequences]( /post/show/4/finding-common-motifs-in-sequences/#32)", "html": "<p>Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in <a href=\"/post/show/4/finding-common-motifs-in-sequences/#32\">Comment:Finding common motifs in sequences</a></p>", "date": "2009-09-30 19:43:58", "unread": true, "type": 1, "target": 3}}, {"pk": 11, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/4/finding-common-motifs-in-sequences/#32", "content": "Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in [Comment:Finding common motifs in sequences]( /post/show/4/finding-common-motifs-in-sequences/#32)", "html": "<p>Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in <a href=\"/post/show/4/finding-common-motifs-in-sequences/#32\">Comment:Finding common motifs in sequences</a></p>", "date": "2009-09-30 19:43:58", "unread": true, "type": 1, "target": 5}}, {"pk": 12, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/4/finding-common-motifs-in-sequences/#32", "content": "Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in [Comment:Finding common motifs in sequences]( /post/show/4/finding-common-motifs-in-sequences/#32)", "html": "<p>Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in <a href=\"/post/show/4/finding-common-motifs-in-sequences/#32\">Comment:Finding common motifs in sequences</a></p>", "date": "2009-09-30 19:43:58", "unread": true, "type": 1, "target": 6}}, {"pk": 13, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/4/finding-common-motifs-in-sequences/#32", "content": "Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in [Comment:Finding common motifs in sequences]( /post/show/4/finding-common-motifs-in-sequences/#32)", "html": "<p>Istvan Albert: post the python code as well (put it into pre tags then it will be shown nicely formatted, see help on the right) in <a href=\"/post/show/4/finding-common-motifs-in-sequences/#32\">Comment:Finding common motifs in sequences</a></p>", "date": "2009-09-30 19:43:58", "unread": true, "type": 1, "target": 7}}, {"pk": 14, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/1/site-use-guidelines/#33", "content": "Istvan Albert: I see, then I guess they will need to contribute first in some way to gain some reputation first.  in [Comment:Site use guidelines]( /post/show/1/site-use-guidelines/#33)", "html": "<p>Istvan Albert: I see, then I guess they will need to contribute first in some way to gain some reputation first.  in <a href=\"/post/show/1/site-use-guidelines/#33\">Comment:Site use guidelines</a></p>", "date": "2009-10-09 16:57:23", "unread": false, "type": 1, "target": 2}}, {"pk": 15, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/1/site-use-guidelines/#33", "content": "Istvan Albert: I see, then I guess they will need to contribute first in some way to gain some reputation first.  in [Comment:Site use guidelines]( /post/show/1/site-use-guidelines/#33)", "html": "<p>Istvan Albert: I see, then I guess they will need to contribute first in some way to gain some reputation first.  in <a href=\"/post/show/1/site-use-guidelines/#33\">Comment:Site use guidelines</a></p>", "date": "2009-10-09 16:57:23", "unread": true, "type": 1, "target": 10}}, {"pk": 16, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/1/site-use-guidelines/#33", "content": "Istvan Albert: I see, then I guess they will need to contribute first in some way to gain some reputation first.  in [Comment:Site use guidelines]( /post/show/1/site-use-guidelines/#33)", "html": "<p>Istvan Albert: I see, then I guess they will need to contribute first in some way to gain some reputation first.  in <a href=\"/post/show/1/site-use-guidelines/#33\">Comment:Site use guidelines</a></p>", "date": "2009-10-09 16:57:23", "unread": true, "type": 1, "target": 14}}, {"pk": 17, "model": "server.note", "fields": {"sender": 16, "url": "/post/show/22/gene-id-conversion-tool/#34", "content": "Renee: Thanks! That's great! But I'm not student there...Can I access to that anyway? I am using Human whole genome Agilent array. Thank you so much.  in [Comment:Gene ID conversion tool]( /post/show/22/gene-id-conversion-tool/#34)", "html": "<p>Renee: Thanks! That's great! But I'm not student there...Can I access to that anyway? I am using Human whole genome Agilent array. Thank you so much.  in <a href=\"/post/show/22/gene-id-conversion-tool/#34\">Comment:Gene ID conversion tool</a></p>", "date": "2009-10-28 16:38:40", "unread": false, "type": 1, "target": 16}}, {"pk": 18, "model": "server.note", "fields": {"sender": 16, "url": "/post/show/22/gene-id-conversion-tool/#34", "content": "Renee: Thanks! That's great! But I'm not student there...Can I access to that anyway? I am using Human whole genome Agilent array. Thank you so much.  in [Comment:Gene ID conversion tool]( /post/show/22/gene-id-conversion-tool/#34)", "html": "<p>Renee: Thanks! That's great! But I'm not student there...Can I access to that anyway? I am using Human whole genome Agilent array. Thank you so much.  in <a href=\"/post/show/22/gene-id-conversion-tool/#34\">Comment:Gene ID conversion tool</a></p>", "date": "2009-10-28 16:38:40", "unread": true, "type": 1, "target": 2}}, {"pk": 19, "model": "server.note", "fields": {"sender": 16, "url": "/post/show/22/gene-id-conversion-tool/#34", "content": "Renee: Thanks! That's great! But I'm not student there...Can I access to that anyway? I am using Human whole genome Agilent array. Thank you so much.  in [Comment:Gene ID conversion tool]( /post/show/22/gene-id-conversion-tool/#34)", "html": "<p>Renee: Thanks! That's great! But I'm not student there...Can I access to that anyway? I am using Human whole genome Agilent array. Thank you so much.  in <a href=\"/post/show/22/gene-id-conversion-tool/#34\">Comment:Gene ID conversion tool</a></p>", "date": "2009-10-28 16:38:40", "unread": true, "type": 1, "target": 19}}, {"pk": 20, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#35", "content": "Istvan Albert: Just a note: code above need bioperl in [Comment:How do I convert from BED format to...]( /post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#35)", "html": "<p>Istvan Albert: Just a note: code above need bioperl in <a href=\"/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#35\">Comment:How do I convert from BED format to...</a></p>", "date": "2010-01-22 15:14:58", "unread": false, "type": 1, "target": 2}}, {"pk": 21, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#35", "content": "Istvan Albert: Just a note: code above need bioperl in [Comment:How do I convert from BED format to...]( /post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#35)", "html": "<p>Istvan Albert: Just a note: code above need bioperl in <a href=\"/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#35\">Comment:How do I convert from BED format to...</a></p>", "date": "2010-01-22 15:14:58", "unread": true, "type": 1, "target": 20}}, {"pk": 22, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/22/gene-id-conversion-tool/#36", "content": "Istvan Albert: missed this comment, sorry about it! in [Comment:Gene ID conversion tool]( /post/show/22/gene-id-conversion-tool/#36)", "html": "<p>Istvan Albert: missed this comment, sorry about it! in <a href=\"/post/show/22/gene-id-conversion-tool/#36\">Comment:Gene ID conversion tool</a></p>", "date": "2010-01-27 14:20:22", "unread": true, "type": 1, "target": 16}}, {"pk": 23, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/22/gene-id-conversion-tool/#36", "content": "Istvan Albert: missed this comment, sorry about it! in [Comment:Gene ID conversion tool]( /post/show/22/gene-id-conversion-tool/#36)", "html": "<p>Istvan Albert: missed this comment, sorry about it! in <a href=\"/post/show/22/gene-id-conversion-tool/#36\">Comment:Gene ID conversion tool</a></p>", "date": "2010-01-27 14:20:22", "unread": false, "type": 1, "target": 2}}, {"pk": 24, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/22/gene-id-conversion-tool/#36", "content": "Istvan Albert: missed this comment, sorry about it! in [Comment:Gene ID conversion tool]( /post/show/22/gene-id-conversion-tool/#36)", "html": "<p>Istvan Albert: missed this comment, sorry about it! in <a href=\"/post/show/22/gene-id-conversion-tool/#36\">Comment:Gene ID conversion tool</a></p>", "date": "2010-01-27 14:20:22", "unread": true, "type": 1, "target": 19}}, {"pk": 25, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#37", "content": "Istvan Albert: Hi Alex, one thing you could do it replace the case block with a hash map that remaps chromosomes. That way it is a lot easier to add other entries withouth make the code longer and longer... in [Comment:How do I convert from BED format to...]( /post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#37)", "html": "<p>Istvan Albert: Hi Alex, one thing you could do it replace the case block with a hash map that remaps chromosomes. That way it is a lot easier to add other entries withouth make the code longer and longer... in <a href=\"/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#37\">Comment:How do I convert from BED format to...</a></p>", "date": "2010-01-29 16:17:44", "unread": false, "type": 1, "target": 2}}, {"pk": 26, "model": "server.note", "fields": {"sender": 2, "url": "/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#37", "content": "Istvan Albert: Hi Alex, one thing you could do it replace the case block with a hash map that remaps chromosomes. That way it is a lot easier to add other entries withouth make the code longer and longer... in [Comment:How do I convert from BED format to...]( /post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#37)", "html": "<p>Istvan Albert: Hi Alex, one thing you could do it replace the case block with a hash map that remaps chromosomes. That way it is a lot easier to add other entries withouth make the code longer and longer... in <a href=\"/post/show/2/how-do-i-convert-from-bed-format-to-gff-format/#37\">Comment:How do I convert from BED format to...</a></p>", "date": "2010-01-29 16:17:44", "unread": true, "type": 1, "target": 20}}, {"pk": 27, "model": "server.note", "fields": {"sender": 31, "url": "/post/show/38/biostar-forum-posting-guidelines/", "content": "Default Admin: Biostar is divided into several sections:\n\nThe **Questions** tab is meant to help users find answers to well defined and specific bioinformatics questions. Most importantly the tile must be formulated as a question.\n\nThe **Forum** tab displays all ot... in [Forum:Biostar forum posting guidelines]( /post/show/38/biostar-forum-posting-guidelines/)", "html": "<p>Default Admin: Biostar is divided into several sections:</p>\n<p>The <strong>Questions</strong> tab is meant to help users find answers to well defined and specific bioinformatics questions. Most importantly the tile must be formulated as a question.</p>\n<p>The <strong>Forum</strong> tab displays all ot... in <a href=\"/post/show/38/biostar-forum-posting-guidelines/\">Forum:Biostar forum posting guidelines</a></p>", "date": "2012-03-28 08:29:13", "unread": false, "type": 1, "target": 31}}, {"pk": 28, "model": "server.note", "fields": {"sender": 32, "url": "/post/show/39/installing-and-running-ncbi-blast/", "content": "MSU course 2011: ##rest\n\n\"Installing and Running NCBI BLAST\" tutorial imported from the MSU course **Analyzing Next Generation Sequencing Data** (http://bioinformatics.msu.edu/ngs-summer-course-2011)\n\nYou should start this tutorial at a prompt that looks something li... in [Tutorial:Installing and Running NCBI BLAST]( /post/show/39/installing-and-running-ncbi-blast/)", "html": "<p>MSU course 2011: ##rest</p>\n<p>\"Installing and Running NCBI BLAST\" tutorial imported from the MSU course <strong>Analyzing Next Generation Sequencing Data</strong> (<a href=\"http://bioinformatics.msu.edu/ngs\">http://bioinformatics.msu.edu/ngs</a>-summer-course-2011)</p>\n<p>You should start this tutorial at a prompt that looks something li... in <a href=\"/post/show/39/installing-and-running-ncbi-blast/\">Tutorial:Installing and Running NCBI BLAST</a></p>", "date": "2012-03-28 08:29:13", "unread": false, "type": 1, "target": 32}}, {"pk": 29, "model": "server.note", "fields": {"sender": 32, "url": "/post/show/40/mapping-reads-with-bwa-and-bowtie/", "content": "MSU course 2011: ##rest\n\n\"Mapping reads with bwa and bowtie tutorial\" imported from the MSU course **Analyzing Next Generation Sequencing Data** (http://bioinformatics.msu.edu/ngs-summer-course-2011)\n\nIn this tutorial, we're going to take a set of Illumina reads from... in [Tutorial:Mapping reads with bwa and bowtie]( /post/show/40/mapping-reads-with-bwa-and-bowtie/)", "html": "<p>MSU course 2011: ##rest</p>\n<p>\"Mapping reads with bwa and bowtie tutorial\" imported from the MSU course <strong>Analyzing Next Generation Sequencing Data</strong> (<a href=\"http://bioinformatics.msu.edu/ngs\">http://bioinformatics.msu.edu/ngs</a>-summer-course-2011)</p>\n<p>In this tutorial, we're going to take a set of Illumina reads from... in <a href=\"/post/show/40/mapping-reads-with-bwa-and-bowtie/\">Tutorial:Mapping reads with bwa and bowtie</a></p>", "date": "2012-03-28 08:29:13", "unread": false, "type": 1, "target": 32}}, {"pk": 30, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 1}}, {"pk": 31, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 2}}, {"pk": 32, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 3}}, {"pk": 33, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 4}}, {"pk": 34, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 5}}, {"pk": 35, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 6}}, {"pk": 36, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 7}}, {"pk": 37, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 8}}, {"pk": 38, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 9}}, {"pk": 39, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 10}}, {"pk": 40, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 11}}, {"pk": 41, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 12}}, {"pk": 42, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 13}}, {"pk": 43, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 14}}, {"pk": 44, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 15}}, {"pk": 45, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 16}}, {"pk": 46, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 17}}, {"pk": 47, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 18}}, {"pk": 48, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 19}}, {"pk": 49, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 20}}, {"pk": 50, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 21}}, {"pk": 51, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 22}}, {"pk": 52, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 23}}, {"pk": 53, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 24}}, {"pk": 54, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 25}}, {"pk": 55, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 26}}, {"pk": 56, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 27}}, {"pk": 57, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 28}}, {"pk": 58, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 29}}, {"pk": 59, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 30}}, {"pk": 60, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 31}}, {"pk": 61, "model": "server.note", "fields": {"sender": 1, "url": "", "content": "Welcome to **Biostar!**", "html": "<p>Welcome to <strong>Biostar!</strong></p>", "date": "2012-03-28 08:29:13", "unread": true, "type": 1, "target": 32}}, {"pk": 1, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 7, "type": 1, "author": 2}}, {"pk": 2, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 7, "type": 1, "author": 2}}, {"pk": 3, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 4, "type": 1, "author": 2}}, {"pk": 4, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 8, "type": 1, "author": 2}}, {"pk": 5, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 11, "type": 1, "author": 10}}, {"pk": 6, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 11, "type": 1, "author": 10}}, {"pk": 7, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 11, "type": 3, "author": 10}}, {"pk": 8, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 12, "type": 1, "author": 10}}, {"pk": 9, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 10, "type": 1, "author": 2}}, {"pk": 10, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 13, "type": 1, "author": 2}}, {"pk": 11, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 15, "type": 1, "author": 2}}, {"pk": 12, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 13, "type": 1, "author": 2}}, {"pk": 13, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 16, "type": 1, "author": 2}}, {"pk": 14, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 1, "type": 1, "author": 10}}, {"pk": 15, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 18, "type": 1, "author": 2}}, {"pk": 16, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 18, "type": 3, "author": 2}}, {"pk": 17, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 10, "type": 1, "author": 5}}, {"pk": 18, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 24, "type": 1, "author": 2}}, {"pk": 19, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 25, "type": 1, "author": 2}}, {"pk": 20, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 25, "type": 1, "author": 14}}, {"pk": 21, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 26, "type": 1, "author": 14}}, {"pk": 22, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 27, "type": 1, "author": 2}}, {"pk": 23, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 28, "type": 1, "author": 2}}, {"pk": 24, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 29, "type": 3, "author": 20}}, {"pk": 25, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 30, "type": 1, "author": 2}}, {"pk": 26, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 8, "type": 1, "author": 23}}, {"pk": 27, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 7, "type": 1, "author": 23}}, {"pk": 28, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 1, "type": 1, "author": 23}}, {"pk": 29, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 17, "type": 1, "author": 23}}, {"pk": 30, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 3, "type": 1, "author": 23}}, {"pk": 31, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 22, "type": 1, "author": 2}}, {"pk": 32, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 8, "type": 1, "author": 4}}, {"pk": 33, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 27, "type": 1, "author": 23}}, {"pk": 34, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 5, "type": 1, "author": 10}}, {"pk": 35, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 4, "type": 1, "author": 23}}, {"pk": 36, "model": "server.vote", "fields": {"date": "2012-03-28 08:29:12", "post": 22, "type": 1, "author": 23}}, {"pk": 1, "model": "server.badge", "fields": {"count": 18, "name": "Teacher", "secret": false, "unique": true, "type": 0, "description": "Answered first question with at least one up vote"}}, {"pk": 2, "model": "server.badge", "fields": {"count": 12, "name": "Student", "secret": false, "unique": true, "type": 0, "description": "Asked first question with at least one up vote"}}, {"pk": 3, "model": "server.badge", "fields": {"count": 5, "name": "Editor", "secret": false, "unique": true, "type": 0, "description": "First edit"}}, {"pk": 4, "model": "server.badge", "fields": {"count": 0, "name": "Cleanup", "secret": false, "unique": true, "type": 0, "description": "First rollback"}}, {"pk": 5, "model": "server.badge", "fields": {"count": 5, "name": "Organizer", "secret": false, "unique": true, "type": 0, "description": "First retag"}}, {"pk": 6, "model": "server.badge", "fields": {"count": 10, "name": "Supporter", "secret": false, "unique": true, "type": 0, "description": "First up vote"}}, {"pk": 7, "model": "server.badge", "fields": {"count": 4, "name": "Critic", "secret": false, "unique": true, "type": 0, "description": "First down vote"}}, {"pk": 8, "model": "server.badge", "fields": {"count": 1, "name": "Citizen Patrol", "secret": false, "unique": true, "type": 0, "description": "First flagged post"}}, {"pk": 9, "model": "server.badge", "fields": {"count": 1, "name": "Autobiographer", "secret": false, "unique": true, "type": 0, "description": "Completed all user profile fields"}}, {"pk": 10, "model": "server.badge", "fields": {"count": 8, "name": "Scholar", "secret": false, "unique": true, "type": 0, "description": "First accepted answer on your own question"}}, {"pk": 11, "model": "server.badge", "fields": {"count": 0, "name": "Taxonomist", "secret": false, "unique": true, "type": 1, "description": "Created a tag used by 50 questions"}}, {"pk": 12, "model": "server.badge", "fields": {"count": 0, "name": "Strunk & White", "secret": false, "unique": true, "type": 1, "description": "Edited 100 entries"}}, {"pk": 13, "model": "server.badge", "fields": {"count": 0, "name": "Yearling", "secret": false, "unique": false, "type": 1, "description": "Active member for a year, earning at least 200 reputation"}}, {"pk": 14, "model": "server.badge", "fields": {"count": 2, "name": "Self-Learner", "secret": false, "unique": true, "type": 0, "description": "Answered your own question with at least 3 up votes"}}, {"pk": 15, "model": "server.badge", "fields": {"count": 0, "name": "Generalist", "secret": true, "unique": false, "type": 1, "description": "Active in many different tags"}}, {"pk": 16, "model": "server.badge", "fields": {"count": 0, "name": "Necromancer", "secret": false, "unique": false, "type": 1, "description": "Answered a question more than 60 days later with at least 5 votes"}}, {"pk": 17, "model": "server.badge", "fields": {"count": 0, "name": "Guru", "secret": false, "unique": false, "type": 1, "description": "Accepted answer and voted up 40 times"}}, {"pk": 18, "model": "server.badge", "fields": {"count": 1, "name": "Enlightened", "secret": false, "unique": false, "type": 1, "description": "First answer was accepted with at least 10 up votes"}}, {"pk": 19, "model": "server.badge", "fields": {"count": 1, "name": "Nice Question", "secret": false, "unique": false, "type": 0, "description": "Question voted up 10 times"}}, {"pk": 20, "model": "server.badge", "fields": {"count": 0, "name": "Good Question", "secret": false, "unique": false, "type": 1, "description": "Question voted up 25 times"}}, {"pk": 21, "model": "server.badge", "fields": {"count": 0, "name": "Great Question", "secret": false, "unique": false, "type": 2, "description": "Question voted up 100 times"}}, {"pk": 22, "model": "server.badge", "fields": {"count": 1, "name": "Nice Answer", "secret": false, "unique": false, "type": 0, "description": "Answer voted up 10 times"}}, {"pk": 23, "model": "server.badge", "fields": {"count": 0, "name": "Good Answer", "secret": false, "unique": false, "type": 1, "description": "Answer voted up 25 times"}}, {"pk": 24, "model": "server.badge", "fields": {"count": 0, "name": "Great Answer", "secret": false, "unique": false, "type": 2, "description": "Answer voted up 100 times"}}, {"pk": 25, "model": "server.badge", "fields": {"count": 0, "name": "Popular Question", "secret": false, "unique": false, "type": 0, "description": "Asked a question with 1,000 views"}}, {"pk": 26, "model": "server.badge", "fields": {"count": 0, "name": "Notable Question", "secret": false, "unique": false, "type": 1, "description": "Asked a question with 2,500 views"}}, {"pk": 27, "model": "server.badge", "fields": {"count": 0, "name": "Famous Question", "secret": false, "unique": false, "type": 2, "description": "Asked a question with 10,000 views"}}, {"pk": 28, "model": "server.badge", "fields": {"count": 0, "name": "Hacker", "secret": true, "unique": true, "type": 1, "description": "Contributed to Stack Overflow in an unusual way"}}, {"pk": 29, "model": "server.badge", "fields": {"count": 4, "name": "Commentator", "secret": false, "unique": true, "type": 0, "description": "Left 10 comments"}}, {"pk": 30, "model": "server.badge", "fields": {"count": 2, "name": "Civic Duty", "secret": false, "unique": true, "type": 1, "description": "Voted 300 times"}}, {"pk": 31, "model": "server.badge", "fields": {"count": 0, "name": "Beta Tester", "secret": false, "unique": true, "type": 2, "description": "Participated in the BioStar public beta test. Thank you!"}}, {"pk": 1, "model": "server.award", "fields": {"date": "2009-09-30 15:19:15", "badge": 3, "user": 2}}, {"pk": 2, "model": "server.award", "fields": {"date": "2009-09-30 19:05:41", "badge": 1, "user": 5}}, {"pk": 3, "model": "server.award", "fields": {"date": "2009-09-30 19:05:41", "badge": 2, "user": 3}}, {"pk": 4, "model": "server.award", "fields": {"date": "2009-09-30 19:05:41", "badge": 6, "user": 2}}, {"pk": 5, "model": "server.award", "fields": {"date": "2009-09-30 19:45:41", "badge": 1, "user": 6}}, {"pk": 6, "model": "server.award", "fields": {"date": "2009-10-05 16:05:41", "badge": 1, "user": 2}}, {"pk": 7, "model": "server.award", "fields": {"date": "2009-10-05 16:05:41", "badge": 6, "user": 10}}, {"pk": 8, "model": "server.award", "fields": {"date": "2009-10-05 16:05:41", "badge": 10, "user": 10}}, {"pk": 9, "model": "server.award", "fields": {"date": "2009-10-05 16:15:41", "badge": 3, "user": 10}}, {"pk": 10, "model": "server.award", "fields": {"date": "2009-10-06 12:50:43", "badge": 2, "user": 10}}, {"pk": 11, "model": "server.award", "fields": {"date": "2009-10-06 20:10:44", "badge": 2, "user": 12}}, {"pk": 12, "model": "server.award", "fields": {"date": "2009-10-06 22:15:44", "badge": 1, "user": 13}}, {"pk": 13, "model": "server.award", "fields": {"date": "2009-10-07 16:46:17", "badge": 2, "user": 2}}, {"pk": 14, "model": "server.award", "fields": {"date": "2009-10-09 17:05:57", "badge": 1, "user": 14}}, {"pk": 15, "model": "server.award", "fields": {"date": "2009-10-09 17:05:57", "badge": 10, "user": 2}}, {"pk": 16, "model": "server.award", "fields": {"date": "2009-10-13 15:07:39", "badge": 5, "user": 2}}, {"pk": 17, "model": "server.award", "fields": {"date": "2009-10-13 16:52:21", "badge": 6, "user": 5}}, {"pk": 18, "model": "server.award", "fields": {"date": "2009-12-01 20:59:37", "badge": 1, "user": 18}}, {"pk": 19, "model": "server.award", "fields": {"date": "2009-12-01 20:59:37", "badge": 2, "user": 14}}, {"pk": 20, "model": "server.award", "fields": {"date": "2009-12-01 20:59:37", "badge": 6, "user": 14}}, {"pk": 21, "model": "server.award", "fields": {"date": "2009-12-09 21:20:50", "badge": 1, "user": 19}}, {"pk": 22, "model": "server.award", "fields": {"date": "2010-01-13 22:08:19", "badge": 2, "user": 20}}, {"pk": 23, "model": "server.award", "fields": {"date": "2010-01-15 08:50:57", "badge": 10, "user": 20}}, {"pk": 24, "model": "server.award", "fields": {"date": "2010-01-22 15:51:05", "badge": 1, "user": 20}}, {"pk": 25, "model": "server.award", "fields": {"date": "2010-01-26 20:35:54", "badge": 2, "user": 23}}, {"pk": 26, "model": "server.award", "fields": {"date": "2010-01-26 20:50:52", "badge": 2, "user": 4}}, {"pk": 27, "model": "server.award", "fields": {"date": "2010-01-27 14:21:01", "badge": 1, "user": 23}}, {"pk": 28, "model": "server.award", "fields": {"date": "2010-01-28 16:25:01", "badge": 1, "user": 10}}, {"pk": 29, "model": "server.award", "fields": {"date": "2010-01-28 16:25:02", "badge": 1, "user": 25}}, {"pk": 30, "model": "server.award", "fields": {"date": "2010-01-28 16:25:02", "badge": 6, "user": 23}}, {"pk": 31, "model": "server.award", "fields": {"date": "2010-01-29 10:24:51", "badge": 1, "user": 4}}, {"pk": 32, "model": "server.award", "fields": {"date": "2010-01-29 14:20:52", "badge": 10, "user": 23}}, {"pk": 33, "model": "server.award", "fields": {"date": "2010-01-29 16:20:56", "badge": 2, "user": 16}}, {"pk": 34, "model": "server.award", "fields": {"date": "2010-01-31 03:20:52", "badge": 6, "user": 4}}, {"pk": 35, "model": "server.award", "fields": {"date": "2010-01-31 03:20:52", "badge": 10, "user": 4}}, {"pk": 36, "model": "server.award", "fields": {"date": "2010-02-13 18:06:00", "badge": 1, "user": 26}}, {"pk": 37, "model": "server.award", "fields": {"date": "2010-02-16 19:35:59", "badge": 10, "user": 14}}, {"pk": 38, "model": "server.award", "fields": {"date": "2010-02-20 17:06:05", "badge": 1, "user": 27}}, {"pk": 39, "model": "server.award", "fields": {"date": "2010-02-20 17:06:05", "badge": 1, "user": 28}}, {"pk": 40, "model": "server.award", "fields": {"date": "2010-02-20 17:06:05", "badge": 2, "user": 27}}, {"pk": 41, "model": "server.award", "fields": {"date": "2010-02-25 18:35:56", "badge": 1, "user": 30}}, {"pk": 42, "model": "server.award", "fields": {"date": "2010-02-25 18:35:57", "badge": 29, "user": 2}}, {"pk": 43, "model": "server.award", "fields": {"date": "2010-02-25 22:38:48", "badge": 7, "user": 2}}, {"pk": 44, "model": "server.award", "fields": {"date": "2010-02-26 11:36:19", "badge": 6, "user": 30}}, {"pk": 45, "model": "server.award", "fields": {"date": "2010-02-26 13:23:45", "badge": 2, "user": 30}}, {"pk": 46, "model": "server.award", "fields": {"date": "2010-02-26 15:07:43", "badge": 9, "user": 23}}, {"pk": 47, "model": "server.award", "fields": {"date": "2010-02-26 15:07:43", "badge": 10, "user": 30}}, {"pk": 48, "model": "server.award", "fields": {"date": "2010-02-26 18:05:54", "badge": 1, "user": 7}}, {"pk": 49, "model": "server.award", "fields": {"date": "2010-02-27 10:05:55", "badge": 1, "user": 24}}, {"pk": 50, "model": "server.award", "fields": {"date": "2010-02-27 10:05:55", "badge": 3, "user": 23}}, {"pk": 51, "model": "server.award", "fields": {"date": "2010-03-01 18:36:14", "badge": 29, "user": 23}}, {"pk": 52, "model": "server.award", "fields": {"date": "2010-03-02 17:50:51", "badge": 5, "user": 23}}, {"pk": 53, "model": "server.award", "fields": {"date": "2010-03-03 11:36:57", "badge": 7, "user": 23}}, {"pk": 54, "model": "server.award", "fields": {"date": "2010-03-04 04:05:54", "badge": 2, "user": 7}}, {"pk": 55, "model": "server.award", "fields": {"date": "2010-03-04 14:38:59", "badge": 6, "user": 7}}, {"pk": 56, "model": "server.award", "fields": {"date": "2010-03-04 14:38:59", "badge": 10, "user": 7}}, {"pk": 57, "model": "server.award", "fields": {"date": "2010-03-04 15:36:11", "badge": 14, "user": 23}}, {"pk": 58, "model": "server.award", "fields": {"date": "2010-03-04 16:20:56", "badge": 29, "user": 30}}, {"pk": 59, "model": "server.award", "fields": {"date": "2010-03-04 22:38:57", "badge": 6, "user": 26}}, {"pk": 60, "model": "server.award", "fields": {"date": "2010-03-04 23:36:16", "badge": 3, "user": 7}}, {"pk": 61, "model": "server.award", "fields": {"date": "2010-03-06 18:50:55", "badge": 5, "user": 10}}, {"pk": 62, "model": "server.award", "fields": {"date": "2010-03-09 20:20:53", "badge": 5, "user": 26}}, {"pk": 63, "model": "server.award", "fields": {"date": "2010-03-10 20:50:54", "badge": 6, "user": 24}}, {"pk": 64, "model": "server.award", "fields": {"date": "2010-03-12 17:51:07", "badge": 3, "user": 30}}, {"pk": 65, "model": "server.award", "fields": {"date": "2010-03-13 18:20:54", "badge": 7, "user": 26}}, {"pk": 66, "model": "server.award", "fields": {"date": "2010-03-14 08:22:32", "badge": 29, "user": 26}}, {"pk": 67, "model": "server.award", "fields": {"date": "2010-03-22 10:20:57", "badge": 19, "user": 2}}, {"pk": 68, "model": "server.award", "fields": {"date": "2010-03-22 17:35:54", "badge": 5, "user": 30}}, {"pk": 69, "model": "server.award", "fields": {"date": "2010-03-26 11:50:47", "badge": 8, "user": 23}}, {"pk": 70, "model": "server.award", "fields": {"date": "2010-03-31 12:35:47", "badge": 22, "user": 30}}, {"pk": 71, "model": "server.award", "fields": {"date": "2010-03-31 12:50:49", "badge": 18, "user": 30}}, {"pk": 72, "model": "server.award", "fields": {"date": "2010-04-10 01:50:47", "badge": 30, "user": 2}}, {"pk": 73, "model": "server.award", "fields": {"date": "2010-04-12 15:05:48", "badge": 14, "user": 2}}, {"pk": 74, "model": "server.award", "fields": {"date": "2010-04-12 15:20:48", "badge": 7, "user": 30}}, {"pk": 75, "model": "server.award", "fields": {"date": "2010-04-15 07:50:50", "badge": 30, "user": 23}}]